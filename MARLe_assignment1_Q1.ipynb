{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVMoGIt_-yBb",
        "outputId": "e2953010-6000-4d55-8aff-4cef69e89f9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SOLUTION BY POLICY ITERATION:-\n",
            "\n",
            "Optimal Value Function: {'Hostel': 16.056233536137125, 'Academic-Building': 21.846509538726654, 'Canteen': 18.826701241558315}\n",
            "Optimal Policy: {'Hostel': 'attend', 'Academic-Building': 'attend', 'Canteen': 'attend'}\n",
            "\n",
            "-------------------------------------\n",
            "\n",
            "SOLUTION BY VALUE ITERATION:-\n",
            "\n",
            "Optimal Value Function: {'Hostel': 16.056233839779527, 'Academic-Building': 21.846509827444127, 'Canteen': 18.826701506623575}\n",
            "Optimal Policy: {'Hostel': 'attend', 'Academic-Building': 'attend', 'Canteen': 'attend'}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, states, actions, transition_probabilities, rewards, discount_factor):\n",
        "        self.states = states\n",
        "        self.actions = actions\n",
        "        self.transition_probabilities = transition_probabilities\n",
        "        self.rewards = rewards\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "    def get_transition_probabilities(self, state, action):\n",
        "        return self.transition_probabilities[state][action]\n",
        "\n",
        "    def get_reward(self, state, action):\n",
        "        return self.rewards[state][action]\n",
        "\n",
        "def value_iteration(mdp, epsilon=1e-6):\n",
        "\n",
        "    V = {state: 0 for state in mdp.states}\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in mdp.states:\n",
        "            max_value = float('-inf')\n",
        "            for action in mdp.actions:\n",
        "                expected_value = sum(prob * (mdp.get_reward(state, action) + mdp.discount_factor * V[next_state]) for next_state, prob in mdp.get_transition_probabilities(state, action).items())\n",
        "                max_value = max(max_value, expected_value)\n",
        "\n",
        "            delta = max(delta, abs(max_value - V[state]))\n",
        "            V[state] = max_value\n",
        "\n",
        "        if delta < epsilon:\n",
        "            break\n",
        "\n",
        "    policy = {}\n",
        "    for state in mdp.states:\n",
        "        best_action = None\n",
        "        best_value = float('-inf')\n",
        "        for action in mdp.actions:\n",
        "            expected_value = sum(prob * (mdp.get_reward(state, action) + mdp.discount_factor * V[next_state]) for next_state, prob in mdp.get_transition_probabilities(state, action).items())\n",
        "            if expected_value > best_value:\n",
        "                best_value = expected_value\n",
        "                best_action = action\n",
        "        policy[state] = best_action\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "\n",
        "def policy_evaluation(policy, mdp, V, epsilon=1e-6):\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for state in mdp.states:\n",
        "            action = policy[state]\n",
        "            expected_value = sum(prob * (mdp.get_reward(state, action) + mdp.discount_factor * V[next_state]) for next_state, prob in mdp.get_transition_probabilities(state, action).items())\n",
        "            delta = max(delta, abs(expected_value - V[state]))\n",
        "            V[state] = expected_value\n",
        "\n",
        "        if delta < epsilon:\n",
        "            break\n",
        "    return V\n",
        "\n",
        "def policy_iteration(mdp):\n",
        "\n",
        "    policy = {state: np.random.choice(mdp.actions) for state in mdp.states}\n",
        "\n",
        "    V = {state: 0 for state in mdp.states}\n",
        "\n",
        "    while True:\n",
        "        # Policy Evaluation\n",
        "        V = policy_evaluation(policy, mdp, V)\n",
        "\n",
        "        policy_stable = True\n",
        "\n",
        "        # Policy Improvement\n",
        "        for state in mdp.states:\n",
        "            old_action = policy[state]\n",
        "            action_values = {}\n",
        "            for action in mdp.actions:\n",
        "                action_values[action] = sum(prob * (mdp.get_reward(state, action) + mdp.discount_factor * V[next_state]) for next_state, prob in mdp.get_transition_probabilities(state, action).items())\n",
        "            best_action = max(action_values, key=action_values.get)\n",
        "\n",
        "            if best_action != old_action:\n",
        "                policy_stable = False\n",
        "\n",
        "            policy[state] = best_action\n",
        "\n",
        "        if policy_stable:\n",
        "            break\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "# Defining states, action and reward\n",
        "states = ['Hostel', 'Academic-Building', 'Canteen']\n",
        "actions = ['attend', 'hungry']\n",
        "transition_probabilities = {\n",
        "    'Hostel': {'attend': {'Hostel': 0.5, 'Academic-Building': 0.5}, 'hungry': {'Canteen': 1.0}},\n",
        "    'Academic-Building': {'attend': {'Academic-Building': 0.7, 'Canteen': 0.3}, 'hungry': {'Academic-Building': 0.2, 'Canteen': 0.8}},\n",
        "    'Canteen': {'attend': {'Hostel': 0.3, 'Academic-Building': 0.6, 'Canteen': 0.1}, 'hungry': {'Canteen': 1}}\n",
        "}\n",
        "rewards = {\n",
        "    'Hostel': {'attend': -1, 'hungry': -1},\n",
        "    'Academic-Building': {'attend': 3, 'hungry': 3},\n",
        "    'Canteen': {'attend': 1, 'hungry': 1}\n",
        "}\n",
        "discount_factor = 0.9\n",
        "\n",
        "mdp = MDP(states, actions, transition_probabilities, rewards, discount_factor)\n",
        "\n",
        "\n",
        "V, policy = policy_iteration(mdp)\n",
        "print(\"SOLUTION BY POLICY ITERATION:-\")\n",
        "print()\n",
        "print(\"Optimal Value Function:\", V)\n",
        "print(\"Optimal Policy:\", policy)\n",
        "print()\n",
        "print(\"-------------------------------------\")\n",
        "print()\n",
        "print(\"SOLUTION BY VALUE ITERATION:-\")\n",
        "print()\n",
        "V, policy = value_iteration(mdp)\n",
        "print(\"Optimal Value Function:\", V)\n",
        "print(\"Optimal Policy:\", policy)"
      ]
    }
  ]
}