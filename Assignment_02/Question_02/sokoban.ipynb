{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/abhinav/anaconda3/envs/RL/lib/python3.13/site-packages (2.1.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/abhinav/anaconda3/envs/RL/lib/python3.13/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class DPAgent:\n",
    "    \"\"\"\n",
    "    Agent that uses Dynamic Programming for policy computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "        self.all_states = self._enumerate_all_states()\n",
    "        self.state_map = {state: idx for idx, state in enumerate(self.all_states)}\n",
    "        self.num_states = len(self.all_states)\n",
    "        self.num_actions = environment.action_space.n\n",
    "        self.value_function = np.zeros(self.num_states)\n",
    "        self.policy = np.zeros(self.num_states, dtype=int)\n",
    "\n",
    "    def _enumerate_all_states(self):\n",
    "        \"\"\"\n",
    "        Generate all possible states for the Sokoban puzzle.\n",
    "        \"\"\"\n",
    "        states = []\n",
    "        positions = [\n",
    "            (i, j) for i in range(self.environment.grid_height) for j in range(self.environment.grid_width)\n",
    "        ]\n",
    "        for player_pos in positions:\n",
    "            for box_pos in positions:\n",
    "                for target_pos in positions:\n",
    "                    if (\n",
    "                        player_pos != box_pos\n",
    "                        and box_pos != target_pos\n",
    "                        and player_pos != target_pos\n",
    "                    ):\n",
    "                        states.append((player_pos, box_pos, target_pos))\n",
    "        return states\n",
    "\n",
    "    def value_iteration(self, gamma=0.99, epsilon=1e-6, max_iterations=1000):\n",
    "        \"\"\"\n",
    "        Perform value iteration to compute the optimal policy.\n",
    "        \"\"\"\n",
    "        for _ in tqdm(range(max_iterations), desc=\"Value Iteration Progress\"):\n",
    "            delta = 0\n",
    "            for idx, state in enumerate(self.all_states):\n",
    "                v = self.value_function[idx]\n",
    "                action_values = []\n",
    "                for action in range(self.num_actions):\n",
    "                    new_state, reward, terminal = self._simulate_step(state, action)\n",
    "                    next_idx = self.state_map.get(new_state, -1)\n",
    "                    if next_idx >= 0:\n",
    "                        action_value = reward + gamma * self.value_function[next_idx] * (not terminal)\n",
    "                    else:\n",
    "                        action_value = reward\n",
    "                    action_values.append(action_value)\n",
    "                self.value_function[idx] = max(action_values)\n",
    "                delta = max(delta, abs(v - self.value_function[idx]))\n",
    "            if delta < epsilon:\n",
    "                break\n",
    "        self._derive_policy(gamma)\n",
    "\n",
    "    def _simulate_step(self, state, action):\n",
    "        \"\"\"\n",
    "        Simulate an environment step for the given state and action.\n",
    "        \"\"\"\n",
    "        self.environment.reset()\n",
    "        self.environment.set_state(state)\n",
    "        _, reward, terminal, _ = self.environment.step(action)\n",
    "        return self.environment.get_state(), reward, terminal\n",
    "\n",
    "    def _derive_policy(self, gamma):\n",
    "        \"\"\"\n",
    "        Extract the optimal policy from the computed value function.\n",
    "        \"\"\"\n",
    "        for idx, state in enumerate(self.all_states):\n",
    "            action_values = []\n",
    "            for action in range(self.num_actions):\n",
    "                new_state, reward, terminal = self._simulate_step(state, action)\n",
    "                next_idx = self.state_map.get(new_state, -1)\n",
    "                if next_idx >= 0:\n",
    "                    action_value = reward + gamma * self.value_function[next_idx] * (not terminal)\n",
    "                else:\n",
    "                    action_value = reward\n",
    "                action_values.append(action_value)\n",
    "            self.policy[idx] = np.argmax(action_values)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select an action based on the computed policy.\n",
    "        \"\"\"\n",
    "        idx = self.state_map.get(state, -1)\n",
    "        if idx >= 0:\n",
    "            return self.policy[idx]\n",
    "        return self.environment.action_space.sample()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAgent:\n",
    "    \"\"\"\n",
    "    Agent that uses Monte Carlo methods for policy computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "        self.q_values = defaultdict(lambda: np.zeros(environment.action_space.n))\n",
    "        self.returns = defaultdict(lambda: defaultdict(list))\n",
    "        self.action_policy = {}\n",
    "        self.initial_epsilon = 1.0\n",
    "        self.minimum_epsilon = 0.01\n",
    "        self.epsilon_decay_rate = 0.995\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def _epsilon_greedy(self, state, epsilon):\n",
    "        \"\"\"\n",
    "        Epsilon-greedy action selection.\n",
    "        \"\"\"\n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.environment.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.q_values[state])\n",
    "\n",
    "    def generate_episode(self, epsilon):\n",
    "        \"\"\"\n",
    "        Generate an episode using the current policy.\n",
    "        \"\"\"\n",
    "        episode = []\n",
    "        self.environment.reset()\n",
    "        terminal = False\n",
    "        steps = 0\n",
    "        while not terminal and steps < 100:\n",
    "            state = self.environment.get_state()\n",
    "            action = self._epsilon_greedy(state, epsilon)\n",
    "            _, reward, terminal, _ = self.environment.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            steps += 1\n",
    "        return episode\n",
    "\n",
    "    def learn_policy(self, episode_count=1000):\n",
    "        \"\"\"\n",
    "        Train the agent using Monte Carlo methods.\n",
    "        \"\"\"\n",
    "        epsilon = self.initial_epsilon\n",
    "        for _ in tqdm(range(episode_count), desc=\"Monte Carlo Training Progress\"):\n",
    "            episode = self.generate_episode(epsilon)\n",
    "            G = 0\n",
    "            visited_state_actions = set()\n",
    "            for state, action, reward in reversed(episode):\n",
    "                G = self.gamma * G + reward\n",
    "                if (state, action) not in visited_state_actions:\n",
    "                    visited_state_actions.add((state, action))\n",
    "                    self.returns[state][action].append(G)\n",
    "                    self.q_values[state][action] = np.mean(self.returns[state][action])\n",
    "                    self.action_policy[state] = np.argmax(self.q_values[state])\n",
    "            epsilon = max(self.minimum_epsilon, epsilon * self.epsilon_decay_rate)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Select action based on the learned policy.\n",
    "        \"\"\"\n",
    "        return self.action_policy.get(state, self.environment.action_space.sample())\n",
    "\n",
    "\n",
    "def simulate_episode(environment, agent, max_steps=100, render=False):\n",
    "    \"\"\"\n",
    "    Simulate a single episode with the given agent.\n",
    "\n",
    "    Args:\n",
    "        environment: The Sokoban environment instance.\n",
    "        agent: The agent to use for the simulation.\n",
    "        max_steps (int): Maximum steps to simulate.\n",
    "        render (bool): Whether to render the environment.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (total reward, total steps)\n",
    "    \"\"\"\n",
    "    state, _ = environment.reset()\n",
    "    total_return_val = 0\n",
    "    terminal = False\n",
    "    steps = 0\n",
    "    while not terminal and steps < max_steps:\n",
    "        current_state = environment.get_state()\n",
    "        action = agent.select_action(current_state)\n",
    "        state, reward, terminal, _ = environment.step(action)\n",
    "        total_return_val += reward\n",
    "        steps += 1\n",
    "        if render:\n",
    "            environment.render()\n",
    "    return total_return_val, steps\n",
    "\n",
    "\n",
    "def agent_assess(environment, agent, episode_count=1000):\n",
    "    \"\"\"\n",
    "    Assess the agent's performance over multiple episodes.\n",
    "\n",
    "    Args:\n",
    "        environment: The Sokoban environment instance.\n",
    "        agent: The agent to be evaluated.\n",
    "        episode_count (int): Number of episodes to evaluate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (average reward, average steps)\n",
    "    \"\"\"\n",
    "    return_vals = []\n",
    "    step_count_list = []\n",
    "    for _ in range(episode_count):\n",
    "        total_return_val, steps = simulate_episode(environment, agent)\n",
    "        return_vals.append(total_return_val)\n",
    "        step_count_list.append(steps)\n",
    "    return np.mean(return_vals), np.mean(step_count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /home/abhinav/anaconda3/envs/RL/lib/python3.13/site-packages (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /home/abhinav/anaconda3/envs/RL/lib/python3.13/site-packages (from gym) (2.1.3)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/abhinav/anaconda3/envs/RL/lib/python3.13/site-packages (from gym) (3.1.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /home/abhinav/anaconda3/envs/RL/lib/python3.13/site-packages (from gym) (0.0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class SokobanEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Sokoban environment for reinforcement learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SokobanEnv, self).__init__()\n",
    "\n",
    "        # Define the grid size\n",
    "        self.grid_height = 6\n",
    "        self.grid_width = 7\n",
    "\n",
    "        # Define action and observation space\n",
    "        # Actions: Up, Down, Left, Right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=5, shape=(self.grid_height, self.grid_width), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        # Define grid elements\n",
    "        self.EMPTY = 0\n",
    "        self.WALL = 1\n",
    "        self.BOX = 2\n",
    "        self.TARGET = 3\n",
    "        self.PLAYER = 4\n",
    "        self.BOX_ON_TARGET = 5\n",
    "\n",
    "        # Map actions to movements\n",
    "        self.action_map = {\n",
    "            0: (-1, 0),  # Up\n",
    "            1: (1, 0),   # Down\n",
    "            2: (0, -1),  # Left\n",
    "            3: (0, 1),   # Right\n",
    "        }\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset the environment to its initial state.\n",
    "        \"\"\"\n",
    "        # Initialize the grid\n",
    "        self.grid = np.zeros((self.grid_height, self.grid_width), dtype=int)\n",
    "        self.grid[1:-1, 1:-1] = self.EMPTY  # Set inner area to empty\n",
    "\n",
    "        # Add walls around the grid\n",
    "        self.grid[0, :] = self.WALL\n",
    "        self.grid[-1, :] = self.WALL\n",
    "        self.grid[:, 0] = self.WALL\n",
    "        self.grid[:, -1] = self.WALL\n",
    "\n",
    "        # Place the player at a random empty position\n",
    "        empty_positions = list(zip(*np.where(self.grid == self.EMPTY)))\n",
    "        self.player_position = np.array(empty_positions[np.random.choice(len(empty_positions))])\n",
    "        self.grid[self.player_position[0], self.player_position[1]] = self.PLAYER\n",
    "\n",
    "        # Place the box at a random empty position\n",
    "        empty_positions = list(zip(*np.where(self.grid == self.EMPTY)))\n",
    "        self.box_position = np.array(empty_positions[np.random.choice(len(empty_positions))])\n",
    "        self.grid[self.box_position[0], self.box_position[1]] = self.BOX\n",
    "\n",
    "        # Place the target at a random empty position\n",
    "        empty_positions = list(zip(*np.where(self.grid == self.EMPTY)))\n",
    "        self.target_position = np.array(empty_positions[np.random.choice(len(empty_positions))])\n",
    "        self.grid[self.target_position[0], self.target_position[1]] = self.TARGET\n",
    "\n",
    "        return self.grid.copy(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        Perform an action in the environment.\n",
    "        Args:\n",
    "            action (int): The action to take (0: Up, 1: Down, 2: Left, 3: Right).\n",
    "        Returns:\n",
    "            tuple: (observation, reward, done, info)\n",
    "        \"\"\"\n",
    "        move = self.action_map[action]\n",
    "        new_player_pos = self.player_position + move\n",
    "\n",
    "        # **Bounds Check for new_player_pos**\n",
    "        if (\n",
    "            new_player_pos[0] < 0 or new_player_pos[0] >= self.grid_height or\n",
    "            new_player_pos[1] < 0 or new_player_pos[1] >= self.grid_width\n",
    "        ):\n",
    "            return self.grid.copy(), -1, False, {}  # Invalid move: out of bounds\n",
    "\n",
    "        # Check for wall collision\n",
    "        if self.grid[new_player_pos[0], new_player_pos[1]] == self.WALL:\n",
    "            return self.grid.copy(), -1, False, {}  # Invalid move: collision with wall\n",
    "\n",
    "        # Check if the player is pushing the box\n",
    "        if np.array_equal(new_player_pos, self.box_position):\n",
    "            new_box_pos = self.box_position + move\n",
    "\n",
    "            # Bounds Check for new_box_pos\n",
    "            if (\n",
    "                new_box_pos[0] < 0 or new_box_pos[0] >= self.grid_height or\n",
    "                new_box_pos[1] < 0 or new_box_pos[1] >= self.grid_width or\n",
    "                self.grid[new_box_pos[0], new_box_pos[1]] in [self.WALL, self.BOX]\n",
    "            ):\n",
    "                return self.grid.copy(), -1, False, {}  # Invalid move: collision or out of bounds\n",
    "\n",
    "            # Move the box\n",
    "            self.grid[self.box_position[0], self.box_position[1]] = self.EMPTY\n",
    "            self.box_position = new_box_pos\n",
    "\n",
    "            if np.array_equal(self.box_position, self.target_position):\n",
    "                self.grid[self.box_position[0], self.box_position[1]] = self.BOX_ON_TARGET\n",
    "            else:\n",
    "                self.grid[self.box_position[0], self.box_position[1]] = self.BOX\n",
    "\n",
    "        # Move the player\n",
    "        self.grid[self.player_position[0], self.player_position[1]] = self.EMPTY\n",
    "        self.player_position = new_player_pos\n",
    "        self.grid[self.player_position[0], self.player_position[1]] = self.PLAYER\n",
    "\n",
    "        # Check if the box is on the target\n",
    "        terminal = np.array_equal(self.box_position, self.target_position)\n",
    "        reward = 10 if terminal else -1  # Positive reward if solved, negative otherwise\n",
    "\n",
    "        return self.grid.copy(), reward, terminal, {}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        \"\"\"\n",
    "        Render the environment grid.\n",
    "        \"\"\"\n",
    "        if mode == \"human\":\n",
    "            symbols = {\n",
    "                self.EMPTY: ' ',\n",
    "                self.WALL: '#',\n",
    "                self.BOX: '$',\n",
    "                self.TARGET: '.',\n",
    "                self.PLAYER: '@',\n",
    "                self.BOX_ON_TARGET: '*'\n",
    "            }\n",
    "            print(\"\\n\".join(\"\".join(symbols[cell] for cell in row) for row in self.grid))\n",
    "        elif mode == \"rgb_array\":\n",
    "            # Optional implementation for visualizing the environment\n",
    "            pass\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Returns a tuple representing the current state.\n",
    "        \"\"\"\n",
    "        return (tuple(self.player_position), tuple(self.box_position), tuple(self.target_position))\n",
    "\n",
    "    def set_state(self, state):\n",
    "        \"\"\"\n",
    "        Set the environment to a specific state.\n",
    "        Args:\n",
    "            state (tuple): A tuple of (player_position, box_position, target_position).\n",
    "        \"\"\"\n",
    "        self.grid = np.zeros((self.grid_height, self.grid_width), dtype=int)\n",
    "        self.grid[1:-1, 1:-1] = self.EMPTY  # Reset inner area to empty\n",
    "\n",
    "        # Add walls around the grid\n",
    "        self.grid[0, :] = self.WALL\n",
    "        self.grid[-1, :] = self.WALL\n",
    "        self.grid[:, 0] = self.WALL\n",
    "        self.grid[:, -1] = self.WALL\n",
    "\n",
    "        self.player_position = np.array(state[0])\n",
    "        self.box_position = np.array(state[1])\n",
    "        self.target_position = np.array(state[2])\n",
    "\n",
    "        self.grid[self.player_position[0], self.player_position[1]] = self.PLAYER\n",
    "        self.grid[self.box_position[0], self.box_position[1]] = self.BOX\n",
    "        self.grid[self.target_position[0], self.target_position[1]] = self.TARGET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dynamic Programming Agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Value Iteration Progress: 100%|██████████| 1000/1000 [3:32:05<00:00, 12.73s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Programming training completed in 12738.24 seconds\n",
      "\n",
      "Training Monte Carlo Agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monte Carlo Training Progress: 100%|██████████| 1000/1000 [00:00<00:00, 1139.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo training completed in 0.88 seconds\n",
      "\n",
      "Evaluating Dynamic Programming Agent...\n",
      "Dynamic Programming - Average Reward: -59.34, Average Steps: 63.59\n",
      "\n",
      "Evaluating Monte Carlo Agent...\n",
      "Monte Carlo - Average Reward: -96.17, Average Steps: 96.59\n",
      "\n",
      "Comparison:\n",
      "Training Time - DP: 12738.24s, MC: 0.88s\n",
      "Average Reward - DP: -59.34, MC: -96.17\n",
      "Average Steps - DP: 63.59, MC: 96.59\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def main():\n",
    "    environment = SokobanEnv()\n",
    "\n",
    "    print(\"Training Dynamic Programming Agent...\")\n",
    "    init_time = time.time()\n",
    "    dp_agent = DPAgent(environment)\n",
    "    dp_agent.value_iteration()\n",
    "    dp_training_time = time.time() - init_time\n",
    "    print(f\"Dynamic Programming training completed in {dp_training_time:.2f} seconds\")\n",
    "\n",
    "    print(\"\\nTraining Monte Carlo Agent...\")\n",
    "    init_time = time.time()\n",
    "    mc_agent = MCAgent(environment)\n",
    "    mc_agent.learn_policy(episode_count=1000)\n",
    "    mc_training_time = time.time() - init_time\n",
    "    print(f\"Monte Carlo training completed in {mc_training_time:.2f} seconds\")\n",
    "\n",
    "    print(\"\\nEvaluating Dynamic Programming Agent...\")\n",
    "    dp_avg_reward, dp_avg_steps = agent_assess(environment, dp_agent)\n",
    "    print(f\"Dynamic Programming - Average Reward: {dp_avg_reward:.2f}, Average Steps: {dp_avg_steps:.2f}\")\n",
    "\n",
    "    print(\"\\nEvaluating Monte Carlo Agent...\")\n",
    "    mc_avg_reward, mc_avg_steps = agent_assess(environment, mc_agent)\n",
    "    print(f\"Monte Carlo - Average Reward: {mc_avg_reward:.2f}, Average Steps: {mc_avg_steps:.2f}\")\n",
    "\n",
    "    print(\"\\nComparison:\")\n",
    "    print(f\"Training Time - DP: {dp_training_time:.2f}s, MC: {mc_training_time:.2f}s\")\n",
    "    print(f\"Average Reward - DP: {dp_avg_reward:.2f}, MC: {mc_avg_reward:.2f}\")\n",
    "    print(f\"Average Steps - DP: {dp_avg_steps:.2f}, MC: {mc_avg_steps:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "undefined.undefined.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
