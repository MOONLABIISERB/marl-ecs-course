{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTING DYNAMIC PROGRAMMING - VALUE ITERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qgi4rVOSOLVs",
    "outputId": "f7cfb347-4199-4a63-afdd-33546d9de0a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Agent at (1, 2), Box at (4, 3), Value: 32.61625379000001, Best Action: DOWN\n",
      "Step 2: Agent at (2, 2), Box at (4, 3), Value: 37.35139310000001, Best Action: DOWN\n",
      "Step 3: Agent at (3, 2), Box at (4, 3), Value: 42.61265900000001, Best Action: RIGHT\n",
      "Step 4: Agent at (3, 3), Box at (4, 3), Value: 48.45851000000001, Best Action: RIGHT\n",
      "Step 5: Agent at (3, 4), Box at (4, 3), Value: 54.95390000000001, Best Action: DOWN\n",
      "Step 6: Agent at (4, 4), Box at (4, 3), Value: 62.171000000000014, Best Action: LEFT\n",
      "Step 7: Agent at (4, 3), Box at (4, 2), Value: 70.19000000000001, Best Action: LEFT\n",
      "Step 8: Agent at (4, 2), Box at (4, 1), Value: 79.10000000000001, Best Action: DOWN\n",
      "Step 9: Agent at (5, 2), Box at (4, 1), Value: 89.0, Best Action: LEFT\n",
      "Step 10: Agent at (5, 1), Box at (4, 1), Value: 100.0, Best Action: UP\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "WIDTH = 6\n",
    "HEIGHT = 7\n",
    "ACTIONS_LIST = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "STEP_REWARD = -1\n",
    "GOAL_REWARD = 100\n",
    "TERMINAL_REWARD = -100\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "layout = np.array([\n",
    "    [1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 1, 1, 1],\n",
    "    [1, 0, 0, 1, 1, 1],\n",
    "    [1, 3, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 2, 0, 1],\n",
    "    [1, 0, 0, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "start_agent = (1, 2)\n",
    "start_box = (4, 3)\n",
    "storage_spot = (3, 1)\n",
    "\n",
    "values = np.zeros((HEIGHT, WIDTH, HEIGHT, WIDTH))\n",
    "\n",
    "def valid(pos):\n",
    "    x, y = pos\n",
    "    return 0 <= x < HEIGHT and 0 <= y < WIDTH and layout[x][y] != 1\n",
    "\n",
    "def make_move(pos, action):\n",
    "    x, y = pos\n",
    "    if action == 'UP':\n",
    "        return (x - 1, y)\n",
    "    elif action == 'DOWN':\n",
    "        return (x + 1, y)\n",
    "    elif action == 'LEFT':\n",
    "        return (x, y - 1)\n",
    "    elif action == 'RIGHT':\n",
    "        return (x, y + 1)\n",
    "\n",
    "def check_terminal(box_pos):\n",
    "    return box_pos == storage_spot\n",
    "\n",
    "def perform_transition(state, action):\n",
    "    agent, box = state\n",
    "    next_agent = make_move(agent, action)\n",
    "\n",
    "    if next_agent == box:\n",
    "        next_box = make_move(box, action)\n",
    "        if valid(next_box):\n",
    "            reward = GOAL_REWARD if next_box == storage_spot else STEP_REWARD\n",
    "            return (next_agent, next_box), reward\n",
    "        else:\n",
    "            return state, TERMINAL_REWARD\n",
    "    elif valid(next_agent):\n",
    "        return (next_agent, box), STEP_REWARD\n",
    "    return state, TERMINAL_REWARD\n",
    "\n",
    "def run_value_iteration(max_iter=1000, tol=1e-3):\n",
    "    for _ in range(max_iter):\n",
    "        delta = 0\n",
    "        for i in range(HEIGHT):\n",
    "            for j in range(WIDTH):\n",
    "                for bi in range(HEIGHT):\n",
    "                    for bj in range(WIDTH):\n",
    "                        if layout[i][j] == 1 or layout[bi][bj] == 1:\n",
    "                            continue\n",
    "                        state = ((i, j), (bi, bj))\n",
    "                        if check_terminal((bi, bj)):\n",
    "                            continue\n",
    "                        old_value = values[i, j, bi, bj]\n",
    "                        best_val = float('-inf')\n",
    "                        for action in ACTIONS_LIST:\n",
    "                            next_state, reward = perform_transition(state, action)\n",
    "                            next_i, next_j = next_state[0]\n",
    "                            next_bi, next_bj = next_state[1]\n",
    "                            current_val = reward + DISCOUNT * values[next_i, next_j, next_bi, next_bj]\n",
    "                            best_val = max(best_val, current_val)\n",
    "                        values[i, j, bi, bj] = best_val\n",
    "                        delta = max(delta, abs(old_value - values[i, j, bi, bj]))\n",
    "        if delta < tol:\n",
    "            break\n",
    "    return values\n",
    "\n",
    "def find_best_action(state):\n",
    "    optimal_action = None\n",
    "    optimal_value = float('-inf')\n",
    "    i, j = state[0]\n",
    "    bi, bj = state[1]\n",
    "    for action in ACTIONS_LIST:\n",
    "        next_state, reward = perform_transition(state, action)\n",
    "        next_i, next_j = next_state[0]\n",
    "        next_bi, next_bj = next_state[1]\n",
    "        value = reward + DISCOUNT * values[next_i, next_j, next_bi, next_bj]\n",
    "        if value > optimal_value:\n",
    "            optimal_value = value\n",
    "            optimal_action = action\n",
    "    return optimal_action, optimal_value\n",
    "\n",
    "def run_simulation():\n",
    "    agent = start_agent\n",
    "    box = start_box\n",
    "    state = (agent, box)\n",
    "    step = 0\n",
    "    while not check_terminal(state[1]):\n",
    "        step += 1\n",
    "        action, val = find_best_action(state)\n",
    "        print(f\"Step {step}: Agent at {state[0]}, Box at {state[1]}, Value: {val}, Best Action: {action}\")\n",
    "        state, _ = perform_transition(state, action)\n",
    "\n",
    "final_values = run_value_iteration()\n",
    "run_simulation()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTING MONTE CARLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4eJjtqmVz5qi",
    "outputId": "03b167e5-b778-4ecd-db7f-7a0f2fb63ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 2: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 3: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 4: State: Agent at (1, 2), Box at (4, 3), Best Action: LEFT\n",
      "Step 5: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 6: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 7: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 8: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 9: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 10: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 11: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 12: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 13: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 14: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 15: State: Agent at (1, 2), Box at (4, 3), Best Action: RIGHT\n",
      "Step 16: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 17: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 18: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 19: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 20: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 21: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 22: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 23: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 24: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 25: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 26: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 27: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 28: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 29: State: Agent at (1, 2), Box at (4, 3), Best Action: LEFT\n",
      "Step 30: State: Agent at (1, 2), Box at (4, 3), Best Action: LEFT\n",
      "Step 31: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 32: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 33: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 34: State: Agent at (1, 2), Box at (4, 3), Best Action: RIGHT\n",
      "Step 35: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 36: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 37: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 38: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 39: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 40: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 41: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 42: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 43: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 44: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 45: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 46: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 47: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 48: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 49: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 50: State: Agent at (1, 2), Box at (4, 3), Best Action: LEFT\n",
      "Step 51: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 52: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 53: State: Agent at (1, 2), Box at (4, 3), Best Action: DOWN\n",
      "Step 54: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 55: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 56: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 57: State: Agent at (1, 2), Box at (4, 3), Best Action: RIGHT\n",
      "Step 58: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 59: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 60: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 61: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 62: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 63: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 64: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 65: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 66: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 67: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 68: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 69: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 70: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 71: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 72: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 73: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 74: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 75: State: Agent at (1, 2), Box at (4, 3), Best Action: RIGHT\n",
      "Step 76: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 77: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 78: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 79: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 80: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 81: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 82: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 83: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 84: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 85: State: Agent at (1, 2), Box at (4, 3), Best Action: RIGHT\n",
      "Step 86: State: Agent at (1, 2), Box at (4, 3), Best Action: RIGHT\n",
      "Step 87: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 88: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 89: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 90: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 91: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 92: State: Agent at (1, 2), Box at (4, 3), Best Action: DOWN\n",
      "Step 93: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 94: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 95: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 96: State: Agent at (1, 2), Box at (4, 3), Best Action: RIGHT\n",
      "Step 97: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 98: State: Agent at (1, 2), Box at (4, 3), Best Action: UP\n",
      "Step 99: State: Agent at (1, 2), Box at (4, 3), Best Action: LEFT\n",
      "Step 100: State: Agent at (1, 2), Box at (4, 3), Best Action: RIGHT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "GRID_WIDTH, GRID_HEIGHT = 6, 7\n",
    "ACTIONS = ['UP', 'DOWN', 'LEFT', 'RIGHT']\n",
    "REWARD_STEP, REWARD_GOAL, REWARD_TERMINAL = -1, 100, -100\n",
    "GAMMA, EPSILON = 0.9, 0.2\n",
    "MAX_STEPS_PER_EPISODE = 100\n",
    "\n",
    "grid = np.array([\n",
    "    [1, 1, 1, 1, 1, 1],\n",
    "    [1, 0, 0, 1, 1, 1],\n",
    "    [1, 0, 0, 1, 1, 1],\n",
    "    [1, 3, 0, 0, 0, 1],\n",
    "    [1, 0, 0, 2, 0, 1],\n",
    "    [1, 0, 0, 1, 1, 1],\n",
    "    [1, 1, 1, 1, 1, 1]\n",
    "])\n",
    "\n",
    "agent_start, box_start = (1, 2), (4, 3)\n",
    "storage_position = (3, 1)\n",
    "\n",
    "Q, returns_sum, returns_count = {}, {}, {}\n",
    "\n",
    "for i in range(GRID_HEIGHT):\n",
    "    for j in range(GRID_WIDTH):\n",
    "        for bi in range(GRID_HEIGHT):\n",
    "            for bj in range(GRID_WIDTH):\n",
    "                if grid[i][j] == 1 or grid[bi][bj] == 1:\n",
    "                    continue\n",
    "                state = ((i, j), (bi, bj))\n",
    "                Q[state] = {action: 0 for action in ACTIONS}\n",
    "                returns_sum[state] = {action: 0 for action in ACTIONS}\n",
    "                returns_count[state] = {action: 0 for action in ACTIONS}\n",
    "\n",
    "def is_valid(pos):\n",
    "    return 0 <= pos[0] < GRID_HEIGHT and 0 <= pos[1] < GRID_WIDTH and grid[pos[0]][pos[1]] != 1\n",
    "\n",
    "def move(pos, action):\n",
    "    if action == 'UP':\n",
    "        return (pos[0] - 1, pos[1])\n",
    "    if action == 'DOWN':\n",
    "        return (pos[0] + 1, pos[1])\n",
    "    if action == 'LEFT':\n",
    "        return (pos[0], pos[1] - 1)\n",
    "    return (pos[0], pos[1] + 1)\n",
    "\n",
    "def is_terminal(box_pos):\n",
    "    return box_pos == storage_position\n",
    "\n",
    "visited_states = set()\n",
    "\n",
    "def transition_with_penalty(state, action):\n",
    "    global visited_states\n",
    "    agent_pos, box_pos = state\n",
    "    new_agent_pos = move(agent_pos, action)\n",
    "\n",
    "    if state in visited_states:\n",
    "        return state, REWARD_STEP - 5\n",
    "\n",
    "    visited_states.add(state)\n",
    "\n",
    "    if new_agent_pos == box_pos:\n",
    "        new_box_pos = move(box_pos, action)\n",
    "        if is_valid(new_box_pos):\n",
    "            return (new_agent_pos, new_box_pos), REWARD_GOAL if new_box_pos == storage_position else REWARD_STEP\n",
    "        return state, REWARD_TERMINAL\n",
    "    if is_valid(new_agent_pos):\n",
    "        return (new_agent_pos, box_pos), REWARD_STEP\n",
    "    return state, REWARD_TERMINAL\n",
    "\n",
    "def epsilon_greedy_policy(state):\n",
    "    if random.uniform(0, 1) < EPSILON:\n",
    "        return random.choice(ACTIONS)\n",
    "    return max(Q[state], key=Q[state].get)\n",
    "\n",
    "def detect_and_break_loop(episode):\n",
    "    recent_states = set()\n",
    "    loop_counter = 0\n",
    "    for state, action, reward in episode:\n",
    "        if state in recent_states:\n",
    "            loop_counter += 1\n",
    "        else:\n",
    "            loop_counter = 0\n",
    "            recent_states.clear()\n",
    "        recent_states.add(state)\n",
    "        if loop_counter >= 10:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def generate_episode_with_step_limit():\n",
    "    episode, state, steps = [], (agent_start, box_start), 0\n",
    "    visited_states.clear()\n",
    "\n",
    "    while not is_terminal(state[1]) and steps < MAX_STEPS_PER_EPISODE:\n",
    "        action = epsilon_greedy_policy(state)\n",
    "        next_state, reward = transition_with_penalty(state, action)\n",
    "        episode.append((state, action, reward))\n",
    "        state, steps = next_state, steps + 1\n",
    "\n",
    "    return episode\n",
    "\n",
    "def monte_carlo_first_visit(episodes=1000):\n",
    "    for _ in range(episodes):\n",
    "        episode = generate_episode_with_step_limit()\n",
    "        if detect_and_break_loop(episode):\n",
    "            continue\n",
    "\n",
    "        visited, G = set(), 0\n",
    "\n",
    "        for step in reversed(episode):\n",
    "            state, action, reward = step\n",
    "            G = reward + GAMMA * G\n",
    "            if (state, action) not in visited:\n",
    "                visited.add((state, action))\n",
    "                returns_sum[state][action] += G\n",
    "                returns_count[state][action] += 1\n",
    "                Q[state][action] = returns_sum[state][action] / returns_count[state][action]\n",
    "\n",
    "def simulate():\n",
    "    state = (agent_start, box_start)\n",
    "    step_count = 0\n",
    "\n",
    "    while not is_terminal(state[1]) and step_count < MAX_STEPS_PER_EPISODE:\n",
    "        step_count += 1\n",
    "        action = epsilon_greedy_policy(state)\n",
    "        next_state, _ = transition_with_penalty(state, action)\n",
    "        print(f\"Step {step_count}: State: Agent at {state[0]}, Box at {state[1]}, Best Action: {action}\")\n",
    "        state = next_state\n",
    "\n",
    "monte_carlo_first_visit(episodes=1000)\n",
    "simulate()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
