# Assignment-3

## Training Performance
- Agents successfully learned optimal policies through Q-learning, with rewards stabilizing over 500 episodes.
- All agents reached their goals, navigating obstacles and avoiding collisions.
- Minimum maximum time for agents to complete their paths: **16 steps**.

Agent 1 path: [(1, 1), (1, 2), (2, 2), (3, 2), (3, 3), (3, 4), (4, 4), (5, 4), (5, 5), (5, 6), (6, 6), (6, 7), (6, 8), (5, 8)]  
Agent 2 path: [(8, 1), (8, 2), (7, 2), (7, 3), (6, 3), (5, 3), (5, 4), (5, 5), (4, 5), (3, 5), (3, 6), (2, 6), (1, 6), (1, 5)]  
Agent 3 path: [(8, 8), (7, 8), (7, 7), (7, 6), (6, 6), (6, 5), (6, 4), (6, 3), (6, 2), (6, 1), (5, 1)]  
Agent 4 path: [(1, 8), (1, 7), (2, 7), (3, 7), (3, 6), (4, 6), (4, 5), (4, 4), (4, 3), (5, 3), (6, 3), (7, 3), (8, 3), (8, 4)]  

## Visualizations

![Graph Visualization](https://github.com/MOONLABIISERB/marl-ecs-course/blob/gavit_20114/Assignment-3/graph.png)

        ![Map Visualization](https://github.com/MOONLABIISERB/marl-ecs-course/blob/gavit_20114/Assignment-3/map.png)
