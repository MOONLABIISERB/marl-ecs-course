# -*- coding: utf-8 -*-
"""tspsol.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fqSrAfJEIoZtBvNyxerben_veN1zfWSb
"""

import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym
from collections import defaultdict
from typing import Optional, Tuple, Dict
from numpy import typing as npt

class ModTSP(gym.Env):
    """Travelling Salesman Problem (TSP) RL environment for maximizing profits.

    The agent navigates a set of targets based on precomputed distances. It aims to visit
    all targets so maximize profits. The profits decay with time.
    """

    def __init__(
        self,
        num_targets: int = 10,
        max_area: int = 15,
        shuffle_time: int = 10,
        seed: int = 42,
    ) -> None:
        """Initialize the TSP environment.

        Args:
            num_targets (int): No. of targets the agent needs to visit.
            max_area (int): Max. Square area where the targets are defined.
            shuffle_time (int): No. of episodes after which the profits ar to be shuffled.
            seed (int): Random seed for reproducibility.
        """
        super().__init__()

        np.random.seed(seed)

        self.steps: int = 0
        self.episodes: int = 0

        self.shuffle_time: int = shuffle_time
        self.num_targets: int = num_targets

        self.max_steps: int = num_targets
        self.max_area: int = max_area

        self.locations: npt.NDArray[np.float32] = self._generate_points(self.num_targets)
        self.distances: npt.NDArray[np.float32] = self._calculate_distances(self.locations)

        # Initialize profits for each target
        self.initial_profits: npt.NDArray[np.float32] = np.arange(1, self.num_targets + 1, dtype=np.float32) * 10.0
        self.current_profits: npt.NDArray[np.float32] = self.initial_profits.copy()

        # Observation Space : {current loc (loc), target flag - visited or not, current profits, dist_array (distances), coordintates (locations)}
        self.obs_low = np.concatenate(
            [
                np.array([0], dtype=np.float32),  # Current location
                np.zeros(self.num_targets, dtype=np.float32),  # Check if targets were visited or not
                np.zeros(self.num_targets, dtype=np.float32),  # Array of all current profits values
                np.zeros(self.num_targets, dtype=np.float32),  # Distance to each target from current location
                np.zeros(2 * self.num_targets, dtype=np.float32),  # Cooridinates of all targets
            ]
        )

        self.obs_high = np.concatenate(
            [
                np.array([self.num_targets], dtype=np.float32),  # Current location
                np.ones(self.num_targets, dtype=np.float32),  # Check if targets were visited or not
                100 * np.ones(self.num_targets, dtype=np.float32),  # Array of all current profits values
                2 * self.max_area * np.ones(self.num_targets, dtype=np.float32),  # Distance to each target from current location
                self.max_area * np.ones(2 * self.num_targets, dtype=np.float32),  # Cooridinates of all targets
            ]
        )

        # Action Space : {next_target}
        self.observation_space = gym.spaces.Box(low=self.obs_low, high=self.obs_high)
        self.action_space = gym.spaces.Discrete(self.num_targets)

    def reset(
        self,
        *,
        seed: Optional[int] = None,
        options: Optional[dict] = None,
    ) -> Tuple[np.ndarray, Dict[str, None]]:
        """Reset the environment to the initial state.

        Args:
            seed (Optional[int], optional): Seed to reset the environment. Defaults to None.
            options (Optional[dict], optional): Additional reset options. Defaults to None.

        Returns:
            Tuple[np.ndarray, Dict[str, None]]: The initial state of the environment and an empty info dictionary.
        """
        self.steps: int = 0
        self.episodes += 1

        self.loc: int = 0
        self.visited_targets: npt.NDArray[np.float32] = np.zeros(self.num_targets)
        self.current_profits = self.initial_profits.copy()
        self.dist: List = self.distances[self.loc]

        if self.shuffle_time % self.episodes == 0:
            np.random.shuffle(self.initial_profits)

        state = np.concatenate(
            (
                np.array([self.loc]),
                self.visited_targets,
                self.initial_profits,
                np.array(self.dist),
                np.array(self.locations).reshape(-1),
            ),
            dtype=np.float32,
        )
        return state, {}

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict[str, None]]:
        """Take an action (move to the next target).

        Args:
            action (int): The index of the next target to move to.

        Returns:
            Tuple[np.ndarray, float, bool, bool, Dict[str, None]]:
                - The new state of the environment.
                - The reward for the action.
                - A boolean indicating whether the episode has terminated.
                - A boolean indicating if the episode is truncated.
                - An empty info dictionary.
        """
        self.steps += 1
        past_loc = self.loc
        next_loc = action

        self.current_profits -= self.distances[past_loc, next_loc]
        reward = self._get_rewards(next_loc)

        self.visited_targets[next_loc] = 1

        next_dist = self.distances[next_loc]
        terminated = bool(self.steps == self.max_steps)
        truncated = False

        next_state = np.concatenate(
            [
                np.array([next_loc]),
                self.visited_targets,
                self.current_profits,
                next_dist,
                np.array(self.locations).reshape(-1),
            ],
            dtype=np.float32,
        )

        self.loc, self.dist = next_loc, next_dist
        return (next_state, reward, terminated, truncated, {})

    def _generate_points(self, num_points: int) -> npt.NDArray[np.float32]:
        """Generate random 2D points representing target locations.

        Args:
            num_points (int): Number of points to generate.

        Returns:
            np.ndarray: Array of 2D coordinates for each target.
        """
        return np.random.uniform(low=0, high=self.max_area, size=(num_points, 2)).astype(np.float32)

    def _calculate_distances(self, locations: npt.NDArray[np.float32]) -> npt.NDArray[np.float32]:
        """Calculate the distance matrix between all target locations.

        Args:
            locations: List of 2D target locations.

        Returns:
            np.ndarray: Matrix of pairwise distances between targets.
        """
        n = len(locations)

        distances = np.zeros((n, n), dtype=np.float32)
        for i in range(n):
            for j in range(n):
                distances[i, j] = np.linalg.norm(locations[i] - locations[j])
        return distances

    def _get_rewards(self, next_loc: int) -> float:
        """Calculate the reward based on the distance traveled, however if a target gets visited again then it incurs a high penalty.

        Args:
            next_loc (int): Next location of the agent.

        Returns:
            float: Reward based on the travel distance between past and next locations, or negative reward if repeats visit.
        """
        reward = self.current_profits[next_loc] if not self.visited_targets[next_loc] else -1e4
        return float(reward)

def sarsa(env, num_episodes, alpha=0.1, gamma=0.99, epsilon=0.1):
    # Initialize Q-table as a dictionary of dictionaries
    Q = defaultdict(lambda: np.zeros(env.num_targets))

    # Track cumulative rewards per episode for plotting
    episode_rewards = []

    for ep in range(num_episodes):
        # Reset environment for a new episode
        state, _ = env.reset()
        state_tuple = tuple(state.astype(int))  # Converting state to a hashable type
        action = select_action(Q, state_tuple, epsilon, env)
        cumulative_reward = 0

        for _ in range(env.max_steps):
            # Take the selected action, get next state and reward
            next_state, reward, terminated, truncated, _ = env.step(action)
            next_state_tuple = tuple(next_state.astype(int))
            cumulative_reward += reward

            # Select next action using epsilon-greedy policy
            next_action = select_action(Q, next_state_tuple, epsilon, env)

            # SARSA update rule
            td_target = reward + gamma * Q[next_state_tuple][next_action]
            td_error = td_target - Q[state_tuple][action]
            Q[state_tuple][action] += alpha * td_error

            # Transition to the next state and action
            state_tuple = next_state_tuple
            action = next_action

            # End the episode if terminated
            if terminated or truncated:
                break

        episode_rewards.append(cumulative_reward)

        # Print every 100th episode
        if (ep + 1) % 100 == 0:
            print(f"Episode {ep + 1}/{num_episodes}, Total Reward: {cumulative_reward}, Epsilon: {epsilon}")

        # Decay epsilon over time to reduce exploration
        epsilon = max(0.00005, epsilon * 0.9999)

    return Q, episode_rewards

def select_action(Q, state, epsilon, env):
    """Epsilon-greedy action selection"""

    if np.random.random() < epsilon:
        return env.action_space.sample()  # Explore: random action
    else:
        return np.argmax(Q[state])  # Exploit: choose action with highest Q-value

def plot_rewards(episode_rewards, smoothing_window=1000):
    # Apply moving average to smooth the rewards
    smoothed_rewards = np.convolve(episode_rewards,
                                   np.ones(smoothing_window) / smoothing_window,
                                   mode='valid')

    # Plot both original and smoothed rewards
    plt.figure(figsize=(10, 6))
    plt.plot(episode_rewards, label="Original", alpha=0.3)
    plt.plot(range(smoothing_window - 1, len(episode_rewards)), smoothed_rewards, label="Smoothed", color='r')

    plt.xlabel('Episodes')
    plt.ylabel('Cumulative Reward')
    plt.title('SARSA Training: Episode vs Cumulative Reward')
    plt.legend()
    plt.show()

# Main function to run the SARSA algorithm
def main():
    num_targets = 10
    env = ModTSP(num_targets)

    num_episodes = 100000
    alpha = 0.01  # Learning rate
    gamma = 0.99  # Discount factor
    epsilon = 0.1  # Initial exploration probability

    Q, episode_rewards = sarsa(env, num_episodes, alpha, gamma, epsilon)

    plot_rewards(episode_rewards)

if __name__ == "__main__":
    main()

