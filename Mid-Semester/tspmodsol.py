# -*- coding: utf-8 -*-
"""tspmodsol.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aB4ipQBFE56b8IPs7Fq1Y6FBg-I1QqFk
"""

import numpy as np
import matplotlib.pyplot as plt
import gymnasium as gym
from collections import defaultdict
from typing import Optional, Tuple, Dict, List
from numpy import typing as npt

class ModTSP(gym.Env):
    """Travelling Salesman Problem (TSP) RL environment for maximizing profits.

    The agent navigates a set of targets based on precomputed distances. It aims to visit
    all targets so maximize profits. The profits decay with time.
    """

    def __init__(
        self,
        num_targets: int = 10,
        max_area: int = 15,
        shuffle_time: int = 10,
        seed: int = 45,
    ) -> None:
        """Initialize the TSP environment."""
        super().__init__()

        np.random.seed(seed)

        self.steps: int = 0
        self.episodes: int = 0

        self.shuffle_time: int = shuffle_time
        self.num_targets: int = num_targets

        self.max_steps: int = num_targets
        self.max_area: int = max_area

        self.locations: npt.NDArray[np.float32] = self._generate_points(self.num_targets)
        self.distances: npt.NDArray[np.float32] = self._calculate_distances(self.locations)

        # Initialize profits for each target
        self.initial_profits: npt.NDArray[np.float32] = np.arange(1, self.num_targets + 1, dtype=np.float32) * 10.0
        self.current_profits: npt.NDArray[np.float32] = self.initial_profits.copy()

        # Observation Space : {current loc (loc), target flag - visited or not, current profits, dist_array (distances), coordintates (locations)}
        self.obs_low = np.concatenate(
            [
                np.array([0], dtype=np.float32),  # Current location
                np.zeros(self.num_targets, dtype=np.float32),  # Check if targets were visited or not
                np.zeros(self.num_targets, dtype=np.float32),  # Array of all current profits values
                np.zeros(self.num_targets, dtype=np.float32),  # Distance to each target from current location
                np.zeros(2 * self.num_targets, dtype=np.float32),  # Cooridinates of all targets
            ]
        )

        self.obs_high = np.concatenate(
            [
                np.array([self.num_targets], dtype=np.float32),  # Current location
                np.ones(self.num_targets, dtype=np.float32),  # Check if targets were visited or not
                100 * np.ones(self.num_targets, dtype=np.float32),  # Array of all current profits values
                2 * self.max_area * np.ones(self.num_targets, dtype=np.float32),  # Distance to each target from current location
                self.max_area * np.ones(2 * self.num_targets, dtype=np.float32),  # Cooridinates of all targets
            ]
        )

        # Action Space : {next_target}
        self.observation_space = gym.spaces.Box(low=self.obs_low, high=self.obs_high)
        self.action_space = gym.spaces.Discrete(self.num_targets)

    def reset(
        self,
        *,
        seed: Optional[int] = None,
        options: Optional[dict] = None,
    ) -> Tuple[np.ndarray, Dict[str, None]]:
        """Reset the environment to the initial state."""
        self.steps: int = 0
        self.episodes += 1

        self.loc: int = 0
        self.visited_targets: npt.NDArray[np.float32] = np.zeros(self.num_targets)
        self.current_profits = self.initial_profits.copy()
        self.dist: List = self.distances[self.loc]

        if self.shuffle_time % self.episodes == 0:
            np.random.shuffle(self.initial_profits)

        state = np.concatenate(
            (
                np.array([self.loc]),
                self.visited_targets,
                self.initial_profits,
                np.array(self.dist),
                np.array(self.locations).reshape(-1),
            ),
            dtype=np.float32,
        )
        return state, {}

    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict[str, None]]:
        """Take an action (move to the next target)."""
        self.steps += 1
        past_loc = self.loc
        next_loc = action

        self.current_profits -= self.distances[past_loc, next_loc]
        reward = self._get_rewards(next_loc)

        self.visited_targets[next_loc] = 1

        next_dist = self.distances[next_loc]
        terminated = bool(self.steps == self.max_steps)
        truncated = False

        next_state = np.concatenate(
            [
                np.array([next_loc]),
                self.visited_targets,
                self.current_profits,
                next_dist,
                np.array(self.locations).reshape(-1),
            ],
            dtype=np.float32,
        )

        self.loc, self.dist = next_loc, next_dist
        return (next_state, reward, terminated, truncated, {})

    def _generate_points(self, num_points: int) -> npt.NDArray[np.float32]:
        """Generate random 2D points representing target locations."""
        return np.random.uniform(low=0, high=self.max_area, size=(num_points, 2)).astype(np.float32)

    def _calculate_distances(self, locations: npt.NDArray[np.float32]) -> npt.NDArray[np.float32]:
        """Calculate the distance matrix between all target locations."""
        n = len(locations)
        distances = np.zeros((n, n), dtype=np.float32)
        for i in range(n):
            for j in range(n):
                distances[i, j] = np.linalg.norm(locations[i] - locations[j])
        return distances

    def _get_rewards(self, next_loc: int) -> float:
        """Calculate the reward."""
        reward = self.current_profits[next_loc] if not self.visited_targets[next_loc] else -1e4
        return float(reward)


def sarsa(env, num_episodes, alpha=0.01, gamma=0.99, epsilon=0.1):
    Q = defaultdict(lambda: np.zeros(env.num_targets))
    episode_rewards = []

    for ep in range(num_episodes):
        state, _ = env.reset()
        state_tuple = tuple(state.astype(int))
        action = select_action(Q, state_tuple, epsilon, env)
        cumulative_reward = 0

        for _ in range(env.max_steps):
            next_state, reward, terminated, truncated, _ = env.step(action)
            next_state_tuple = tuple(next_state.astype(int))
            cumulative_reward += reward

            next_action = select_action(Q, next_state_tuple, epsilon, env)

            td_target = reward + gamma * Q[next_state_tuple][next_action]
            td_error = td_target - Q[state_tuple][action]
            Q[state_tuple][action] += alpha * td_error

            state_tuple = next_state_tuple
            action = next_action

            if terminated or truncated:
                break

        episode_rewards.append(cumulative_reward)

        if (ep + 1) % 100 == 0:
            print(f"Episode {ep + 1}/{num_episodes}, Total Reward: {cumulative_reward}, Epsilon: {epsilon}")

        epsilon = max(0.00005, epsilon * 0.9999)

    return Q, episode_rewards


def select_action(Q, state, epsilon, env):
    if np.random.random() < epsilon:
        return env.action_space.sample()
    else:
        return np.argmax(Q[state])


def get_optimal_path(Q, env):
    state, _ = env.reset()
    path = [env.locations[0]]
    total_reward = 0
    state_tuple = tuple(state.astype(int))

    actions_taken = []
    rewards_collected = []

    for _ in range(env.max_steps):
        action = np.argmax(Q[state_tuple])

        next_state, reward, terminated, truncated, _ = env.step(action)
        path.append(env.locations[action])

        total_reward += reward
        actions_taken.append(action)
        rewards_collected.append(reward)

        if terminated or truncated:
            break

        state_tuple = tuple(next_state.astype(int))

    return path, total_reward, actions_taken, rewards_collected


def print_path_details(actions, rewards):
    print("Actions taken (indices):", actions)
    print("Rewards for each action:", rewards)


def plot_rewards(episode_rewards, smoothing_window=1000):
    smoothed_rewards = np.convolve(episode_rewards,
                                   np.ones(smoothing_window) / smoothing_window,
                                   mode='valid')

    plt.figure(figsize=(10, 6))
    plt.plot(episode_rewards, label="Original", alpha=0.3)
    plt.plot(range(smoothing_window - 1, len(episode_rewards)), smoothed_rewards, label="Smoothed", color='r')

    plt.xlabel('Episodes')
    plt.ylabel('Cumulative Reward')
    plt.title('SARSA Training: Episode vs Cumulative Reward')
    plt.legend()
    plt.show()


def plot_optimal_path(path, env):
    locations = np.array(env.locations)
    path = np.array(path)

    plt.figure(figsize=(10, 6))
    plt.scatter(locations[:, 0], locations[:, 1], color='blue', label="Targets")
    plt.plot(path[:, 0], path[:, 1], color='red', marker='o', label="Optimal Path")

    for i, loc in enumerate(locations):
        plt.text(loc[0], loc[1], str(i), fontsize=12, color='black')

    plt.xlabel('X-coordinate')
    plt.ylabel('Y-coordinate')
    plt.title('Optimal Path Traversed')
    plt.legend()
    plt.grid(True)
    plt.show()


# Main SARSA training loop and plotting
env = ModTSP(num_targets=10)
num_episodes = 100000
Q, episode_rewards = sarsa(env, num_episodes)

# Plot episode rewards
plot_rewards(episode_rewards)

# Extract the optimal path and plot it
optimal_path, total_reward, actions_taken, rewards_collected = get_optimal_path(Q, env)
print(f"Total reward for optimal path: {total_reward}")
print_path_details(actions_taken, rewards_collected)
plot_optimal_path(optimal_path, env)

