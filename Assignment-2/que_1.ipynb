{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tsp import TSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSP_DP:\n",
    "    def __init__(self, env, gamma=0.7, theta=1e-6):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.theta = theta\n",
    "        self.value_table = np.zeros((env.num_targets,))  # Value for each target (state)\n",
    "\n",
    "    def value_iteration(self):\n",
    "        \"\"\"Performs value iteration to calculate the optimal value function.\"\"\"\n",
    "        while True:\n",
    "            delta = 0\n",
    "            for state_index in range(self.env.num_targets):\n",
    "                # Store current value of the state\n",
    "                v = self.value_table[state_index]\n",
    "\n",
    "                # Calculate the action values for all possible actions from this state\n",
    "                action_values = self._get_action_values(state_index)\n",
    "\n",
    "                # Select the best action value\n",
    "                best_action_value = max(action_values)\n",
    "\n",
    "                # Update the value table for this state\n",
    "                self.value_table[state_index] = best_action_value\n",
    "\n",
    "                # Calculate the maximum difference for convergence\n",
    "                delta = max(delta, abs(v - best_action_value))\n",
    "\n",
    "            # If the difference is small enough, we assume convergence\n",
    "            if delta < self.theta:\n",
    "                break\n",
    "\n",
    "    def _get_action_values(self, state_index):\n",
    "        \"\"\"Helper function to calculate the value of all actions from a given state.\"\"\"\n",
    "        action_values = []\n",
    "        for action in range(self.env.num_targets):\n",
    "            # We simulate taking the action by calculating the reward and expected value\n",
    "            current_state = self.env.locations[state_index]\n",
    "            next_state = self.env.locations[action]\n",
    "            \n",
    "            # Get the distance between the current state and the action (next state)\n",
    "            distance = self.env.distances[state_index, action]\n",
    "            \n",
    "            # Reward is negative distance to minimize the travel cost\n",
    "            reward = -distance\n",
    "            \n",
    "            # Calculate the action value: immediate reward + discounted future value\n",
    "            action_value = reward + self.gamma * self.value_table[action]\n",
    "            action_values.append(action_value)\n",
    "\n",
    "        return action_values\n",
    "\n",
    "    def get_policy(self):\n",
    "        \"\"\"Extract the optimal policy based on the value table.\"\"\"\n",
    "        policy = np.zeros(self.env.num_targets, dtype=int)\n",
    "        for state_index in range(self.env.num_targets):\n",
    "            # For each state, calculate the best action\n",
    "            action_values = self._get_action_values(state_index)\n",
    "            best_action = np.argmax(action_values)\n",
    "            policy[state_index] = best_action\n",
    "        return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class TSP_MC:\n",
    "    def __init__(self, env, gamma=0.9):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.returns_sum = np.zeros((env.num_targets, env.num_targets))\n",
    "        self.returns_count = np.zeros((env.num_targets, env.num_targets))\n",
    "        self.value_table = np.zeros(env.num_targets)\n",
    "        self.policy = np.random.choice(env.num_targets, env.num_targets)  # Random initial policy\n",
    "\n",
    "    def epsilon_greedy_policy(self, state, epsilon=0.1, decay=0.999):\n",
    "        \"\"\"Select an action using an epsilon-greedy policy.\"\"\"\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.random.choice(self.env.num_targets)  # Explore: random action\n",
    "        else:\n",
    "            return self.policy[state]  # Exploit: greedy action\n",
    "\n",
    "    def generate_episode(self, epsilon=0.1):\n",
    "        \"\"\"Generate an episode using epsilon-greedy action selection, ensuring all targets are visited.\"\"\"\n",
    "        episode = []\n",
    "        state, _ = self.env.reset()\n",
    "        visited_targets = set()\n",
    "        \n",
    "        for _ in range(self.env.num_targets):\n",
    "            action = self.epsilon_greedy_policy(int(state[0]), epsilon)\n",
    "            next_state, reward, done, _, _ = self.env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            \n",
    "            visited_targets.add(int(state[0]))  # Add the target to visited set\n",
    "            state = next_state\n",
    "            \n",
    "            # End only if all targets have been visited\n",
    "            if len(visited_targets) == self.env.num_targets:\n",
    "                break\n",
    "        \n",
    "        return episode\n",
    "\n",
    "    def first_visit_mc(self, episodes=5000, epsilon=0.1, decay=0.999):\n",
    "        \"\"\"Monte Carlo with First-Visit updating.\"\"\"\n",
    "        for _ in range(episodes):\n",
    "            episode = self.generate_episode(epsilon)\n",
    "            visited = set()\n",
    "            G = 0\n",
    "            for t in reversed(range(len(episode))):\n",
    "                state, action, reward = episode[t]\n",
    "                G = self.gamma * G + reward\n",
    "                state_action = (int(state[0]), action)\n",
    "                if state_action not in visited:\n",
    "                    visited.add(state_action)\n",
    "                    self.returns_sum[state_action] += G\n",
    "                    self.returns_count[state_action] += 1\n",
    "                    self.value_table[int(state[0])] = (\n",
    "                        self.returns_sum[state_action] / self.returns_count[state_action]\n",
    "                    )\n",
    "                    \n",
    "            epsilon *= decay\n",
    "            self.update_policy()\n",
    "            \n",
    "    def every_visit_mc(self, episodes=5000, epsilon=0.1, decay=0.999):\n",
    "        \"\"\"Monte Carlo with Every-Visit updating.\"\"\"\n",
    "        for _ in range(episodes):\n",
    "            episode = self.generate_episode(epsilon)\n",
    "            G = 0\n",
    "            for t in reversed(range(len(episode))):\n",
    "                state, action, reward = episode[t]\n",
    "                G = self.gamma * G + reward\n",
    "                state_action = (int(state[0]), action)\n",
    "                self.returns_sum[state_action] += G\n",
    "                self.returns_count[state_action] += 1\n",
    "                self.value_table[int(state[0])] = (\n",
    "                    self.returns_sum[state_action] / self.returns_count[state_action]\n",
    "                )\n",
    "                \n",
    "            epsilon *= decay\n",
    "            self.update_policy()\n",
    "\n",
    "    def update_policy(self):\n",
    "        \"\"\"Update the policy based on the current value table.\"\"\"\n",
    "        for state in range(self.env.num_targets):\n",
    "            action_values = []\n",
    "            for action in range(self.env.num_targets):\n",
    "                # Use the stored value table instead of interacting with the environment\n",
    "                next_state_value = self.value_table[action]\n",
    "                distance = self.env.distances[state, action]\n",
    "                reward = -distance  # Reward is negative distance\n",
    "                action_values.append(reward + self.gamma * next_state_value)\n",
    "            self.policy[state] = np.argmax(action_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Dynamic Programming (Value Iteration) Solution...\n",
      "Optimal Policy from Dynamic Programming (DP): [0 1 2 3 4 5]\n",
      "Value Table from DP: [0. 0. 0. 0. 0. 0.]\n",
      "\n",
      "Running Monte Carlo (First-Visit) Solution...\n",
      "Policy from Monte Carlo (First-Visit): [1 1 1 1 1 1]\n",
      "Value Table from MC (First-Visit): [-33992.57752487 -10737.27231851 -23785.12859689 -19091.83465818\n",
      " -20285.12775162 -21143.81121275]\n",
      "\n",
      "Running Monte Carlo (Every-Visit) Solution...\n",
      "Policy from Monte Carlo (Every-Visit): [2 1 2 2 1 2]\n",
      "Value Table from MC (Every-Visit): [-34064.52352334 -25210.29997131 -25199.47501583 -28182.03728123\n",
      " -26088.30601061 -25338.93792017]\n",
      "\n",
      "Comparison of DP and MC Solutions:\n",
      "DP Policy:  [0 1 2 3 4 5]\n",
      "MC First-Visit Policy:  [1 1 1 1 1 1]\n",
      "MC Every-Visit Policy:  [2 1 2 2 1 2]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the TSP environment\n",
    "num_targets = 6  # You can set this to 50 as per the problem statement\n",
    "env = TSP(num_targets=num_targets, max_area=30, seed=42)\n",
    "\n",
    "# Dynamic Programming (Value Iteration) Solution\n",
    "print(\"Running Dynamic Programming (Value Iteration) Solution...\")\n",
    "dp_solver = TSP_DP(env)\n",
    "dp_solver.value_iteration()\n",
    "dp_policy = dp_solver.get_policy()\n",
    "\n",
    "print(\"Optimal Policy from Dynamic Programming (DP):\", dp_policy)\n",
    "print(\"Value Table from DP:\", dp_solver.value_table)\n",
    "\n",
    "# Monte Carlo (First-Visit) Solution\n",
    "print(\"\\nRunning Monte Carlo (First-Visit) Solution...\")\n",
    "mc_solver_first_visit = TSP_MC(env)\n",
    "mc_solver_first_visit.first_visit_mc(episodes=500)  # You can adjust the number of episodes\n",
    "\n",
    "print(\"Policy from Monte Carlo (First-Visit):\", mc_solver_first_visit.policy)\n",
    "print(\"Value Table from MC (First-Visit):\", mc_solver_first_visit.value_table)\n",
    "\n",
    "# Monte Carlo (Every-Visit) Solution\n",
    "print(\"\\nRunning Monte Carlo (Every-Visit) Solution...\")\n",
    "mc_solver_every_visit = TSP_MC(env)\n",
    "mc_solver_every_visit.every_visit_mc(episodes=500)  # You can adjust the number of episodes\n",
    "\n",
    "print(\"Policy from Monte Carlo (Every-Visit):\", mc_solver_every_visit.policy)\n",
    "print(\"Value Table from MC (Every-Visit):\", mc_solver_every_visit.value_table)\n",
    "\n",
    "# Compare DP and MC policies\n",
    "print(\"\\nComparison of DP and MC Solutions:\")\n",
    "print(\"DP Policy: \", dp_policy)\n",
    "print(\"MC First-Visit Policy: \", mc_solver_first_visit.policy)\n",
    "print(\"MC Every-Visit Policy: \", mc_solver_every_visit.policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
