{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFXFPXa6nopL",
        "outputId": "ba565f59-b0db-41cd-a739-4a97497c46e9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Environment for Travelling Salesman Problem.\"\"\"\n",
        "\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class TSP(gym.Env):\n",
        "    \"\"\"Traveling Salesman Problem (TSP) RL environment for persistent monitoring.\n",
        "\n",
        "    The agent navigates a set of targets based on precomputed distances. It aims to visit\n",
        "    all targets in the least number of steps, with rewards determined by the distance traveled.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_targets: int, max_area: int = 30, seed: int = None) -> None:\n",
        "        \"\"\"Initialize the TSP environment.\n",
        "\n",
        "        Args:\n",
        "            num_targets (int): Number of targets the agent needs to visit.\n",
        "            max_area (int): Max Square area where the targets are defined. Defaults to 30\n",
        "            seed (int, optional): Random seed for reproducibility. Defaults to None.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed=seed)\n",
        "\n",
        "        self.steps: int = 0\n",
        "        self.num_targets: int = num_targets\n",
        "\n",
        "        self.max_steps: int = num_targets\n",
        "        self.max_area: int = max_area\n",
        "\n",
        "        self.locations: np.ndarray = self._generate_points(self.num_targets)\n",
        "        self.distances: np.ndarray = self._calculate_distances(self.locations)\n",
        "\n",
        "        # Observation Space : {current loc (loc), dist_array (distances), coordintates (locations)}\n",
        "        self.obs_low = np.concatenate(\n",
        "            [\n",
        "                np.array([0], dtype=np.float32),\n",
        "                np.zeros(self.num_targets, dtype=np.float32),\n",
        "                np.zeros(2 * self.num_targets, dtype=np.float32),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.obs_high = np.concatenate(\n",
        "            [\n",
        "                np.array([self.num_targets], dtype=np.float32),\n",
        "                2 * self.max_area * np.ones(self.num_targets, dtype=np.float32),\n",
        "                self.max_area * np.ones(2 * self.num_targets, dtype=np.float32),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Action Space : {next_target}\n",
        "        self.observation_space = gym.spaces.Box(low=self.obs_low, high=self.obs_high)\n",
        "        self.action_space = gym.spaces.Discrete(self.num_targets)\n",
        "\n",
        "    def reset(\n",
        "        self,\n",
        "        *,\n",
        "        seed: Optional[int] = None,\n",
        "        options: Optional[dict] = None,\n",
        "    ) -> Tuple[np.ndarray, Dict[str, None]]:\n",
        "        \"\"\"Reset the environment to the initial state.\n",
        "\n",
        "        Args:\n",
        "            seed (Optional[int], optional): Seed to reset the environment. Defaults to None.\n",
        "            options (Optional[dict], optional): Additional reset options. Defaults to None.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, Dict[str, None]]: The initial state of the environment and an empty info dictionary.\n",
        "        \"\"\"\n",
        "        self.steps: int = 0\n",
        "\n",
        "        self.loc: int = 0\n",
        "        self.visited_targets: List = []\n",
        "        self.dist: List = self.distances[self.loc]\n",
        "\n",
        "        state = np.concatenate(\n",
        "            (\n",
        "                np.array([self.loc]),\n",
        "                np.array(self.dist),\n",
        "                np.array(self.locations).reshape(-1),\n",
        "            ),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        return state, {}\n",
        "\n",
        "    def step(\n",
        "        self, action: int\n",
        "    ) -> Tuple[np.ndarray, float, bool, bool, Dict[str, None]]:\n",
        "        \"\"\"Take an action (move to the next target).\n",
        "\n",
        "        Args:\n",
        "            action (int): The index of the next target to move to.\n",
        "\n",
        "        Returns:\n",
        "            Tuple[np.ndarray, float, bool, bool, Dict[str, None]]:\n",
        "                - The new state of the environment.\n",
        "                - The reward for the action.\n",
        "                - A boolean indicating whether the episode has terminated.\n",
        "                - A boolean indicating if the episode is truncated.\n",
        "                - An empty info dictionary.\n",
        "        \"\"\"\n",
        "        self.steps += 1\n",
        "        past_loc = self.loc\n",
        "        next_loc = action\n",
        "\n",
        "        reward = self._get_rewards(past_loc, next_loc)\n",
        "        self.visited_targets.append(next_loc)\n",
        "\n",
        "        next_dist = self.distances[next_loc]\n",
        "        terminated = bool(self.steps == self.max_steps)\n",
        "        truncated = False\n",
        "\n",
        "        next_state = np.concatenate(\n",
        "            [\n",
        "                np.array([next_loc]),\n",
        "                next_dist,\n",
        "                np.array(self.locations).reshape(-1),\n",
        "            ],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "\n",
        "        self.loc, self.dist = next_loc, next_dist\n",
        "        return (next_state, reward, terminated, truncated, {})\n",
        "\n",
        "    def _generate_points(self, num_points: int) -> np.ndarray:\n",
        "        \"\"\"Generate random 2D points representing target locations.\n",
        "\n",
        "        Args:\n",
        "            num_points (int): Number of points to generate.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Array of 2D coordinates for each target.\n",
        "        \"\"\"\n",
        "        points = []\n",
        "        # Generate n random 2D points within the 10x10 grid\n",
        "        while len(points) < num_points:\n",
        "            x = np.random.random() * self.max_area\n",
        "            y = np.random.random() * self.max_area\n",
        "            if [x, y] not in points:\n",
        "                points.append([x, y])\n",
        "\n",
        "        return np.array(points)\n",
        "\n",
        "    def _calculate_distances(self, locations: List) -> float:\n",
        "        \"\"\"Calculate the distance matrix between all target locations.\n",
        "\n",
        "        Args:\n",
        "            locations (List): List of 2D target locations.\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray: Matrix of pairwise distances between targets.\n",
        "        \"\"\"\n",
        "        n = len(locations)\n",
        "\n",
        "        distances = np.zeros((n, n))\n",
        "        for i in range(n):\n",
        "            for j in range(n):\n",
        "                distances[i, j] = np.linalg.norm(locations[i] - locations[j])\n",
        "        return distances\n",
        "\n",
        "    def _get_rewards(self, past_loc: int, next_loc: int) -> float:\n",
        "        \"\"\"Calculate the reward based on the distance traveled, however if a target gets visited again then it incurs a high penalty.\n",
        "\n",
        "        Args:\n",
        "            past_loc (int): Previous location of the agent.\n",
        "            next_loc (int): Next location of the agent.\n",
        "\n",
        "        Returns:\n",
        "            float: Reward based on the travel distance between past and next locations, or negative reward if repeats visit.\n",
        "        \"\"\"\n",
        "        if next_loc not in self.visited_targets:\n",
        "            reward = -self.distances[past_loc][next_loc]\n",
        "        else:\n",
        "            reward = -10000\n",
        "        return reward\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_targets = 50\n",
        "\n",
        "    env = TSP(num_targets)\n",
        "    obs = env.reset()\n",
        "    ep_rets = []\n",
        "\n",
        "    for ep in range(100):\n",
        "        ret = 0\n",
        "        obs = env.reset()\n",
        "        for _ in range(100):\n",
        "            action = (\n",
        "                env.action_space.sample()\n",
        "            )  # You need to replace this with your algorithm that predicts the action.\n",
        "\n",
        "            obs_, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "            ret += reward\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        ep_rets.append(ret)\n",
        "        print(f\"Episode {ep} : {ret}\")\n",
        "\n",
        "    print(np.mean(ep_rets))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAkiNMduSDMy",
        "outputId": "58564245-8354-40bd-9de1-70b8dc064725"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 0 : -130627.18664893821\n",
            "Episode 1 : -180514.35306187256\n",
            "Episode 2 : -170424.04792070037\n",
            "Episode 3 : -180499.15038853214\n",
            "Episode 4 : -160516.74462474248\n",
            "Episode 5 : -200486.27271322315\n",
            "Episode 6 : -150485.29296108687\n",
            "Episode 7 : -140593.1220440809\n",
            "Episode 8 : -210431.3748213466\n",
            "Episode 9 : -140608.58114143155\n",
            "Episode 10 : -170521.38383412457\n",
            "Episode 11 : -180523.60975241387\n",
            "Episode 12 : -160570.50151922947\n",
            "Episode 13 : -190521.42235727346\n",
            "Episode 14 : -210460.71191647908\n",
            "Episode 15 : -180534.45793769782\n",
            "Episode 16 : -180507.9919525675\n",
            "Episode 17 : -210418.90442410234\n",
            "Episode 18 : -160543.68273922364\n",
            "Episode 19 : -150568.98963839532\n",
            "Episode 20 : -180493.50656491696\n",
            "Episode 21 : -190508.64347938471\n",
            "Episode 22 : -170535.66117748973\n",
            "Episode 23 : -190410.551913164\n",
            "Episode 24 : -170506.75282639923\n",
            "Episode 25 : -180511.83450659947\n",
            "Episode 26 : -210393.81399415366\n",
            "Episode 27 : -150504.401913685\n",
            "Episode 28 : -150531.26094496765\n",
            "Episode 29 : -160521.70676653576\n",
            "Episode 30 : -170484.69337478824\n",
            "Episode 31 : -160530.31271675325\n",
            "Episode 32 : -150524.4428262334\n",
            "Episode 33 : -170482.39436913954\n",
            "Episode 34 : -190481.3923577643\n",
            "Episode 35 : -140583.1979280008\n",
            "Episode 36 : -190459.37135386406\n",
            "Episode 37 : -180474.53459469127\n",
            "Episode 38 : -200428.6992271699\n",
            "Episode 39 : -230388.61583624055\n",
            "Episode 40 : -140530.67340405378\n",
            "Episode 41 : -170496.3782105707\n",
            "Episode 42 : -140523.41170931837\n",
            "Episode 43 : -180460.9193437357\n",
            "Episode 44 : -170499.4536685278\n",
            "Episode 45 : -170562.35109333578\n",
            "Episode 46 : -160499.53279867122\n",
            "Episode 47 : -180493.70860347289\n",
            "Episode 48 : -210452.02033093717\n",
            "Episode 49 : -210428.5628274862\n",
            "Episode 50 : -190426.02915303095\n",
            "Episode 51 : -190457.4883847844\n",
            "Episode 52 : -180471.94282139826\n",
            "Episode 53 : -150449.13940832313\n",
            "Episode 54 : -160419.45316813755\n",
            "Episode 55 : -160529.85556012832\n",
            "Episode 56 : -190455.1367745937\n",
            "Episode 57 : -150491.06962710636\n",
            "Episode 58 : -150581.0970591387\n",
            "Episode 59 : -160450.91743666548\n",
            "Episode 60 : -190448.70266164132\n",
            "Episode 61 : -170522.63938369468\n",
            "Episode 62 : -190496.22694868923\n",
            "Episode 63 : -200518.82936122548\n",
            "Episode 64 : -200454.04790209403\n",
            "Episode 65 : -170427.01246940403\n",
            "Episode 66 : -150506.70087371144\n",
            "Episode 67 : -170525.8087398047\n",
            "Episode 68 : -180463.9014370336\n",
            "Episode 69 : -190495.67100821622\n",
            "Episode 70 : -220390.98259835452\n",
            "Episode 71 : -220440.25196866455\n",
            "Episode 72 : -160501.87902906915\n",
            "Episode 73 : -200450.7326093629\n",
            "Episode 74 : -200469.68988647088\n",
            "Episode 75 : -150517.1132620938\n",
            "Episode 76 : -190445.16242983183\n",
            "Episode 77 : -180564.04251499713\n",
            "Episode 78 : -190517.34900285373\n",
            "Episode 79 : -210386.88255633117\n",
            "Episode 80 : -180481.83030717858\n",
            "Episode 81 : -190469.81731055974\n",
            "Episode 82 : -180400.10277460597\n",
            "Episode 83 : -200439.2835894803\n",
            "Episode 84 : -180487.11027828636\n",
            "Episode 85 : -130559.43877162889\n",
            "Episode 86 : -190471.35623633803\n",
            "Episode 87 : -200472.22806746708\n",
            "Episode 88 : -180401.23983134358\n",
            "Episode 89 : -190447.78328579554\n",
            "Episode 90 : -180500.41475939567\n",
            "Episode 91 : -170489.90098331103\n",
            "Episode 92 : -180477.18124562103\n",
            "Episode 93 : -170524.13509131028\n",
            "Episode 94 : -150596.40581604728\n",
            "Episode 95 : -200470.76383747623\n",
            "Episode 96 : -180402.07905279717\n",
            "Episode 97 : -180479.65312157915\n",
            "Episode 98 : -200440.16473355898\n",
            "Episode 99 : -160505.56511513147\n",
            "-177588.52793305274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic Programming"
      ],
      "metadata": {
        "id": "CqtAQwYnRyjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DPSolver:\n",
        "    def __init__(self, env: TSP, learning_rate: float = 0.1, discount_factor: float = 0.9):\n",
        "        self.env = env\n",
        "        self.learning_rate = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "        self.q_table = np.zeros((env.num_targets, env.num_targets))\n",
        "\n",
        "    def train(self, num_episodes: int = 1000):\n",
        "        for episode in range(num_episodes):\n",
        "            state, _ = self.env.reset()\n",
        "            current_target = int(state[0])\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = np.argmax(self.q_table[current_target])\n",
        "                next_state, reward, done, _, _ = self.env.step(action)\n",
        "                next_target = int(next_state[0])\n",
        "\n",
        "                # Q-learning update\n",
        "                best_next_action = np.argmax(self.q_table[next_target])\n",
        "                td_target = reward + self.discount_factor * self.q_table[next_target, best_next_action]\n",
        "                td_error = td_target - self.q_table[current_target, action]\n",
        "                self.q_table[current_target, action] += self.learning_rate * td_error\n",
        "\n",
        "                current_target = next_target\n",
        "\n",
        "            if episode % 100 == 0:\n",
        "                print(f\"Episode {episode} completed\")\n",
        "\n",
        "    def get_optimal_policy(self):\n",
        "        return np.argmax(self.q_table, axis=1)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_targets = 50\n",
        "    env = TSP(num_targets)\n",
        "    solver = DPSolver(env)\n",
        "\n",
        "    print(\"Executing DP\")\n",
        "    solver.train()\n",
        "\n",
        "    optimal_policy = solver.get_optimal_policy()\n",
        "    print(\"Optimal policy:\", optimal_policy)\n",
        "\n",
        "    # Evaluate the policy\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    for _ in range(100):\n",
        "        action = optimal_policy[int(state[0])]\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(f\"Total reward using optimal policy: {total_reward}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6urFyYvn04Y",
        "outputId": "3dc6064d-32a0-4bf9-e5a6-a13c08908d9b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing DP solver\n",
            "Episode 0 completed\n",
            "Episode 100 completed\n",
            "Episode 200 completed\n",
            "Episode 300 completed\n",
            "Episode 400 completed\n",
            "Episode 500 completed\n",
            "Episode 600 completed\n",
            "Episode 700 completed\n",
            "Episode 800 completed\n",
            "Episode 900 completed\n",
            "Optimal policy: [47  0 23 21  5 23 15 39 22  1 46 31 45 14  8 25 41 49  8 46 33 27 34  9\n",
            " 29  9 49 46 12 25 36 46 25 20 27 48 16 27  2 34 35  4  8 21 19 21 43 18\n",
            " 25 37]\n",
            "Total reward using optimal policy: -410107.4259394547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Monte-Carlo"
      ],
      "metadata": {
        "id": "FHJ8yXkjXyOX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "\n",
        "class MCSolver:\n",
        "    def __init__(self, env: TSP, epsilon: float = 0.1, discount_factor: float = 0.9):\n",
        "        self.env = env\n",
        "        self.epsilon = epsilon\n",
        "        self.discount_factor = discount_factor\n",
        "        self.q_table = defaultdict(lambda: np.zeros(env.num_targets))\n",
        "        self.returns = defaultdict(list)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.q_table[state])\n",
        "\n",
        "    def train(self, num_episodes: int = 1000):\n",
        "        for episode in range(num_episodes):\n",
        "            episode_history = []\n",
        "            state, _ = self.env.reset()\n",
        "            state = int(state[0])\n",
        "            done = False\n",
        "\n",
        "            while not done:\n",
        "                action = self.choose_action(state)\n",
        "                next_state, reward, done, _, _ = self.env.step(action)\n",
        "                episode_history.append((state, action, reward))\n",
        "                state = int(next_state[0])\n",
        "\n",
        "            # Calculating returns and update Q-values\n",
        "            G = 0\n",
        "            for state, action, reward in reversed(episode_history):\n",
        "                G = reward + self.discount_factor * G\n",
        "                self.returns[(state, action)].append(G)\n",
        "                self.q_table[state][action] = np.mean(self.returns[(state, action)])\n",
        "\n",
        "            if episode % 100 == 0:\n",
        "                print(f\"Episode {episode} completed\")\n",
        "\n",
        "    def get_optimal_policy(self):\n",
        "        return {state: np.argmax(actions) for state, actions in self.q_table.items()}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    num_targets = 50\n",
        "    env = TSP(num_targets)\n",
        "    solver = MCSolver(env)\n",
        "\n",
        "    print(\"Executing MC\")\n",
        "    solver.train()\n",
        "\n",
        "    optimal_policy = solver.get_optimal_policy()\n",
        "    print(\"Optimal policy:\", optimal_policy)\n",
        "\n",
        "    # Evaluate the policy\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    for _ in range(100):\n",
        "        action = optimal_policy.get(int(state[0]), env.action_space.sample())\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    print(f\"Total reward using optimal policy: {total_reward}\")"
      ],
      "metadata": {
        "id": "hzNLu4NMugxF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf730c2d-149f-4c53-a5ad-f89fac301bfa"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing MC\n",
            "Episode 0 completed\n",
            "Episode 100 completed\n",
            "Episode 200 completed\n",
            "Episode 300 completed\n",
            "Episode 400 completed\n",
            "Episode 500 completed\n",
            "Episode 600 completed\n",
            "Episode 700 completed\n",
            "Episode 800 completed\n",
            "Episode 900 completed\n",
            "Optimal policy: {4: 12, 8: 1, 0: 47, 29: 20, 48: 5, 47: 18, 1: 22, 42: 14, 37: 33, 41: 24, 23: 36, 7: 46, 2: 19, 20: 36, 36: 8, 32: 44, 3: 49, 45: 13, 39: 45, 25: 35, 40: 7, 5: 39, 31: 2, 15: 40, 6: 41, 35: 11, 38: 48, 34: 6, 19: 15, 27: 33, 28: 37, 30: 5, 9: 45, 21: 32, 10: 9, 11: 31, 17: 25, 14: 18, 12: 10, 46: 4, 24: 42, 13: 23, 49: 2, 16: 4, 22: 26, 18: 28, 26: 17, 44: 4, 33: 3, 43: 7}\n",
            "Total reward using optimal policy: -200491.4952307807\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gsayfEdgyPX8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}