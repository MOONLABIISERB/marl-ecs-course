{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8JMPuZFkrYF",
        "outputId": "48df4a57-e58d-40f6-91fe-caf67abac987"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjULi0lZkfke",
        "outputId": "101c412d-95e4-40e0-f9e6-09bc7ef6770f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing and training DP solver... \n",
            " progress bar might not move, let it run!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Value Iteration:   0%|          | 1/1000 [01:15<20:57:39, 75.53s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DP training completed in 106.92 seconds\n",
            "\n",
            "Evaluating DP policy...\n",
            "DP Average Reward: -100.00\n",
            "DP Average Steps: 100.00\n",
            "\n",
            "Initializing and training MC solver...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Monte Carlo Training: 100%|██████████| 10000/10000 [01:22<00:00, 121.59it/s, epsilon=0.0100]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MC training completed in 82.26 seconds\n",
            "\n",
            "Evaluating MC policy...\n",
            "MC Average Reward: -84.22\n",
            "MC Average Steps: 85.87\n",
            "\n",
            "Comparing:\n",
            "Training Time - DP: 106.92s, MC: 82.26s\n",
            "Average Reward - DP: -100.00, MC: -84.22\n",
            "Average Steps - DP: 100.00, MC: 85.87\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from gymnasium import spaces\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "\n",
        "class SokobanGame(gym.Env):\n",
        "    def __init__(self):\n",
        "        super(SokobanGame, self).__init__()\n",
        "\n",
        "        self.height = 6\n",
        "        self.width = 7\n",
        "\n",
        "        self.action_space = spaces.Discrete(4)  # UP, DOWN, LEFT, RIGHT\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0, high=5, shape=(self.height, self.width), dtype=np.uint8\n",
        "        )\n",
        "\n",
        "        self.WALL = 0\n",
        "        self.FLOOR = 1\n",
        "        self.BOX = 2\n",
        "        self.STORAGE = 3\n",
        "        self.PLAYER = 4\n",
        "        self.BOX_ON_STORAGE = 5\n",
        "\n",
        "        self.actions = {\n",
        "            0: (-1, 0),  # UP\n",
        "            1: (1, 0),  # DOWN\n",
        "            2: (0, -1),  # LEFT\n",
        "            3: (0, 1),  # RIGHT\n",
        "        }\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.grid = np.zeros((self.height, self.width), dtype=int)\n",
        "        self.grid[1:-1, 1:-1] = self.FLOOR\n",
        "\n",
        "        # Ensure there's at least one clear path\n",
        "        self.grid[1:-1, 2] = self.FLOOR\n",
        "        self.grid[2, 1:-1] = self.FLOOR\n",
        "\n",
        "        # Randomly place player\n",
        "        player_positions = list(zip(*np.where(self.grid == self.FLOOR)))\n",
        "        self.player_pos = np.array(\n",
        "            player_positions[np.random.choice(len(player_positions))]\n",
        "        )\n",
        "        self.grid[self.player_pos[0], self.player_pos[1]] = self.PLAYER\n",
        "\n",
        "        # Randomly place box\n",
        "        available_positions = list(zip(*np.where(self.grid == self.FLOOR)))\n",
        "        self.box_pos = np.array(\n",
        "            available_positions[np.random.choice(len(available_positions))]\n",
        "        )\n",
        "        self.grid[self.box_pos[0], self.box_pos[1]] = self.BOX\n",
        "\n",
        "        # Randomly place storage\n",
        "        available_positions = list(zip(*np.where(self.grid == self.FLOOR)))\n",
        "        self.storage_pos = np.array(\n",
        "            available_positions[np.random.choice(len(available_positions))]\n",
        "        )\n",
        "        self.grid[self.storage_pos[0], self.storage_pos[1]] = self.STORAGE\n",
        "\n",
        "        return self.grid.copy(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        move = self.actions[action]\n",
        "        new_pos = self.player_pos + move\n",
        "\n",
        "        if self.grid[new_pos[0], new_pos[1]] == self.WALL:\n",
        "            return self.grid.copy(), -1, False, False, {}\n",
        "\n",
        "        if np.array_equal(new_pos, self.box_pos):\n",
        "            new_box_pos = self.box_pos + move\n",
        "            if self.grid[new_box_pos[0], new_box_pos[1]] in [self.WALL, self.BOX]:\n",
        "                return self.grid.copy(), -1, False, False, {}\n",
        "            self.grid[self.box_pos[0], self.box_pos[1]] = self.FLOOR\n",
        "            self.box_pos = new_box_pos\n",
        "            self.grid[new_box_pos[0], new_box_pos[1]] = (\n",
        "                self.BOX\n",
        "                if not np.array_equal(new_box_pos, self.storage_pos)\n",
        "                else self.BOX_ON_STORAGE\n",
        "            )\n",
        "\n",
        "        self.grid[self.player_pos[0], self.player_pos[1]] = self.FLOOR\n",
        "        self.player_pos = new_pos\n",
        "        self.grid[new_pos[0], new_pos[1]] = self.PLAYER\n",
        "\n",
        "        done = np.array_equal(self.box_pos, self.storage_pos)\n",
        "        reward = 10 if done else -1\n",
        "\n",
        "        return self.grid.copy(), reward, done, False, {}\n",
        "\n",
        "    def get_current_state(self):\n",
        "        return (tuple(self.player_pos), tuple(self.box_pos), tuple(self.storage_pos))\n",
        "\n",
        "class DynamicProgramming:\n",
        "    def __init__(self, environment):\n",
        "        self.env = environment\n",
        "        self.all_states = self._generate_state_space()\n",
        "        self.state_index_map = {state: idx for idx, state in enumerate(self.all_states)}\n",
        "        self.total_states = len(self.all_states)\n",
        "        self.total_actions = environment.action_space.n\n",
        "        self.value_function = np.zeros(self.total_states)\n",
        "        self.policy = np.zeros(self.total_states, dtype=int)\n",
        "\n",
        "    def _generate_state_space(self):\n",
        "        state_space = []\n",
        "        for player_pos in [(i, j) for i in range(self.env.height) for j in range(self.env.width)]:\n",
        "            for box_pos in [(i, j) for i in range(self.env.height) for j in range(self.env.width)]:\n",
        "                for storage_pos in [(i, j) for i in range(self.env.height) for j in range(self.env.width)]:\n",
        "                    if (player_pos != box_pos and box_pos != storage_pos and player_pos != storage_pos):\n",
        "                        state_space.append((player_pos, box_pos, storage_pos))\n",
        "        return state_space\n",
        "\n",
        "    def perform_value_iteration(self, gamma=0.9, convergence_threshold=1e-6, max_iter=1000):\n",
        "        for _ in tqdm(range(max_iter), desc=\"Value Iteration\"):\n",
        "            max_diff = 0\n",
        "            for idx, state in enumerate(self.all_states):\n",
        "                old_value = self.value_function[idx]\n",
        "                action_values = []\n",
        "                for action in range(self.total_actions):\n",
        "                    next_state, reward, done = self._calculate_next_state(state, action)\n",
        "                    if next_state in self.state_index_map:\n",
        "                        next_state_idx = self.state_index_map[next_state]\n",
        "                        action_values.append(\n",
        "                            reward + gamma * self.value_function[next_state_idx] * (not done)\n",
        "                        )\n",
        "                    else:\n",
        "                        action_values.append(reward)\n",
        "                self.value_function[idx] = max(action_values)\n",
        "                max_diff = max(max_diff, abs(old_value - self.value_function[idx]))\n",
        "            if max_diff < convergence_threshold:\n",
        "                break\n",
        "\n",
        "        self._update_policy(gamma)\n",
        "        return self.policy\n",
        "\n",
        "    def _calculate_next_state(self, state, action):\n",
        "        self.env.reset()\n",
        "        self.env.player_pos, self.env.box_pos, self.env.storage_pos = state\n",
        "        self.env.grid[self.env.player_pos[0], self.env.player_pos[1]] = self.env.PLAYER\n",
        "        self.env.grid[self.env.box_pos[0], self.env.box_pos[1]] = self.env.BOX\n",
        "        self.env.grid[self.env.storage_pos[0], self.env.storage_pos[1]] = self.env.STORAGE\n",
        "\n",
        "        next_state_obs, reward, done, _, _ = self.env.step(action)\n",
        "        next_state = self.env.get_current_state()\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def _update_policy(self, gamma):\n",
        "        for idx, state in enumerate(self.all_states):\n",
        "            action_values = []\n",
        "            for action in range(self.total_actions):\n",
        "                next_state, reward, done = self._calculate_next_state(state, action)\n",
        "                if next_state in self.state_index_map:\n",
        "                    next_state_idx = self.state_index_map[next_state]\n",
        "                    action_values.append(\n",
        "                        reward + gamma * self.value_function[next_state_idx] * (not done)\n",
        "                    )\n",
        "                else:\n",
        "                    action_values.append(reward)\n",
        "            self.policy[idx] = np.argmax(action_values)\n",
        "\n",
        "    def select_action(self, state):\n",
        "        if state in self.state_index_map:\n",
        "            return self.policy[self.state_index_map[state]]\n",
        "        return np.random.randint(self.total_actions)\n",
        "\n",
        "class MonteCarlo:\n",
        "    def __init__(self, environment):\n",
        "        self.env = environment\n",
        "        self.q_values = defaultdict(lambda: np.zeros(environment.action_space.n))\n",
        "        self.state_action_returns = defaultdict(lambda: defaultdict(list))\n",
        "        self.policy = {}\n",
        "        self.initial_epsilon = 1.0\n",
        "        self.min_epsilon = 0.01\n",
        "        self.epsilon_decay_rate = 0.95\n",
        "        self.discount_factor = 0.9  # Gamma (discount factor)\n",
        "\n",
        "    def epsilon_greedy(self, state, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "            return self.env.action_space.sample()\n",
        "        else:\n",
        "            return np.argmax(self.q_values[state]) if state in self.q_values else self.env.action_space.sample()\n",
        "\n",
        "    def create_episode(self, epsilon):\n",
        "        episode = []\n",
        "        state, _ = self.env.reset()\n",
        "        done = False\n",
        "        step_count = 0\n",
        "        while not done and step_count < 100:  # Limiting episode length\n",
        "            state = self.env.get_current_state()\n",
        "            action = self.epsilon_greedy(state, epsilon)\n",
        "            next_state, reward, done, _, _ = self.env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            step_count += 1\n",
        "        return episode\n",
        "\n",
        "    def learn(self, num_episodes=10000):\n",
        "        epsilon = self.initial_epsilon\n",
        "        progress_bar = tqdm(range(num_episodes), desc=\"Monte Carlo Training\")\n",
        "        for _ in progress_bar:\n",
        "            episode = self.create_episode(epsilon)\n",
        "            G = 0\n",
        "            for t in range(len(episode) - 1, -1, -1):\n",
        "                state, action, reward = episode[t]\n",
        "                G = self.discount_factor * G + reward\n",
        "                if (state, action) not in [(x[0], x[1]) for x in episode[:t]]:\n",
        "                    self.state_action_returns[state][action].append(G)\n",
        "                    self.q_values[state][action] = np.mean(self.state_action_returns[state][action])\n",
        "                    self.policy[state] = np.argmax(self.q_values[state])\n",
        "\n",
        "            # Decay epsilon\n",
        "            epsilon = max(self.min_epsilon, epsilon * self.epsilon_decay_rate)\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\"epsilon\": f\"{epsilon:.4f}\"})\n",
        "\n",
        "        return self.policy\n",
        "\n",
        "    def select_action(self, state):\n",
        "        return self.policy.get(state, self.env.action_space.sample())\n",
        "\n",
        "def run_episode(env, policy, save_frames=False):\n",
        "    state, _ = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "    steps = 0\n",
        "    frames = []\n",
        "    while not done and steps < 100:\n",
        "        action = policy.select_action(env.get_current_state())\n",
        "        state, reward, done, _, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "        steps += 1\n",
        "        if save_frames:\n",
        "            frames.append(state)\n",
        "    return total_reward, steps, frames\n",
        "\n",
        "def evaluate_policy(env, policy, num_episodes=100):\n",
        "    total_rewards = []\n",
        "    total_steps = []\n",
        "    for _ in range(num_episodes):\n",
        "        reward, steps, _ = run_episode(env, policy)\n",
        "        total_rewards.append(reward)\n",
        "        total_steps.append(steps)\n",
        "    return np.mean(total_rewards), np.mean(total_steps)\n",
        "\n",
        "def main():\n",
        "    env = SokobanGame()\n",
        "\n",
        "    print(\"Initializing and training DP solver... \\n progress bar might not move, let it run!\")\n",
        "    start_time = time.time()\n",
        "    dp_solver = DynamicProgramming(env)\n",
        "    dp_solver.perform_value_iteration()\n",
        "    dp_training_time = time.time() - start_time\n",
        "    print(f\"DP training completed in {dp_training_time:.2f} seconds\")\n",
        "\n",
        "    print(\"\\nEvaluating DP policy...\")\n",
        "    dp_avg_reward, dp_avg_steps = evaluate_policy(env, dp_solver)\n",
        "    print(f\"DP Average Reward: {dp_avg_reward:.2f}\")\n",
        "    print(f\"DP Average Steps: {dp_avg_steps:.2f}\")\n",
        "\n",
        "    print(\"\\nInitializing and training MC solver...\")\n",
        "    start_time = time.time()\n",
        "    mc_solver = MonteCarlo(env)\n",
        "    mc_solver.learn(num_episodes=10000)\n",
        "    mc_training_time = time.time() - start_time\n",
        "    print(f\"MC training completed in {mc_training_time:.2f} seconds\")\n",
        "\n",
        "    print(\"\\nEvaluating MC policy...\")\n",
        "    mc_avg_reward, mc_avg_steps = evaluate_policy(env, mc_solver)\n",
        "    print(f\"MC Average Reward: {mc_avg_reward:.2f}\")\n",
        "    print(f\"MC Average Steps: {mc_avg_steps:.2f}\")\n",
        "\n",
        "    print(\"\\nComparing:\")\n",
        "    print(f\"Training Time - DP: {dp_training_time:.2f}s, MC: {mc_training_time:.2f}s\")\n",
        "    print(f\"Average Reward - DP: {dp_avg_reward:.2f}, MC: {mc_avg_reward:.2f}\")\n",
        "    print(f\"Average Steps - DP: {dp_avg_steps:.2f}, MC: {mc_avg_steps:.2f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nmRr2wE4kmUH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}