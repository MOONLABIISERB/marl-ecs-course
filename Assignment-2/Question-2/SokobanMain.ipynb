{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Uv-UnLb-sfr",
        "outputId": "271a58ba-6302-4500-ed80-7e2fe57f4ffd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "pip install gymnasium"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.patches import Circle, Rectangle, Polygon\n",
        "\n",
        "class SokobanEnvironment:\n",
        "    def __init__(self):\n",
        "        self.grid = [\n",
        "            [1, 1, 1, 1, 1, 1],\n",
        "            [1, 0, 0, 1, 1, 1],\n",
        "            [1, 0, 0, 1, 1, 1],\n",
        "            [1, 3, 0, 0, 0, 1],\n",
        "            [1, 0, 0, 2, 0, 1],\n",
        "            [1, 0, 0, 1, 1, 1],\n",
        "            [1, 1, 1, 1, 1, 1],\n",
        "        ]\n",
        "        self.agent_position = (1, 2)\n",
        "        self.box_positions = [(4, 3)]\n",
        "        self.storage_positions = [(3, 1)]\n",
        "        self.actions = {\n",
        "            \"UP\": (-1, 0),\n",
        "            \"DOWN\": (1, 0),\n",
        "            \"LEFT\": (0, -1),\n",
        "            \"RIGHT\": (0, 1)\n",
        "        }\n",
        "        self.done = False\n",
        "\n",
        "    def display_grid(self):\n",
        "        \"\"\"Graphical display of the grid with a new color scheme and shapes.\"\"\"\n",
        "        fig, ax = plt.subplots()\n",
        "\n",
        "        for row in range(len(self.grid)):\n",
        "            for col in range(len(self.grid[row])):\n",
        "                if self.grid[row][col] == 1:\n",
        "                    ax.add_patch(Rectangle((col, row), 1, 1, edgecolor='black', facecolor='lightgray'))\n",
        "                elif (row, col) == self.agent_position:\n",
        "                    ax.add_patch(Circle((col + 0.5, row + 0.5), 0.4, edgecolor='black', facecolor='green'))\n",
        "                elif (row, col) in self.box_positions:\n",
        "                    ax.add_patch(Rectangle((col, row), 1, 1, edgecolor='black', facecolor='purple'))\n",
        "                    ax.plot([col, col + 1], [row, row + 1], color='white')\n",
        "                    ax.plot([col, col + 1], [row + 1, row], color='white')\n",
        "                elif (row, col) in self.storage_positions:\n",
        "                    diamond = Polygon([[col + 0.5, row], [col + 1, row + 0.5], [col + 0.5, row + 1], [col, row + 0.5]],\n",
        "                                      edgecolor='black', facecolor='cyan')\n",
        "                    ax.add_patch(diamond)\n",
        "                else:\n",
        "                    ax.add_patch(Rectangle((col, row), 1, 1, edgecolor='black', facecolor='beige'))\n",
        "\n",
        "        ax.set_xlim(0, len(self.grid[0]))\n",
        "        ax.set_ylim(0, len(self.grid))\n",
        "        ax.set_aspect('equal')\n",
        "        plt.gca().invert_yaxis()  # Keeping the origin at the top-left for visual consistency\n",
        "        plt.axis('off')  # Removing axes for a cleaner look\n",
        "        plt.show()\n",
        "\n",
        "    def reset(self):\n",
        "        self.agent_position = (1, 2)\n",
        "        self.box_positions = [(4, 3)]\n",
        "        self.done = False\n",
        "\n",
        "    def get_next_state(self, action):\n",
        "        dx, dy = action\n",
        "        x, y = self.agent_position\n",
        "        next_agent_position = (x + dx, y + dy)\n",
        "\n",
        "        if self.is_valid_position(next_agent_position):\n",
        "            if next_agent_position in self.box_positions:\n",
        "                next_box_position = (next_agent_position[0] + dx, next_agent_position[1] + dy)\n",
        "                if self.is_valid_position(next_box_position) and next_box_position not in self.box_positions:\n",
        "                    self.box_positions.remove(next_agent_position)\n",
        "                    self.box_positions.append(next_box_position)\n",
        "                    return next_agent_position\n",
        "                else:\n",
        "                    return self.agent_position\n",
        "            else:\n",
        "                return next_agent_position\n",
        "        else:\n",
        "            return self.agent_position\n",
        "\n",
        "    def is_valid_position(self, position):\n",
        "        x, y = position\n",
        "        if x < 0 or x >= len(self.grid) or y < 0 or y >= len(self.grid[0]):\n",
        "            return False\n",
        "        if self.grid[x][y] == 1:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def step(self, action):\n",
        "        if self.done:\n",
        "            return self.agent_position, 0, self.done\n",
        "\n",
        "\n",
        "        previous_agent_position = self.agent_position\n",
        "        self.agent_position = self.get_next_state(self.actions[action])\n",
        "\n",
        "\n",
        "        if self.agent_position != previous_agent_position:\n",
        "\n",
        "            if previous_agent_position in self.box_positions:\n",
        "                box_index = self.box_positions.index(previous_agent_position)\n",
        "                if self.box_positions[box_index] in self.storage_positions:\n",
        "                    reward = 5\n",
        "                else:\n",
        "                    reward = -1\n",
        "            else:\n",
        "                reward = -1\n",
        "\n",
        "            if all(box in self.storage_positions for box in self.box_positions):\n",
        "                print(\"All boxes are placed in storage! Episode complete.\")\n",
        "                self.done = True\n",
        "\n",
        "            for box in self.box_positions:\n",
        "                if self.is_box_stuck(box):\n",
        "                    print(\"A box is stuck! Episode complete.\")\n",
        "                    self.done = True\n",
        "        else:\n",
        "            if not self.is_valid_position(self.agent_position):\n",
        "                reward = -10\n",
        "            else:\n",
        "                reward = -1\n",
        "\n",
        "        return self.agent_position, reward, self.done\n",
        "\n",
        "    def is_box_stuck(self, box_position):\n",
        "        x, y = box_position\n",
        "        if (self.grid[x-1][y] == 1 and self.grid[x][y-1] == 1) or \\\n",
        "           (self.grid[x-1][y] == 1 and self.grid[x][y+1] == 1) or \\\n",
        "           (self.grid[x+1][y] == 1 and self.grid[x][y-1] == 1) or \\\n",
        "           (self.grid[x+1][y] == 1 and self.grid[x][y+1] == 1):\n",
        "            return True\n",
        "        return False"
      ],
      "metadata": {
        "id": "kEiKkJtBAg49"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "BXqAJ4IlAi3u"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import defaultdict\n",
        "\n",
        "class SokobanAgent:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.value_function = defaultdict(float)\n",
        "        self.policy = {state: random.choice(list(env.actions.keys())) for state in self.get_all_states()}\n",
        "        self.discount_factor = 0.9\n",
        "        self.alpha = 0.1\n",
        "        self.epsilon = 0.3\n",
        "\n",
        "    def value_iteration(self, theta=1e-5):\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for state in self.get_all_states():\n",
        "                v = self.value_function[state]\n",
        "                self.value_function[state] = max(self.calculate_q_value(state, action) for action in self.env.actions.keys())\n",
        "                delta = max(delta, abs(v - self.value_function[state]))\n",
        "            if delta < theta:\n",
        "                break\n",
        "\n",
        "        self.extract_optimal_policy()\n",
        "\n",
        "    def calculate_q_value(self, state, action):\n",
        "        total = 0\n",
        "        for next_action in self.env.actions.keys():\n",
        "            next_state, reward, _ = self.simulate_action(state, action)\n",
        "            total += (1 / len(self.env.actions)) * (reward + self.discount_factor * self.value_function[next_state])\n",
        "        return total\n",
        "\n",
        "    def simulate_action(self, state, action):\n",
        "        x, y = state\n",
        "        dx, dy = self.env.actions[action]\n",
        "        next_agent_position = (x + dx, y + dy)\n",
        "\n",
        "        if self.env.is_valid_position(next_agent_position):\n",
        "            if next_agent_position in self.env.box_positions:\n",
        "                next_box_position = (next_agent_position[0] + dx, next_agent_position[1] + dy)\n",
        "                if self.env.is_valid_position(next_box_position) and next_box_position not in self.env.box_positions:\n",
        "                    next_agent_position = next_box_position\n",
        "                    return next_agent_position, 5, False\n",
        "            return next_agent_position, -1, False\n",
        "        return state, -10, False\n",
        "\n",
        "    def get_all_states(self):\n",
        "        states = []\n",
        "        for x in range(len(self.env.grid)):\n",
        "            for y in range(len(self.env.grid[0])):\n",
        "                states.append((x, y))\n",
        "        return states\n",
        "\n",
        "    def extract_optimal_policy(self):\n",
        "        for state in self.get_all_states():\n",
        "            if state in self.value_function:\n",
        "                self.policy[state] = max(self.env.actions.keys(), key=lambda action: self.calculate_q_value(state, action))\n",
        "\n",
        "    def monte_carlo_control(self, num_episodes):\n",
        "        returns = defaultdict(list)\n",
        "        for episode_num in range(num_episodes):\n",
        "            episode_data = self.generate_episode()\n",
        "            visited_states = set()\n",
        "            G = 0\n",
        "\n",
        "            for state, action, reward in reversed(episode_data):\n",
        "                G = reward + self.discount_factor * G\n",
        "                if state not in visited_states:\n",
        "                    returns[(state, action)].append(G)\n",
        "                    self.value_function[state] = np.mean(returns[(state, action)])\n",
        "                    visited_states.add(state)\n",
        "            self.extract_optimal_policy()\n",
        "\n",
        "    def generate_episode(self):\n",
        "        episode = []\n",
        "        state = self.env.agent_position\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            if state in self.policy:\n",
        "                if random.random() < self.epsilon:\n",
        "                    action = random.choice(list(self.env.actions.keys()))\n",
        "                else:\n",
        "                    action = self.policy[state]\n",
        "            else:\n",
        "                action = random.choice(list(self.env.actions.keys()))\n",
        "\n",
        "            next_state, reward, done = self.env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            state = next_state\n",
        "        return episode"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    env = SokobanEnvironment()\n",
        "    env.display_grid()\n",
        "\n",
        "    agent_dp = SokobanAgent(env)\n",
        "    agent_dp.value_iteration()\n",
        "    print(\"Value Function after Value Iteration:\")\n",
        "    print(\"{:<15} {:<15}\".format(\"State\", \"Value\"))\n",
        "    print(\"-\" * 30)\n",
        "    for state, value in agent_dp.value_function.items():\n",
        "        print(f\"{str(state):<15} {value:<15.5f}\")\n",
        "\n",
        "    print(\"\\nOptimal Policy after Value Iteration:\")\n",
        "    print(\"{:<15} {:<15}\".format(\"State\", \"Optimal Action\"))\n",
        "    print(\"-\" * 30)\n",
        "    for state, action in agent_dp.policy.items():\n",
        "        print(f\"{str(state):<15} {action:<15}\")\n",
        "\n",
        "    env.reset()\n",
        "\n",
        "    agent_mc = SokobanAgent(env)\n",
        "    agent_mc.monte_carlo_control(num_episodes=1000)\n",
        "    print(\"\\nValue Function after Monte Carlo Control:\")\n",
        "    print(\"{:<15} {:<15}\".format(\"State\", \"Value\"))\n",
        "    print(\"-\" * 30)\n",
        "    for state, value in agent_mc.value_function.items():\n",
        "        print(f\"{str(state):<15} {value:<15.5f}\")\n",
        "\n",
        "    print(\"\\nOptimal Policy after Monte Carlo Control:\")\n",
        "    print(\"{:<15} {:<15}\".format(\"State\", \"Optimal Action\"))\n",
        "    print(\"-\" * 30)\n",
        "    for state, action in agent_mc.policy.items():\n",
        "        print(f\"{str(state):<15} {action:<15}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZYbQVbxWAkdB",
        "outputId": "014772b9-93fa-42b7-a9dd-d4076b60c08d"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Function after Value Iteration:\n",
            "State           Value          \n",
            "------------------------------\n",
            "(0, 0)          -99.99992      \n",
            "(0, 1)          25.42940       \n",
            "(1, 1)          29.36600       \n",
            "(0, 2)          29.36600       \n",
            "(1, 2)          33.74000       \n",
            "(0, 3)          -99.99992      \n",
            "(0, 4)          -99.99992      \n",
            "(0, 5)          -99.99992      \n",
            "(1, 0)          25.42940       \n",
            "(2, 1)          33.74000       \n",
            "(2, 2)          38.60000       \n",
            "(1, 3)          29.36600       \n",
            "(1, 4)          -99.99992      \n",
            "(1, 5)          -99.99992      \n",
            "(2, 0)          29.36600       \n",
            "(3, 1)          38.60000       \n",
            "(3, 2)          44.00000       \n",
            "(2, 3)          33.74000       \n",
            "(3, 3)          38.60000       \n",
            "(2, 4)          38.60000       \n",
            "(3, 4)          44.00000       \n",
            "(2, 5)          -99.99992      \n",
            "(3, 0)          33.74000       \n",
            "(4, 1)          44.00000       \n",
            "(4, 2)          50.00000       \n",
            "(4, 3)          44.00000       \n",
            "(4, 4)          50.00000       \n",
            "(3, 5)          38.60000       \n",
            "(4, 0)          38.60000       \n",
            "(5, 1)          38.60000       \n",
            "(5, 2)          44.00000       \n",
            "(4, 5)          44.00000       \n",
            "(5, 0)          33.74000       \n",
            "(5, 3)          39.74000       \n",
            "(5, 4)          44.00000       \n",
            "(5, 5)          -99.99992      \n",
            "(6, 0)          -99.99992      \n",
            "(6, 1)          33.74000       \n",
            "(6, 2)          38.60000       \n",
            "(6, 3)          -99.99992      \n",
            "(6, 4)          -99.99992      \n",
            "(6, 5)          -99.99992      \n",
            "\n",
            "Optimal Policy after Value Iteration:\n",
            "State           Optimal Action \n",
            "------------------------------\n",
            "(0, 0)          UP             \n",
            "(0, 1)          DOWN           \n",
            "(0, 2)          DOWN           \n",
            "(0, 3)          UP             \n",
            "(0, 4)          UP             \n",
            "(0, 5)          UP             \n",
            "(1, 0)          RIGHT          \n",
            "(1, 1)          DOWN           \n",
            "(1, 2)          DOWN           \n",
            "(1, 3)          LEFT           \n",
            "(1, 4)          UP             \n",
            "(1, 5)          UP             \n",
            "(2, 0)          RIGHT          \n",
            "(2, 1)          DOWN           \n",
            "(2, 2)          DOWN           \n",
            "(2, 3)          DOWN           \n",
            "(2, 4)          DOWN           \n",
            "(2, 5)          UP             \n",
            "(3, 0)          RIGHT          \n",
            "(3, 1)          DOWN           \n",
            "(3, 2)          DOWN           \n",
            "(3, 3)          DOWN           \n",
            "(3, 4)          DOWN           \n",
            "(3, 5)          LEFT           \n",
            "(4, 0)          RIGHT          \n",
            "(4, 1)          RIGHT          \n",
            "(4, 2)          RIGHT          \n",
            "(4, 3)          RIGHT          \n",
            "(4, 4)          LEFT           \n",
            "(4, 5)          LEFT           \n",
            "(5, 0)          RIGHT          \n",
            "(5, 1)          RIGHT          \n",
            "(5, 2)          UP             \n",
            "(5, 3)          UP             \n",
            "(5, 4)          UP             \n",
            "(5, 5)          UP             \n",
            "(6, 0)          UP             \n",
            "(6, 1)          UP             \n",
            "(6, 2)          UP             \n",
            "(6, 3)          UP             \n",
            "(6, 4)          UP             \n",
            "(6, 5)          UP             \n",
            "A box is stuck! Episode complete.\n",
            "\n",
            "Value Function after Monte Carlo Control:\n",
            "State           Value          \n",
            "------------------------------\n",
            "(4, 2)          -1.00000       \n",
            "(5, 2)          -1.90000       \n",
            "(5, 1)          -4.09510       \n",
            "(4, 1)          -9.99970       \n",
            "(3, 1)          -9.99998       \n",
            "(3, 2)          -10.00000      \n",
            "(2, 1)          -10.00000      \n",
            "(2, 2)          -10.00000      \n",
            "(1, 2)          -10.00000      \n",
            "(1, 1)          0.00000        \n",
            "(3, 3)          0.00000        \n",
            "(4, 3)          0.00000        \n",
            "(3, 4)          0.00000        \n",
            "(4, 4)          0.00000        \n",
            "\n",
            "Optimal Policy after Monte Carlo Control:\n",
            "State           Optimal Action \n",
            "------------------------------\n",
            "(0, 0)          LEFT           \n",
            "(0, 1)          RIGHT          \n",
            "(0, 2)          DOWN           \n",
            "(0, 3)          RIGHT          \n",
            "(0, 4)          RIGHT          \n",
            "(0, 5)          UP             \n",
            "(1, 0)          UP             \n",
            "(1, 1)          DOWN           \n",
            "(1, 2)          LEFT           \n",
            "(1, 3)          RIGHT          \n",
            "(1, 4)          UP             \n",
            "(1, 5)          RIGHT          \n",
            "(2, 0)          LEFT           \n",
            "(2, 1)          UP             \n",
            "(2, 2)          DOWN           \n",
            "(2, 3)          RIGHT          \n",
            "(2, 4)          LEFT           \n",
            "(2, 5)          RIGHT          \n",
            "(3, 0)          RIGHT          \n",
            "(3, 1)          DOWN           \n",
            "(3, 2)          RIGHT          \n",
            "(3, 3)          DOWN           \n",
            "(3, 4)          DOWN           \n",
            "(3, 5)          LEFT           \n",
            "(4, 0)          DOWN           \n",
            "(4, 1)          RIGHT          \n",
            "(4, 2)          RIGHT          \n",
            "(4, 3)          UP             \n",
            "(4, 4)          UP             \n",
            "(4, 5)          UP             \n",
            "(5, 0)          LEFT           \n",
            "(5, 1)          RIGHT          \n",
            "(5, 2)          UP             \n",
            "(5, 3)          UP             \n",
            "(5, 4)          LEFT           \n",
            "(5, 5)          LEFT           \n",
            "(6, 0)          DOWN           \n",
            "(6, 1)          UP             \n",
            "(6, 2)          RIGHT          \n",
            "(6, 3)          UP             \n",
            "(6, 4)          DOWN           \n",
            "(6, 5)          DOWN           \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-jCEcw8XKXbN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}