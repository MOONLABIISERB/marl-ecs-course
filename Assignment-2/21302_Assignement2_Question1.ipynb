{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZm4V40wf-Ht",
        "outputId": "1efd8786-3a03-4b8b-8c97-4480964348d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/958.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Traveling Salesman Problem (TSP) Solving with Reinforcement Learning Techniques.\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from typing import List, Tuple, Optional, Dict\n",
        "\n",
        "\n",
        "class TravelingSalesmanEnviroment(gym.Env):\n",
        "    \"\"\"Custom Gymnasium environment for the Traveling Salesman Problem.\"\"\"\n",
        "\n",
        "    def __init__(self, num_targts: int, max_arena: int = 50, seed: Optional[int] = None):\n",
        "        \"\"\"\n",
        "        Initialize the TSP environment with configurable parameters.\n",
        "\n",
        "        Args:\n",
        "            num_targts: Number of cities/targets to visit\n",
        "            max_arena: Maximum coordinate range for target locations\n",
        "            seed: Random seed for reproducibility\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        np.random.seed(seed)\n",
        "\n",
        "        self.num_targts = num_targts\n",
        "        self.max_arena = max_arena\n",
        "        self.max_steps = num_targts\n",
        "\n",
        "        self.locations = self._genrate_unique_points()\n",
        "        self.dist_matrix = self._compute_dist_matrix()\n",
        "\n",
        "        obs_shape = 1 + self.num_targts + 2 * self.num_targts\n",
        "        obs_low = np.zeros(obs_shape)\n",
        "        obs_high = np.concatenate([\n",
        "            [num_targts],\n",
        "            2 * max_arena * np.ones(self.num_targts),\n",
        "            max_arena * np.ones(2 * self.num_targts)\n",
        "        ])\n",
        "\n",
        "        self.observation_space = gym.spaces.Box(low=obs_low, high=obs_high, dtype=np.float32)\n",
        "        self.action_space = gym.spaces.Discrete(num_targts)\n",
        "\n",
        "    def _genrate_unique_points(self) -> np.ndarray:\n",
        "        \"\"\"Generate unique random points within the defined area.\"\"\"\n",
        "        points = set()\n",
        "        while len(points) < self.num_targts:\n",
        "            x = np.random.uniform(0, self.max_arena)\n",
        "            y = np.random.uniform(0, self.max_arena)\n",
        "            points.add((x, y))\n",
        "        return np.array(list(points))\n",
        "\n",
        "    def _compute_dist_matrix(self) -> np.ndarray:\n",
        "        \"\"\"Compute pairwise Euclidean distances between points.\"\"\"\n",
        "        return np.linalg.norm(\n",
        "            self.locations[:, np.newaxis] - self.locations,\n",
        "            axis=2\n",
        "        )\n",
        "\n",
        "    def reset(self, seed=None, options=None) -> Tuple[np.ndarray, Dict]:\n",
        "        \"\"\"Reset the environment to initial state.\"\"\"\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        self.curr_location = 0\n",
        "        self.visited_targts = set()\n",
        "        self.steps = 0\n",
        "\n",
        "        state = np.concatenate([\n",
        "            [self.curr_location],\n",
        "            self.dist_matrix[self.curr_location],\n",
        "            self.locations.flatten()\n",
        "        ])\n",
        "\n",
        "        return state, {}\n",
        "\n",
        "    def step(self, action: int) -> Tuple[np.ndarray, float, bool, bool, Dict]:\n",
        "        \"\"\"Take a step in the environment.\"\"\"\n",
        "        self.steps += 1\n",
        "\n",
        "        reward = -self.dist_matrix[self.curr_location][action]\n",
        "\n",
        "        if action in self.visited_targts:\n",
        "            reward -= 1000\n",
        "\n",
        "        self.curr_location = action\n",
        "        self.visited_targts.add(action)\n",
        "\n",
        "        terminated = len(self.visited_targts) == self.num_targts\n",
        "        truncated = self.steps >= self.max_steps\n",
        "\n",
        "        next_state = np.concatenate([\n",
        "            [self.curr_location],\n",
        "            self.dist_matrix[self.curr_location],\n",
        "            self.locations.flatten()\n",
        "        ])\n",
        "\n",
        "        return next_state, reward, terminated, truncated, {}\n",
        "\n",
        "\n",
        "class TSPValueIterSolver:\n",
        "    \"\"\"Value Iteration approach for solving Traveling Salesman Problem.\"\"\"\n",
        "\n",
        "    def __init__(self, env, gamma=0.9, convergence_thresh=1e-5):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.convergence_thresh = convergence_thresh\n",
        "        self.val_table = np.zeros(env.num_targts)\n",
        "\n",
        "    def solve(self) -> np.ndarray:\n",
        "        \"\"\"Perform value iteration to find optimal policy.\"\"\"\n",
        "        while True:\n",
        "            max_change = 0\n",
        "            for state in range(self.env.num_targts):\n",
        "                old_value = self.val_table[state]\n",
        "\n",
        "                act_values = [\n",
        "                    -self.env.dist_matrix[state][action] +\n",
        "                    self.gamma * self.val_table[action]\n",
        "                    for action in range(self.env.num_targts)\n",
        "                ]\n",
        "\n",
        "                self.val_table[state] = max(act_values)\n",
        "                max_change = max(max_change, abs(old_value - self.val_table[state]))\n",
        "\n",
        "            if max_change < self.convergence_thresh:\n",
        "                break\n",
        "\n",
        "        return np.argmax(\n",
        "            [\n",
        "                -self.env.dist_matrix[state][action] +\n",
        "                self.gamma * self.val_table[action]\n",
        "                for action in range(self.env.num_targts)\n",
        "            ]\n",
        "            for state in range(self.env.num_targts)\n",
        "        )\n",
        "\n",
        "\n",
        "class TSPMonteCarloPolSolver:\n",
        "    \"\"\"Monte Carlo Learning approach for solving Traveling Salesman Problem.\"\"\"\n",
        "\n",
        "    def __init__(self, env, gamma=0.95):\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.policy = np.random.randint(env.num_targts, size=env.num_targts)\n",
        "        self.retrns = {(s, a): [] for s in range(env.num_targts) for a in range(env.num_targts)}\n",
        "\n",
        "    def genrate_episode(self, epsilon=0.1) -> List[Tuple]:\n",
        "        \"\"\"Generate an episode using epsilon-greedy policy.\"\"\"\n",
        "        episode = []\n",
        "        state, _ = self.env.reset()\n",
        "        visited = set()\n",
        "\n",
        "        for _ in range(self.env.num_targts):\n",
        "            curr_state = int(state[0])\n",
        "\n",
        "            # Epsilon-greedy action selection\n",
        "            if np.random.random() < epsilon:\n",
        "                action = np.random.choice(self.env.num_targts)\n",
        "            else:\n",
        "                action = self.policy[curr_state]\n",
        "\n",
        "            next_state, reward, done, _, _ = self.env.step(action)\n",
        "            episode.append((curr_state, action, reward))\n",
        "\n",
        "            visited.add(curr_state)\n",
        "            state = next_state\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        return episode\n",
        "\n",
        "    def monte_carlo_ctl(self, episodes=1000):\n",
        "        \"\"\"Perform Monte Carlo Control to improve policy.\"\"\"\n",
        "        for _ in range(episodes):\n",
        "            episode = self.genrate_episode()\n",
        "            G = 0\n",
        "            visited_state_actions = set()\n",
        "\n",
        "            for t in reversed(range(len(episode))):\n",
        "                state, action, reward = episode[t]\n",
        "                G = self.gamma * G + reward\n",
        "\n",
        "                if (state, action) not in visited_state_actions:\n",
        "                    self.retrns[(state, action)].append(G)\n",
        "                    visited_state_actions.add((state, action))\n",
        "\n",
        "                    # Update value-based policy\n",
        "                    action_values = [\n",
        "                        np.mean(self.retrns.get((state, a), [0]))\n",
        "                        for a in range(self.env.num_targts)\n",
        "                    ]\n",
        "                    self.policy[state] = np.argmax(action_values)\n",
        "\n",
        "        return self.policy\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Demonstrate TSP solving techniques.\"\"\"\n",
        "    np.random.seed(42)\n",
        "    env = TravelingSalesmanEnviroment(num_targts=6)\n",
        "\n",
        "    print(\"Value Iteration Solution:\")\n",
        "    vi_solver = TSPValueIterSolver(env)\n",
        "    vi_policy = vi_solver.solve()\n",
        "    print(\"Policy:\", vi_policy)\n",
        "    print(\"Value Table:\", vi_solver.val_table)\n",
        "\n",
        "    print(\"\\nMonte Carlo Solution:\")\n",
        "    mc_solver = TSPMonteCarloPolSolver(env)\n",
        "    mc_policy = mc_solver.monte_carlo_ctl(episodes=500)\n",
        "    print(\"Policy:\", mc_policy)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmSBclx2gAGG",
        "outputId": "151dd19a-9980-45d2-aff6-cf6d0c902e5c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Iteration Solution:\n",
            "Policy: 0\n",
            "Value Table: [0. 0. 0. 0. 0. 0.]\n",
            "\n",
            "Monte Carlo Solution:\n",
            "Policy: [0 2 2 3 1 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LSbOgl_njiPw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}