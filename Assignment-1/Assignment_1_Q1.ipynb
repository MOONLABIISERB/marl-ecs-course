{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOLUTION BY POLICY ITERATION:\n",
      "Optimal Value Function: {'Hostel': 16.056233584568062, 'Academic-Building': 21.846509584777067, 'Canteen': 18.826701283836204}\n",
      "Optimal Policy: {'Hostel': 'attend', 'Academic-Building': 'attend', 'Canteen': 'attend'}\n",
      "-------------------------------------\n",
      "SOLUTION BY VALUE ITERATION:\n",
      "Optimal Value Function: {'Hostel': 16.05623383977953, 'Academic-Building': 21.846509827444127, 'Canteen': 18.82670150662358}\n",
      "Optimal Policy: {'Hostel': 'attend', 'Academic-Building': 'attend', 'Canteen': 'attend'}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MarkovDecisionProcess:\n",
    "    def __init__(self, states, actions, transitions, rewards, gamma):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transitions = transitions\n",
    "        self.rewards = rewards\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def get_transitions(self, state, action):\n",
    "        return self.transitions[state][action]\n",
    "\n",
    "    def get_reward(self, state, action):\n",
    "        return self.rewards[state][action]\n",
    "\n",
    "def perform_value_iteration(mdp, threshold=1e-6):\n",
    "    state_values = {s: 0 for s in mdp.states}\n",
    "\n",
    "    while True:\n",
    "        max_change = 0\n",
    "        for s in mdp.states:\n",
    "            best_action_value = float('-inf')\n",
    "            for a in mdp.actions:\n",
    "                action_value = sum(prob * (mdp.get_reward(s, a) + mdp.gamma * state_values[next_s])\n",
    "                                   for next_s, prob in mdp.get_transitions(s, a).items())\n",
    "                best_action_value = max(best_action_value, action_value)\n",
    "\n",
    "            max_change = max(max_change, abs(best_action_value - state_values[s]))\n",
    "            state_values[s] = best_action_value\n",
    "\n",
    "        if max_change < threshold:\n",
    "            break\n",
    "\n",
    "    policy = {}\n",
    "    for s in mdp.states:\n",
    "        best_action = None\n",
    "        best_value = float('-inf')\n",
    "        for a in mdp.actions:\n",
    "            action_value = sum(prob * (mdp.get_reward(s, a) + mdp.gamma * state_values[next_s])\n",
    "                               for next_s, prob in mdp.get_transitions(s, a).items())\n",
    "            if action_value > best_value:\n",
    "                best_value = action_value\n",
    "                best_action = a\n",
    "        policy[s] = best_action\n",
    "\n",
    "    return state_values, policy\n",
    "\n",
    "def evaluate_policy(policy, mdp, state_values, threshold=1e-6):\n",
    "    while True:\n",
    "        max_change = 0\n",
    "        for s in mdp.states:\n",
    "            chosen_action = policy[s]\n",
    "            action_value = sum(prob * (mdp.get_reward(s, chosen_action) + mdp.gamma * state_values[next_s])\n",
    "                               for next_s, prob in mdp.get_transitions(s, chosen_action).items())\n",
    "            max_change = max(max_change, abs(action_value - state_values[s]))\n",
    "            state_values[s] = action_value\n",
    "\n",
    "        if max_change < threshold:\n",
    "            break\n",
    "    return state_values\n",
    "\n",
    "def perform_policy_iteration(mdp):\n",
    "    policy = {s: np.random.choice(mdp.actions) for s in mdp.states}\n",
    "    state_values = {s: 0 for s in mdp.states}\n",
    "\n",
    "    while True:\n",
    "        state_values = evaluate_policy(policy, mdp, state_values)\n",
    "        policy_stable = True\n",
    "\n",
    "        for s in mdp.states:\n",
    "            current_action = policy[s]\n",
    "            action_evaluations = {}\n",
    "            for a in mdp.actions:\n",
    "                action_evaluations[a] = sum(prob * (mdp.get_reward(s, a) + mdp.gamma * state_values[next_s])\n",
    "                                            for next_s, prob in mdp.get_transitions(s, a).items())\n",
    "            optimal_action = max(action_evaluations, key=action_evaluations.get)\n",
    "\n",
    "            if optimal_action != current_action:\n",
    "                policy_stable = False\n",
    "\n",
    "            policy[s] = optimal_action\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return state_values, policy\n",
    "\n",
    "# Defining states, actions, and rewards\n",
    "locations = ['Hostel', 'Academic-Building', 'Canteen']\n",
    "options = ['attend', 'hungry']\n",
    "probabilities = {\n",
    "    'Hostel': {'attend': {'Hostel': 0.5, 'Academic-Building': 0.5}, 'hungry': {'Canteen': 1.0}},\n",
    "    'Academic-Building': {'attend': {'Academic-Building': 0.7, 'Canteen': 0.3}, 'hungry': {'Academic-Building': 0.2, 'Canteen': 0.8}},\n",
    "    'Canteen': {'attend': {'Hostel': 0.3, 'Academic-Building': 0.6, 'Canteen': 0.1}, 'hungry': {'Canteen': 1}}\n",
    "}\n",
    "rewards_map = {\n",
    "    'Hostel': {'attend': -1, 'hungry': -1},\n",
    "    'Academic-Building': {'attend': 3, 'hungry': 3},\n",
    "    'Canteen': {'attend': 1, 'hungry': 1}\n",
    "}\n",
    "gamma_factor = 0.9\n",
    "\n",
    "mdp_instance = MarkovDecisionProcess(locations, options, probabilities, rewards_map, gamma_factor)\n",
    "\n",
    "value_function, optimal_policy = perform_policy_iteration(mdp_instance)\n",
    "print(\"SOLUTION BY POLICY ITERATION:\")\n",
    "print(\"Optimal Value Function:\", value_function)\n",
    "print(\"Optimal Policy:\", optimal_policy)\n",
    "print(\"-------------------------------------\")\n",
    "value_function, optimal_policy = perform_value_iteration(mdp_instance)\n",
    "print(\"SOLUTION BY VALUE ITERATION:\")\n",
    "print(\"Optimal Value Function:\", value_function)\n",
    "print(\"Optimal Policy:\", optimal_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
