{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the MDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define states\n",
    "states = [\"Hostel\", \"Academic Building\", \"Canteen\"]\n",
    "\n",
    "# Define actions\n",
    "actions = [\"Class\", \"Food\"]\n",
    "\n",
    "# Define rewards for each state\n",
    "rewards = {\n",
    "    \"Hostel\": -1,\n",
    "    \"Academic Building\": 3,\n",
    "    \"Canteen\": 1\n",
    "}\n",
    "\n",
    "# Define transition probabilities as a dictionary\n",
    "transition_probabilities = {\n",
    "    (\"Hostel\", \"Class\"): [(\"Hostel\", 0.5), (\"Academic Building\", 0.5)],\n",
    "    (\"Hostel\", \"Food\"): [(\"Canteen\", 1.0)],\n",
    "    (\"Academic Building\", \"Class\"): [(\"Academic Building\", 0.7), (\"Canteen\", 0.3)],\n",
    "    (\"Academic Building\", \"Food\"): [(\"Canteen\", 0.8), (\"Academic Building\", 0.2)],\n",
    "    (\"Canteen\", \"Class\"): [(\"Academic Building\", 0.6), (\"Hostel\", 0.3), (\"Canteen\", 0.1)],\n",
    "    (\"Canteen\", \"Food\"): [(\"Canteen\", 1.0)]\n",
    "}\n",
    "\n",
    "# Discount factor\n",
    "gamma = 0.9\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Values: {'Hostel': 16.056233839779527, 'Academic Building': 21.846509827444127, 'Canteen': 18.826701506623575}\n"
     ]
    }
   ],
   "source": [
    "# Initialize value function for each state to 0\n",
    "V = {state: 0 for state in states}\n",
    "\n",
    "# Threshold for convergence\n",
    "theta = 1e-6\n",
    "\n",
    "def value_iteration():\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in states:\n",
    "            v = V[state]\n",
    "            # Update the value function based on the maximum expected return\n",
    "            V[state] = max(\n",
    "                sum(prob * (rewards[state] + gamma * V[next_state])\n",
    "                    for next_state, prob in transition_probabilities[(state, action)])\n",
    "                for action in actions\n",
    "            )\n",
    "            # Track the maximum change across all states\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "        # Stop if the value function converges\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "# Run value iteration\n",
    "optimal_values_vi = value_iteration()\n",
    "print(\"Optimal Values:\", optimal_values_vi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy: {'Hostel': 'Class', 'Academic Building': 'Class', 'Canteen': 'Class'}\n"
     ]
    }
   ],
   "source": [
    "def extract_policy():\n",
    "    policy = {}\n",
    "    for state in states:\n",
    "        # Choose the action that maximizes the expected return\n",
    "        policy[state] = max(\n",
    "            actions,\n",
    "            key=lambda action: sum(prob * (rewards[state] + gamma * V[next_state])\n",
    "                                   for next_state, prob in transition_probabilities[(state, action)])\n",
    "        )\n",
    "    return policy\n",
    "\n",
    "# Extract the optimal policy\n",
    "optimal_policy_vi = extract_policy()\n",
    "print(\"Optimal Policy:\", optimal_policy_vi)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Policy (Policy Iteration): {'Hostel': 'Class', 'Academic Building': 'Class', 'Canteen': 'Class'}\n",
      "Optimal Values (Policy Iteration): {'Hostel': 16.056233536108103, 'Academic Building': 21.846509538699056, 'Canteen': 18.826701241532977}\n"
     ]
    }
   ],
   "source": [
    "# Initialize a random policy\n",
    "policy = {state: np.random.choice(actions) for state in states}\n",
    "\n",
    "def policy_evaluation(policy):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in states:\n",
    "            v = V[state]\n",
    "            # Evaluate the policy by calculating the value for the chosen action\n",
    "            action = policy[state]\n",
    "            V[state] = sum(prob * (rewards[state] + gamma * V[next_state])\n",
    "                           for next_state, prob in transition_probabilities[(state, action)])\n",
    "            delta = max(delta, abs(v - V[state]))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "def policy_improvement():\n",
    "    stable = True\n",
    "    for state in states:\n",
    "        old_action = policy[state]\n",
    "        # Choose the action that maximizes expected return\n",
    "        policy[state] = max(\n",
    "            actions,\n",
    "            key=lambda action: sum(prob * (rewards[state] + gamma * V[next_state])\n",
    "                                   for next_state, prob in transition_probabilities[(state, action)])\n",
    "        )\n",
    "        if old_action != policy[state]:\n",
    "            stable = False\n",
    "    return stable\n",
    "\n",
    "def policy_iteration():\n",
    "    while True:\n",
    "        policy_evaluation(policy)\n",
    "        if policy_improvement():\n",
    "            break\n",
    "    return policy\n",
    "\n",
    "# Run policy iteration\n",
    "optimal_policy = policy_iteration()\n",
    "print(\"Optimal Policy (Policy Iteration):\", optimal_policy)\n",
    "print(\"Optimal Values (Policy Iteration):\", V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
