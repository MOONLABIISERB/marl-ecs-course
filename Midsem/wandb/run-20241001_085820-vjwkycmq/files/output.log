Episode 0,  Reward: 107.04, Profit: 97.96, Distance: 70.04 Loss: 1000.00 Goal Locations: [[ 5.6181016 14.260715 ]
 [10.979909   8.979877 ]
 [ 2.3402796  2.339918 ]
 [ 0.8712542 12.992642 ]
 [ 9.016726  10.621089 ]
 [ 0.3087674 14.548648 ]
 [12.48664    3.1850867]
 [ 2.7273746  2.7510676]
 [ 4.5636334  7.8713465]
 [ 6.479175   4.3684373]]
Episode 100,  Reward: 212.58, Profit: 268.24, Distance: 57.74 Loss: 0.17 Goal Locations: [[ 5.6181016 14.260715 ]
 [10.979909   8.979877 ]
 [ 2.3402796  2.339918 ]
 [ 0.8712542 12.992642 ]
 [ 9.016726  10.621089 ]
 [ 0.3087674 14.548648 ]
 [12.48664    3.1850867]
 [ 2.7273746  2.7510676]
 [ 4.5636334  7.8713465]
 [ 6.479175   4.3684373]]
Episode 200,  Reward: 121.87, Profit: 166.49, Distance: 66.70 Loss: 1.57 Goal Locations: [[ 5.6181016 14.260715 ]
 [10.979909   8.979877 ]
 [ 2.3402796  2.339918 ]
 [ 0.8712542 12.992642 ]
 [ 9.016726  10.621089 ]
 [ 0.3087674 14.548648 ]
 [12.48664    3.1850867]
 [ 2.7273746  2.7510676]
 [ 4.5636334  7.8713465]
 [ 6.479175   4.3684373]]
Episode 300,  Reward: 208.72, Profit: 209.02, Distance: 58.26 Loss: 1.98 Goal Locations: [[ 5.6181016 14.260715 ]
 [10.979909   8.979877 ]
 [ 2.3402796  2.339918 ]
 [ 0.8712542 12.992642 ]
 [ 9.016726  10.621089 ]
 [ 0.3087674 14.548648 ]
 [12.48664    3.1850867]
 [ 2.7273746  2.7510676]
 [ 4.5636334  7.8713465]
 [ 6.479175   4.3684373]]
Episode 400,  Reward: 115.48, Profit: 141.83, Distance: 72.36 Loss: 3.07 Goal Locations: [[ 5.6181016 14.260715 ]
 [10.979909   8.979877 ]
 [ 2.3402796  2.339918 ]
 [ 0.8712542 12.992642 ]
 [ 9.016726  10.621089 ]
 [ 0.3087674 14.548648 ]
 [12.48664    3.1850867]
 [ 2.7273746  2.7510676]
 [ 4.5636334  7.8713465]
 [ 6.479175   4.3684373]]
Episode 500,  Reward: 205.79, Profit: 212.07, Distance: 56.10 Loss: 3.15 Goal Locations: [[ 5.6181016 14.260715 ]
 [10.979909   8.979877 ]
 [ 2.3402796  2.339918 ]
 [ 0.8712542 12.992642 ]
 [ 9.016726  10.621089 ]
 [ 0.3087674 14.548648 ]
 [12.48664    3.1850867]
 [ 2.7273746  2.7510676]
 [ 4.5636334  7.8713465]
 [ 6.479175   4.3684373]]
Episode 600,  Reward: 292.71, Profit: 344.71, Distance: 45.34 Loss: 3.46 Goal Locations: [[ 5.6181016 14.260715 ]
 [10.979909   8.979877 ]
 [ 2.3402796  2.339918 ]
 [ 0.8712542 12.992642 ]
 [ 9.016726  10.621089 ]
 [ 0.3087674 14.548648 ]
 [12.48664    3.1850867]
 [ 2.7273746  2.7510676]
 [ 4.5636334  7.8713465]
 [ 6.479175   4.3684373]]
Episode 700,  Reward: 173.66, Profit: 199.34, Distance: 68.93 Loss: 3.73 Goal Locations: [[ 5.6181016 14.260715 ]
 [10.979909   8.979877 ]
 [ 2.3402796  2.339918 ]
 [ 0.8712542 12.992642 ]
 [ 9.016726  10.621089 ]
 [ 0.3087674 14.548648 ]
 [12.48664    3.1850867]
 [ 2.7273746  2.7510676]
 [ 4.5636334  7.8713465]
 [ 6.479175   4.3684373]]
Episode 800,  Reward: 230.53, Profit: 238.23, Distance: 55.32 Loss: 2.65 Goal Locations: [[ 5.6181016 14.260715 ]
 [10.979909   8.979877 ]
 [ 2.3402796  2.339918 ]
 [ 0.8712542 12.992642 ]
 [ 9.016726  10.621089 ]
 [ 0.3087674 14.548648 ]
 [12.48664    3.1850867]
 [ 2.7273746  2.7510676]
 [ 4.5636334  7.8713465]
 [ 6.479175   4.3684373]]
Episode 900,  Reward: 329.18, Profit: 357.48, Distance: 38.12 Loss: 2.41 Goal Locations: [[ 5.6181016 14.260715 ]
 [10.979909   8.979877 ]
 [ 2.3402796  2.339918 ]
 [ 0.8712542 12.992642 ]
 [ 9.016726  10.621089 ]
 [ 0.3087674 14.548648 ]
 [12.48664    3.1850867]
 [ 2.7273746  2.7510676]
 [ 4.5636334  7.8713465]
 [ 6.479175   4.3684373]]
Episode 1000,  Reward: 302.91, Profit: 337.27, Distance: 44.18 Loss: 2.49 Goal Locations: [[ 5.6181016 14.260715 ]
 [10.979909   8.979877 ]
 [ 2.3402796  2.339918 ]
 [ 0.8712542 12.992642 ]
 [ 9.016726  10.621089 ]
 [ 0.3087674 14.548648 ]
 [12.48664    3.1850867]
 [ 2.7273746  2.7510676]
 [ 4.5636334  7.8713465]
 [ 6.479175   4.3684373]]
Traceback (most recent call last):
  File "/Users/agam/projects/marl-ecs-course/Midsem/modified_tsp.py", line 131, in <module>
    main()
  File "/Users/agam/projects/marl-ecs-course/Midsem/modified_tsp.py", line 58, in main
    action = agent.action(obs, exploration_prob=epsilon)
  File "/Users/agam/projects/marl-ecs-course/Midsem/agent.py", line 87, in action
    q_values = self.knowledge(state_tensor)
  File "/Users/agam/miniconda3/envs/marl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/agam/miniconda3/envs/marl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/agam/projects/marl-ecs-course/Midsem/agent.py", line 53, in forward
    x = F.relu(self.fc1(x))
  File "/Users/agam/miniconda3/envs/marl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/Users/agam/miniconda3/envs/marl/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/Users/agam/miniconda3/envs/marl/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
