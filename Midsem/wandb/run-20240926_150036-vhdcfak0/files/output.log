Episode 0, Avg Loss: 0.0000, Reward: 10.0
Episode 10, Avg Loss: 0.0000, Reward: 8.0
Episode 20, Avg Loss: 0.0000, Reward: 9.0
Episode 30, Avg Loss: 0.0000, Reward: 10.0
Episode 40, Avg Loss: 0.0000, Reward: 9.0
Episode 50, Avg Loss: 0.0000, Reward: 9.0
Episode 60, Avg Loss: 0.0518, Reward: 10.0
Episode 70, Avg Loss: 0.1063, Reward: 9.0
Episode 80, Avg Loss: 0.1758, Reward: 8.0
Episode 90, Avg Loss: 0.2060, Reward: 9.0
Episode 100, Avg Loss: 0.2075, Reward: 11.0
Episode 110, Avg Loss: 0.2175, Reward: 10.0
Episode 120, Avg Loss: 0.2258, Reward: 10.0
Episode 130, Avg Loss: 0.2630, Reward: 10.0
Episode 140, Avg Loss: 0.3021, Reward: 10.0
Episode 150, Avg Loss: 0.3410, Reward: 9.0
Episode 160, Avg Loss: 0.3440, Reward: 9.0
Episode 170, Avg Loss: 0.2840, Reward: 11.0
Episode 180, Avg Loss: 0.2257, Reward: 11.0
Episode 190, Avg Loss: 0.1444, Reward: 10.0
Episode 200, Avg Loss: 0.1939, Reward: 28.0
Episode 210, Avg Loss: 0.1305, Reward: 118.0
Episode 220, Avg Loss: 0.1012, Reward: 125.0
Episode 230, Avg Loss: 0.0946, Reward: 137.0
Episode 240, Avg Loss: 0.0926, Reward: 171.0
Episode 250, Avg Loss: 0.0891, Reward: 92.0
Episode 260, Avg Loss: 0.0992, Reward: 148.0
Episode 270, Avg Loss: 0.0869, Reward: 142.0
Episode 280, Avg Loss: 0.0846, Reward: 141.0
Episode 290, Avg Loss: 0.0886, Reward: 122.0
Episode 300, Avg Loss: 0.0867, Reward: 131.0
Episode 310, Avg Loss: 0.0802, Reward: 119.0
Episode 320, Avg Loss: 0.0853, Reward: 32.0
Episode 330, Avg Loss: 0.0823, Reward: 20.0
Episode 340, Avg Loss: 0.0943, Reward: 124.0
Episode 350, Avg Loss: 0.0828, Reward: 97.0
Episode 360, Avg Loss: 0.0872, Reward: 91.0
Episode 370, Avg Loss: 0.0678, Reward: 133.0
Episode 380, Avg Loss: 0.0646, Reward: 121.0
Episode 390, Avg Loss: 0.0522, Reward: 116.0
Episode 400, Avg Loss: 0.0555, Reward: 114.0
Episode 410, Avg Loss: 0.0780, Reward: 84.0
Episode 420, Avg Loss: 0.0564, Reward: 69.0
Episode 430, Avg Loss: 0.0493, Reward: 492.0
Episode 440, Avg Loss: 0.0638, Reward: 422.0
Episode 450, Avg Loss: 0.0536, Reward: 500.0
Episode 460, Avg Loss: 0.0670, Reward: 500.0
Traceback (most recent call last):
  File "/home/protomate/marl-ecs-course/Midsem/agent.py", line 133, in <module>
    loss = agent.learn_from_memory(batch_size=batchSize)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/protomate/marl-ecs-course/Midsem/agent.py", line 96, in learn_from_memory
    self.optimizer.step()
  File "/home/protomate/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/optim/optimizer.py", line 484, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/protomate/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/optim/optimizer.py", line 89, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/protomate/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/optim/adam.py", line 226, in step
    adam(
  File "/home/protomate/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/optim/optimizer.py", line 161, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/protomate/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/optim/adam.py", line 766, in adam
    func(
  File "/home/protomate/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/optim/adam.py", line 585, in _multi_tensor_adam
    1 - beta1 ** _get_value(step) for step in device_state_steps
                 ^^^^^^^^^^^^^^^^
  File "/home/protomate/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/optim/optimizer.py", line 104, in _get_value
    return x.item() if isinstance(x, torch.Tensor) else x
           ^^^^^^^^
KeyboardInterrupt
