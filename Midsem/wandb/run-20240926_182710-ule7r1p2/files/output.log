Episode 0, Avg Loss: 1000.0000, Reward: -290.53983306884766, exploration: 0.7000
Episode 100, Avg Loss: 953.7281, Reward: -44298.8642578125, exploration: 0.6663
Traceback (most recent call last):
  File "/home/protomate/marl-ecs-course/Midsem/modified_tsp.py", line 80, in <module>
    main()
  File "/home/protomate/marl-ecs-course/Midsem/modified_tsp.py", line 53, in main
    loss = agent.learn_from_memory(batch_size=batchSize)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/protomate/marl-ecs-course/Midsem/agent.py", line 96, in learn_from_memory
    state_action_values = self.knowledge(state_batch).gather(1, action_batch)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/protomate/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/protomate/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/protomate/marl-ecs-course/Midsem/agent.py", line 51, in forward
    x = F.relu(self.fc1(x))
        ^^^^^^^^^^^^^^^^^^^
  File "/home/protomate/.pyenv/versions/3.12.4/lib/python3.12/site-packages/torch/nn/functional.py", line 1500, in relu
    result = torch.relu(input)
             ^^^^^^^^^^^^^^^^^
KeyboardInterrupt
