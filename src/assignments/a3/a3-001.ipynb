{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with '.venv (Python 3.12.7)' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/sattwik/code/coursework/ecs427/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Tensordict modules\n",
    "from tensordict.nn import TensorDictModule\n",
    "from tensordict.nn.distributions import NormalParamExtractor\n",
    "from torch import multiprocessing\n",
    "\n",
    "# Data collection for training\n",
    "from torchrl.collectors import SyncDataCollector\n",
    "from torchrl.data.replay_buffers import ReplayBuffer\n",
    "from torchrl.data.replay_buffers.samplers import SamplerWithoutReplacement\n",
    "from torchrl.data.replay_buffers.storages import LazyTensorStorage\n",
    "\n",
    "# Environment\n",
    "from torchrl.envs import RewardSum, TransformedEnv\n",
    "from torchrl.envs.libs.vmas import VmasEnv\n",
    "from torchrl.envs.utils import check_env_specs\n",
    "\n",
    "# Multi-Agent Network\n",
    "from torchrl.modules import MultiAgentMLP, ProbabilisticActor, TanhNormal\n",
    "\n",
    "# Objectives\n",
    "from torchrl.objectives import ClipPPOLoss, ValueEstimators\n",
    "\n",
    "# Utils\n",
    "from matplotlib import pyplot as plt\n",
    "from rich.console import Console\n",
    "from rich.progress import Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "console = Console()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the seed\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define Hyperparameters\n",
    "is_fork = multiprocessing.get_start_method() == \"fork\"\n",
    "device = (\n",
    "    torch.device(0)\n",
    "    if torch.cuda.is_available() and not is_fork\n",
    "    else torch.device(\"cpu\")\n",
    ")\n",
    "vmas_device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling config\n",
    "frames_per_batch = 1_000\n",
    "n_iters = 10\n",
    "total_frames = frames_per_batch * n_iters\n",
    "\n",
    "# Training config\n",
    "n_epochs = 30\n",
    "minibatch_size = 400\n",
    "lr = 1e-4  # Learning rate\n",
    "max_grad_norm = 1.0  # Max norm for gradients\n",
    "\n",
    "# PPO config\n",
    "clip_epsilon = 0.2\n",
    "gamma = 0.99\n",
    "lmbda = 0.9\n",
    "entropy_eps = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create VMAS Navigation env\n",
    "max_steps = 100\n",
    "n_vmas_envs = frames_per_batch // max_steps\n",
    "scenario_name = \"navigation\"\n",
    "n_agents = 3\n",
    "\n",
    "env = VmasEnv(\n",
    "    scenario=scenario_name,\n",
    "    num_envs=n_vmas_envs,\n",
    "    max_steps=max_steps,\n",
    "    continuous_actions=True,\n",
    "    device=vmas_device,\n",
    "    # Custom args for navigation env\n",
    "    n_agents=n_agents,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(\"action_spec:\", env.full_action_spec)\n",
    "console.print(\"reward_spec:\", env.full_reward_spec)\n",
    "console.print(\"done_spec:\", env.full_done_spec)\n",
    "console.print(\"observation_spec:\", env.observation_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(f\"Action keys: {env.action_keys}\")\n",
    "console.print(f\"Reward keys: {env.reward_keys}\")\n",
    "console.print(f\"Done key: {env.done_keys}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TransformedEnv(\n",
    "    env=env,\n",
    "    transform=RewardSum(\n",
    "        in_keys=[env.reward_key], out_keys=[(\"agents\", \"episode_reward\")]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env_specs(env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 5\n",
    "rollout = env.rollout(max_steps=n_steps)\n",
    "\n",
    "console.print(f\"Rollout of {n_steps} steps: {rollout}\")\n",
    "console.print(f\"Shape of rollout TensorDict = {rollout.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_params = True\n",
    "policy_net = torch.nn.Sequential(\n",
    "    MultiAgentMLP(\n",
    "            n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "            n_agent_outputs=env.n_agents,\n",
    "            n_agents=env.n_agents,\n",
    "            centralized=False,\n",
    "            share_params=share_params,\n",
    "            device=device,\n",
    "            depth=2,\n",
    "            num_cells=256,\n",
    "            activation_class=torch.nn.Tanh,\n",
    "    ),\n",
    "    NormalParamExtractor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_module = TensorDictModule(\n",
    "    policy_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = ProbabilisticActor(\n",
    "    module=policy_module,\n",
    "    spec=env.unbatched_action_spec,\n",
    "    in_keys=[(\"agents\", \"loc\"), (\"agents\", \"scale\")],\n",
    "    out_keys=[env.action_key],\n",
    "    distribution_class=TanhNormal,\n",
    "    distribution_kwargs={\n",
    "        \"low\": env.unbatched_action_spec[env.action_key].space.low,\n",
    "        \"high\": env.unbatched_action_spec[env.action_key].space.high,\n",
    "    },\n",
    "    return_log_prob=True,\n",
    "    log_prob_key=(\"agents\", \"sample_log_prob\")  # Log proba required for PPO loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_critic_params = True\n",
    "mappo = True\n",
    "\n",
    "critic_net = MultiAgentMLP(\n",
    "\n",
    "    n_agent_inputs=env.observation_spec[\"agents\", \"observation\"].shape[-1],\n",
    "    n_agent_outputs=1,\n",
    "    n_agents=env.n_agents,\n",
    "    centralized=mappo,\n",
    "    share_params=share_critic_params,\n",
    "    device=device,\n",
    "    depth=2,\n",
    "    num_cells=256,\n",
    "    activation_class=torch.nn.Tanh\n",
    ")\n",
    "\n",
    "critic = TensorDictModule(\n",
    "    module=critic_net,\n",
    "    in_keys=[(\"agents\", \"observation\")],\n",
    "    out_keys=[(\"agents\", \"state_value\")],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "console.print(f\"Running Policy: {policy(env.reset())}\")\n",
    "console.print(f\"Running Critic: {critic(env.reset())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "collector = SyncDataCollector(\n",
    "    env,\n",
    "    policy,\n",
    "    device=vmas_device,\n",
    "    storing_device=device,\n",
    "    frames_per_batch=frames_per_batch,\n",
    "    total_frames=total_frames\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer(\n",
    "    storage=LazyTensorStorage(\n",
    "        frames_per_batch,\n",
    "        device=device\n",
    "    ),\n",
    "    sampler=SamplerWithoutReplacement(),\n",
    "    batch_size=minibatch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_module = ClipPPOLoss(\n",
    "    actor_network=policy,\n",
    "    critic_network=critic,\n",
    "    clip_epsilon=clip_epsilon,\n",
    "    entropy_coef=entropy_eps,\n",
    "    normalize_advantage=False\n",
    ")\n",
    "loss_module.set_keys(\n",
    "    reward=env.reward_key,\n",
    "    action=env.action_key,\n",
    "    sample_log_prob=(\"agents\", \"state_value\"),\n",
    "    done=(\"agents\", \"done\"),\n",
    "    terminated=(\"agents\", \"terminated\"),\n",
    ")\n",
    "\n",
    "loss_module.make_value_estimator(\n",
    "    ValueEstimators.GAE, gamma=gamma, lmbda=lmbda\n",
    ")\n",
    "GAE = loss_module.value_estimator\n",
    "optim = torch.optim.Adam(loss_module.parameters(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
