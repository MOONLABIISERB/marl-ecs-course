{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Shx_nBcNh_1U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Shx_nBcNh_1U",
    "outputId": "ef4f8b38-5f90-42a8-f2f6-f324ca3b5f6a"
   },
   "outputs": [],
   "source": [
    "!pip install gym pettingzoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee8b2b",
   "metadata": {
    "id": "34ee8b2b"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import copy\n",
    "from gymnasium.spaces import Discrete , MultiDiscrete\n",
    "from pettingzoo import ParallelEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qdZY7eGyv_W9",
   "metadata": {
    "id": "qdZY7eGyv_W9"
   },
   "source": [
    "### ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642d3e8b",
   "metadata": {
    "id": "642d3e8b"
   },
   "outputs": [],
   "source": [
    "class GridWorld(ParallelEnv):\n",
    "    metadata={\n",
    "\n",
    "        \"name\":\"Grid World v0\"\n",
    "\n",
    "    }\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "\n",
    "        self.player1=[1,1]\n",
    "        self.player2=[1,8]\n",
    "        self.player3=[8,1]\n",
    "        self.player4=[8,8]\n",
    "        #[[1, 1], [1, 8], [8, 1], [8, 8]]\n",
    "        #[[5, 8], [8, 5], [1, 5], [8, 4]]\n",
    "\n",
    "\n",
    "        self.target1=[5,8]\n",
    "        self.target2=[8,4]\n",
    "        self.target3=[1,5]\n",
    "        self.target4=[5,1]\n",
    "        self.walls=[[0,4],[1,4],[2,4],[2,5],[4,0],[4,1],[4,2],[5,2],[5,7],[4,7],[4,8],[4,9],[7,4],[7,5],[8,5],[9,5]]\n",
    "        self.players_index=[self.player1,self.player2,self.player3,self.player4]\n",
    "        self.target_index=[self.target1,self.target2,self.target3,self.target4]\n",
    "        self.actor1_state,self.actor2_state,self.actor3_state,self.actor4_state,self.target_state=self.create_board(self.players_index,self.target_index)\n",
    "        self.timestep=0\n",
    "        self.possible_agents=('green','purple','blue','yellow')\n",
    "        self.rewards={a:0 for a in self.possible_agents}\n",
    "\n",
    "\n",
    "    def create_board(self, player_index, targets_index):\n",
    "        \"\"\"Creates the game board with walls, players, and targets.\"\"\"\n",
    "        # Create a 3D array where each layer corresponds to a player's board\n",
    "        board = np.zeros((4, 10, 10))  # 4 layers for 4 players\n",
    "        board2 = np.zeros((10, 10))    # Separate board for targets and walls\n",
    "\n",
    "        # Set walls on all boards\n",
    "        for (a, b) in self.walls:\n",
    "            for i in range(4):  # Apply to all player boards\n",
    "                board[i][a, b] = -1\n",
    "            board2[a, b] = -1   # Also add walls to the target board\n",
    "\n",
    "        # Place players on their respective boards\n",
    "        for i, (a, b) in enumerate(player_index):\n",
    "            if board[i][a, b] != -1:  # Ensure no overlap with walls\n",
    "                board[i][a, b] = i+1\n",
    "            else:\n",
    "                print(f\"Warning: Player {i + 1} position [{a}, {b}] overlaps with an existing element!\")\n",
    "\n",
    "        # Place targets on the target board\n",
    "        for i, (a, b) in enumerate(targets_index):\n",
    "            if board2[a, b] != -1:  # Ensure no overlap with walls\n",
    "                board2[a, b] = (i + 1) * 5\n",
    "            else:\n",
    "                print(f\"Warning: Target {i + 1} position [{a}, {b}] overlaps with an existing element!\")\n",
    "\n",
    "        # Return each player's board and the target board\n",
    "        return board[0], board[1], board[2], board[3], board2\n",
    "\n",
    "    def update_player_position(self, player, action,board):\n",
    "      row, column = player[0], player[1]\n",
    "\n",
    "      if action == 0:\n",
    "          # position remains same\n",
    "          return [row, column]\n",
    "\n",
    "      if action == 1 and row > 0:  # up\n",
    "          new_row = row - 1\n",
    "          if board[new_row][column] != -1:\n",
    "              return [new_row, column]\n",
    "\n",
    "\n",
    "      if action == 2 and column < 9:  # right\n",
    "          new_column = column + 1\n",
    "          if board[row][new_column] != -1:\n",
    "              return [row, new_column]\n",
    "\n",
    "\n",
    "      if action == 3 and column > 0:  # left\n",
    "          new_column = column - 1\n",
    "          if board[row][new_column] != -1:\n",
    "              return [row, new_column]\n",
    "\n",
    "\n",
    "      if action == 4 and row < 9:  # down\n",
    "          new_row = row + 1\n",
    "          if board[new_row][column] != -1:\n",
    "              return [new_row, column]\n",
    "\n",
    "    # Return original position if no valid move\n",
    "      return [row, column]\n",
    "\n",
    "    def reset(self, choose:bool):\n",
    "        self.agents=copy(self.possible_agents)\n",
    "        self.walls=[[0,4],[1,4],[2,4],[2,5],[4,0],[4,1],[4,2],[5,2],[5,7],[4,7],[4,8],[4,9],[7,4],[7,5],[8,5],[9,5]]\n",
    "        self.board=np.zeros((10,10))\n",
    "        self.timestep=0\n",
    "        self.rewardz=0\n",
    "        self.player1=[1,1]\n",
    "        self.player2=[1,8]\n",
    "        self.player3=[8,1]\n",
    "        self.player4=[8,8]\n",
    "\n",
    "        if choose:\n",
    "            for i in range(4):\n",
    "                valid_target = False\n",
    "                while not valid_target:\n",
    "\n",
    "                    new_target = list(np.random.randint(9, size=2))\n",
    "\n",
    "\n",
    "                    if new_target not in self.walls:\n",
    "                        self.target[i] = new_target\n",
    "                        valid_target = True\n",
    "\n",
    "\n",
    "\n",
    "        else:\n",
    "          self.target1=[5,8]\n",
    "          self.target2=[8,4]\n",
    "          self.target3=[1,5]\n",
    "          self.target4=[5,1]\n",
    "          self.target_index=list((self.target1,self.target2,self.target3,self.target4))\n",
    "        self.rewards={a:0 for a in self.possible_agents}\n",
    "        self.players_index=list((self.player1,self.player2,self.player3,self.player4))\n",
    "\n",
    "        self.actor1_state,self.actor2_state,self.actor3_state,self.actor4_state,self.target_state=self.create_board(self.players_index,self.target_index)\n",
    "        observations= {'at1':[self.players_index[0],self.target_index[0]],'at2':[self.players_index[1],self.target_index[1]],\\\n",
    "                       'at3':[self.players_index[2],self.target_index[2]],'at4':[self.players_index[3],self.target_index[3]]}#,'board':[self.actor1_state,self.actor2_state,self.actor3_state,self.actor4_state,self.target_state]\n",
    "        #observations = np.concatenate([np.array(value).flatten() for value in observations.values()])\n",
    "        infos={a: {} for a in self.agents}\n",
    "        return observations, infos\n",
    "\n",
    "    def step(self,actions):\n",
    "        player1_action=actions[0]#['green']\n",
    "        player2_action=actions[1]#['purple']\n",
    "        player3_action=actions[2]#['blue']\n",
    "        player4_action=actions[3]#['yellow']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.player1 = self.update_player_position(self.player1, player1_action,self.actor1_state)\n",
    "        self.player2 = self.update_player_position(self.player2, player2_action,self.actor2_state)\n",
    "        self.player3 = self.update_player_position(self.player3, player3_action,self.actor3_state)\n",
    "        self.player4 = self.update_player_position(self.player4, player4_action,self.actor4_state)\n",
    "\n",
    "\n",
    "        termi1=(self.player1==self.target1)\n",
    "        termi2=(self.player2==self.target2)\n",
    "        termi3=(self.player3==self.target3)\n",
    "        termi4=(self.player4==self.target4)\n",
    "        termination=[termi1,termi2,termi3,termi4]\n",
    "\n",
    "        if not all(termination):\n",
    "            rewards=self.reward_break(termination,self.rewards)\n",
    "\n",
    "        else:\n",
    "            rewards=self.rewards\n",
    "        self.players_index=list((self.player1,self.player2,self.player3,self.player4))\n",
    "        self.target_index=list((self.target1,self.target2,self.target3,self.target4))\n",
    "        self.actor1_state,self.actor2_state,self.actor3_state,self.actor4_state,self.target_state=self.create_board(self.players_index,self.target_index)\n",
    "\n",
    "\n",
    "        truncation=True if self.timestep==1000 else False\n",
    "        rewards = {f\"agent_{i}\": reward for i, reward in enumerate(self.rewards.values())}\n",
    "        self.timestep+=1\n",
    "        observations =  {'at1':[self.players_index[0],self.target_index[0]],'at2':[self.players_index[1],self.target_index[1]],\\\n",
    "                         'at3':[self.players_index[2],self.target_index[2]],'at4':[self.players_index[3],self.target_index[3]]}#'board':[self.actor1_state,self.actor2_state,self.actor3_state,self.actor4_state,self.target_state\n",
    "        #observations = np.concatenate([np.array(value).flatten() for value in observations.values()])\n",
    "        # Get dummy infos (not used in this example)\n",
    "        infos = {a: {} for a in self.agents}\n",
    "\n",
    "        return observations, rewards, all(termination), infos\n",
    "\n",
    "    def reward_break(self,termins:list,rewards:list):\n",
    "        rew=rewards.copy()\n",
    "        for key,term in zip(rew.keys(),termins):\n",
    "            if not term:\n",
    "                rewards[key]-=1\n",
    "        return rewards\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fP1ThWTWv8k4",
   "metadata": {
    "id": "fP1ThWTWv8k4"
   },
   "source": [
    "### Try 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PosE1C0G2Egb",
   "metadata": {
    "id": "PosE1C0G2Egb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Hyperparameters\n",
    "NUM_AGENTS = 4\n",
    "STATE_SIZE = 4  # [player_position, target_position] for each agent\n",
    "ACTION_SIZE = 5  # [stay, up, right, left, down]\n",
    "GAMMA = 0.99\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = int(1e5)\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_DECAY = 0.001\n",
    "EPSILON_MIN = 0.1\n",
    "UPDATE_EVERY = 10\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XtOeYHaj2Ec3",
   "metadata": {
    "id": "XtOeYHaj2Ec3"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size,256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LVmRlfv32EUa",
   "metadata": {
    "id": "LVmRlfv32EUa"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, action_size, buffer_size, batch_size):\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "\n",
    "        return (torch.tensor(states, dtype=torch.float, device=DEVICE),\n",
    "                torch.tensor(actions, dtype=torch.long, device=DEVICE),\n",
    "                torch.tensor(rewards, dtype=torch.float, device=DEVICE),\n",
    "                torch.tensor(next_states, dtype=torch.float, device=DEVICE),\n",
    "                torch.tensor(dones, dtype=torch.float, device=DEVICE))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kcPury__2EGw",
   "metadata": {
    "id": "kcPury__2EGw"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, action_size, agent_id):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.agent_id = agent_id\n",
    "\n",
    "        self.qnetwork_local = DQN(state_size, action_size).to(DEVICE)\n",
    "        self.qnetwork_target = DQN(state_size, action_size).to(DEVICE)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=LR)\n",
    "\n",
    "        self.memory = ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE)\n",
    "        self.t_step = 0\n",
    "        self.epsilon = EPSILON_START\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() > self.epsilon:\n",
    "            state = torch.tensor(state, dtype=torch.float, device=DEVICE).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                action_values = self.qnetwork_local(state)\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "\n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY steps\n",
    "        self.t_step += 1\n",
    "        if self.t_step % UPDATE_EVERY == 0 and len(self.memory) > BATCH_SIZE:\n",
    "            experiences = self.memory.sample()\n",
    "            self.learn(experiences, GAMMA)\n",
    "\n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        Q_expected = self.qnetwork_local(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        Q_targets_next = self.qnetwork_target(next_states).detach().max(1)[0]\n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "\n",
    "        loss = nn.MSELoss()(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network\n",
    "        if self.t_step % 100 == 0:\n",
    "            self.soft_update(self.qnetwork_local, self.qnetwork_target, tau=1e-3)\n",
    "\n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qP7TMJCu2e_2",
   "metadata": {
    "id": "qP7TMJCu2e_2"
   },
   "outputs": [],
   "source": [
    "# Initialize agents\n",
    "agents = [Agent(STATE_SIZE, ACTION_SIZE, i) for i in range(NUM_AGENTS)]\n",
    "\n",
    "def train(env, n_episodes=1000, max_t=100):\n",
    "    scores = []\n",
    "    for i_episode in range(1, n_episodes + 1):\n",
    "        obs, _ = env.reset(False)\n",
    "        states = [np.array(obs[f'at{i+1}']).flatten() for i in range(NUM_AGENTS)]\n",
    "        scores_episode = np.zeros(NUM_AGENTS)\n",
    "\n",
    "        for t in range(max_t):\n",
    "            actions = [agents[i].act(states[i]) for i in range(NUM_AGENTS)]\n",
    "            next_obs, rewards, done, _ = env.step(actions)\n",
    "\n",
    "            next_states = [np.array(next_obs[f'at{i+1}']).flatten() for i in range(NUM_AGENTS)]\n",
    "            for i in range(NUM_AGENTS):\n",
    "                agents[i].step(states[i], actions[i], rewards[f\"agent_{i}\"], next_states[i], done)\n",
    "                scores_episode[i] += rewards[f\"agent_{i}\"]\n",
    "\n",
    "            states = next_states\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        scores.append(np.mean(scores_episode))\n",
    "        for agent in agents:\n",
    "            agent.epsilon = max(EPSILON_MIN, agent.epsilon * EPSILON_DECAY)\n",
    "\n",
    "        if i_episode % 100 == 0:\n",
    "            print(f\"Episode {i_episode}/{n_episodes}, Average Score: {np.mean(scores[-100:]):.2f}\")\n",
    "\n",
    "    #return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vjyifvtx2e0F",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "id": "Vjyifvtx2e0F",
    "outputId": "ba1f452a-6243-4c92-9e8f-0e96ce285245"
   },
   "outputs": [],
   "source": [
    "env=GridWorld()\n",
    "train(env) #increase number of episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34l6sUBO2D1m",
   "metadata": {
    "id": "34l6sUBO2D1m"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "W7unBjBCu4Rn",
   "metadata": {
    "id": "W7unBjBCu4Rn"
   },
   "source": [
    "### Try2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XMFTt816iqTm",
   "metadata": {
    "id": "XMFTt816iqTm"
   },
   "outputs": [],
   "source": [
    "class Q_Learning:\n",
    "    def __init__(self, epsilon, env):\n",
    "        self.epsilon = epsilon\n",
    "        self.env = env\n",
    "        self.qtable = [self.init_q_table() for _ in range(4)]  # Separate Q-tables for each agent\n",
    "\n",
    "    def init_q_table(self):\n",
    "        \"\"\"Initialize the Q-table for the grid.\"\"\"\n",
    "        key = {}\n",
    "        for row in range(10):\n",
    "            for column in range(10):\n",
    "                if [row, column] not in self.env.walls:  # Valid positions\n",
    "                    key[(row, column)] = np.zeros(5)  # 5 possible actions\n",
    "                else:\n",
    "                    key[(row, column)] = np.full(5, -1000)  # Penalize wall positions\n",
    "        return key\n",
    "\n",
    "    def epsilon_decay(self, decay=0.005, min_epsilon=0.01):\n",
    "        \"\"\"Decay epsilon over time.\"\"\"\n",
    "        self.epsilon = max(self.epsilon - decay, min_epsilon)\n",
    "        return self.epsilon\n",
    "\n",
    "    def select_action(self, state, agent_index):\n",
    "        \"\"\"Select an action using epsilon-greedy policy.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(5)  # Random action\n",
    "        else:\n",
    "            return np.argmax(self.qtable[agent_index][tuple(state)])  # Greedy action\n",
    "\n",
    "    def learn(self, tot_epi, max_steps, gamma,alpha):\n",
    "\n",
    "        cum_rewards = []\n",
    "\n",
    "        for epi in range(tot_epi):\n",
    "\n",
    "            obs, _ = self.env.reset(False)\n",
    "            states = [obs['at1'][0], obs['at2'][0], obs['at3'][0], obs['at4'][0]]\n",
    "            total_reward = [0, 0, 0, 0]\n",
    "\n",
    "            for step in range(max_steps):\n",
    "                actions = [self.select_action(state, i) for i, state in enumerate(states)]\n",
    "\n",
    "\n",
    "                new_obs, rewards, terminations, _ = self.env.step(actions)\n",
    "                new_states = [new_obs['at1'][0], new_obs['at2'][0], new_obs['at3'][0], new_obs['at4'][0]]\n",
    "                #avg=0\n",
    "                #for i in range(4):\n",
    "                #\n",
    "                #   avg+=rewards*[f\"agent_{i}\"]*np.max(np.array(list(self.qtable[i][new_states[i]])))\n",
    "\n",
    "                for i in range(4):\n",
    "                    old_state = tuple(states[i])\n",
    "                    new_state = tuple(new_states[i])\n",
    "                    reward = rewards[f\"agent_{i}\"]\n",
    "\n",
    "\n",
    "                    best_future_q = np.max(self.qtable[i][new_state])\n",
    "                    current_q = self.qtable[i][old_state][actions[i]]\n",
    "                    self.qtable[i][old_state][actions[i]] +=alpha*(\n",
    "                        reward + gamma * best_future_q - current_q\n",
    "                    )\n",
    "\n",
    "                    # Accumulate rewards\n",
    "                    total_reward[i] += reward\n",
    "\n",
    "                states = new_states  # Update states\n",
    "\n",
    "                # Break loop if all agents are done\n",
    "                if terminations:\n",
    "                    break\n",
    "\n",
    "            # Decay epsilon\n",
    "            self.epsilon_decay()\n",
    "\n",
    "            # Track cumulative reward\n",
    "            cum_rewards.append(total_reward)\n",
    "            print(f'Reward for agents in episode {epi}:{cum_rewards[-1]}')\n",
    "\n",
    "        return cum_rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AwoIp9i5CfgB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AwoIp9i5CfgB",
    "outputId": "3bc056a5-becd-495d-ef4e-76e955f6cd72"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "env=GridWorld()\n",
    "p=Q_Learning(0.9,env)\n",
    "reward_array=p.learn(100000,100,0.55,0.46)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Bj2naOMVmgxJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Bj2naOMVmgxJ",
    "outputId": "58b0d16e-d756-40ad-da18-53df2ab871ba"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FrXUgbxWR4hw",
   "metadata": {
    "id": "FrXUgbxWR4hw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "EWttxQcFSipQ",
   "metadata": {
    "id": "EWttxQcFSipQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wDQQYzqLS9VV",
   "metadata": {
    "id": "wDQQYzqLS9VV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
