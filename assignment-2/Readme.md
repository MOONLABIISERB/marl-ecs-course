**Q1: TSP**

*Dynamic Programming Results:*

Episode 0 Q-values:

[[-1000. -1000. -1000. ... 0. 0. 0.]
 [-1000. 0. 0. ... 0. 0. 0.]
 [-1000. 0. 0. ... 0. 0. 0.]
 ...
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]
 [0. 0. 0. ... 0. 0. 0.]]


 
Episode 200, Q-values: 

[[-6895.8706381  -6581.5541625  -6519.2246625  ... -6519.2246625
  -6456.5488875  -6469.049415  ]
 [-4525.42857494 -3970.099875   -3714.025      ... -3970.099875
  -3970.8715125  -3978.1817625 ]
 [-3294.922125   -3970.099875   -3970.099875   ... -3970.099875
  -3970.099875   -2993.1475    ]
 ...
 [-3294.922125   -3970.099875   -3970.099875   ... -3970.099875
  -2994.004875   -3010.249875  ]
 [-3364.177125   -3970.099875   -3970.099875   ... -3970.099875
  -3970.099875   -2993.1475    ]
 [-3574.985925   -3970.099875   -3970.099875   ... -3241.099875
  -3970.099875   -3970.099875  ]]


 
Episode 400, Q-values: 

[[-10729.08403731 -10538.7216394  -11086.0305817  ... -11038.91900871
  -11001.87748592 -10926.04032572]
 [ -7033.36385712  -7861.39128491  -7376.53638806 ...  -6900.42879582
   -6905.15280702  -6914.69849037]
 [ -7050.19404009  -7860.42524501  -7861.39128491 ...  -6909.27114066
   -6921.37264444  -6944.96903969]
 ...
 [ -7062.13226508  -7861.39128491  -7861.39128491 ...  -7861.39128491
   -6911.63416287  -6983.93572576]
 [ -7112.61945697  -7850.64367034  -7861.39128491 ...  -7861.66976541
   -7861.39128491  -6916.96648391]
 [ -7446.70860542  -7860.33947629  -7861.37808808 ...  -7294.922235
   -7861.62760739  -7861.39128491]]


   
Episode 600, Q-values: 

[[-15413.77520515 -14718.22685922 -15134.97229088 ... -15031.40328936
  -15086.13522274 -15000.27511636]
 [-10918.73242579 -11675.43861712 -11046.11908475 ... -10732.11224975
  -10736.87223606 -10755.01111518]
 [-10947.07715342 -11672.20429331 -11675.43861712 ... -10760.37998548
  -10784.03444449 -10840.77843004]
 ...
 [-10994.77955271 -10734.71594417 -11675.43861712 ... -11675.43861712
  -10748.89206601 -10825.58135505]
 [-10994.6789545  -11636.10513579 -11665.91880004 ... -10729.60288894
  -11675.43861712 -10767.55832636]
 [-11362.64272071 -11672.37159698 -11674.5065216  ... -11068.16108532
  -10729.31522239 -11675.43861712]]


  
Episode 800, Q-values:

[[-19077.90394508 -18644.76082558 -18945.85738086 ... -18722.95024952
  -18929.79456011 -18831.61803291]
 [-14809.18313012 -15413.77520515 -14704.62099453 ... -15417.57286809
  -14493.75803234 -14514.69134276]
 [-14897.67541064 -15410.19112615 -15413.77520515 ... -14536.80317146
  -14578.42260139 -14638.70567582]
 ...
 [-14995.51209719 -14491.06672118 -15413.78839564 ... -15413.77520515
  -14502.0001376  -14598.6312    ]
 [-14937.76949537 -15356.67617954 -15396.68219045 ... -14488.75436804
  -15413.77520515 -14533.19445436]
 [-15306.35478833 -15410.14147435 -15413.84922501 ... -14748.77572657
  -14487.4025027  -15413.77520515]]


  
Optimal Policy: [32 48 46 27 49 40 45 48 45 47 45 41 41 46 46 45 43 41 46 45 40 43 46 44
 38 46 39 48 45 47 46 43 44 46 44 40 41 41 44 42 43 40 39 46 23 17 12 12
 36 49]




*Monte Carlo Method:*


First Visit:

[first visit] Episode 0, Reward: -450073.71245972626

[first visit] Episode 200, Reward: -370162.94211850595

[first visit] Episode 400, Reward: -340181.72428274265

[first visit] Episode 600, Reward: -350232.7555967795

[first visit] Episode 800, Reward: -310259.48477476713


Optimal Policy (First Visit): [ 0 40 30 43 39 25 35 13 39 32 14 39 43 46 23 34 37  6 35 48  8 49 39  5
 28 13  8 28 19 33 31 48 48  9 19 49 12 19 24  6  4 11 45  4 43 21  4 21
  4 16]


Every Visit:

[every visit] Episode 0, Reward: -450077.99397766375

[every visit] Episode 200, Reward: -270359.10190867144

[every visit] Episode 400, Reward: -160558.9483436579

[every visit] Episode 600, Reward: -220417.5350452337

[every visit] Episode 800, Reward: -280311.3135306718

Optimal Policy (Every Visit): [34  9 34 46 47 42  4 21 27 24 39  8 48 27 22 17 30 48 12  8 14 18 26 31
  2 15 23 22 45 48 43 32 15  1 18 25 43  8 40 11 10  5 40 36  6 24 20  9
 16  3]



Dynamic Programming (DP) Methods: DP methods like Value Iteration and Policy Iteration calculate the value of each state by repeatedly applying the Bellman equation. These methods need a detailed understanding of the environment, including all possible state transitions and rewards. Because of this, they're best suited for simpler problems where everything about the environment is known and not too complex.


Monte Carlo (MC) Methods: MC methods work by observing what happens in many playthroughs or episodes and averaging the results to estimate how good different states or actions are. Unlike DP, MC methods don't require knowing all the details of the environment beforehand, making them more adaptable to complex situations where it's hard to predict what might happen next.



**Q2: Sokoban Puzzle**


*Dynamic Programming Optimal Value Function for Sokoban:* 

[[-6.89215401 -6.54683861 -6.16315475 -6.54683927 -6.89215534 -7.20293981]
 [-6.54683861 -6.16315475 -5.73683927 -6.16315534 -6.54683981 -6.89215583]
 [-6.16315475 -5.73683927 -5.26315534 -5.73683981 -6.16315583 -6.54684025]
 [-5.73683927 -5.26315534 -4.73683981 -5.26315583 -5.73684025 -6.16315622]
 [-5.26315534 -4.73683981 -5.26315583 -4.73684025 -5.26315622 -5.7368406 ]
 [-5.73683981 -5.26315583 -4.73684025 -5.26315622 -5.7368406  -6.16315654]
 [-6.16315583 -5.73684025 -5.26315622 -5.7368406  -6.16315654 -6.54684089]]



*Monte Carlo Optimal Values for Sokoban:*

[[ -15.66508757  -21.49422492  -26.01879128  -27.61998026  -26.52493805
   -22.01504218]
 [ -16.19813204  -28.57280048  -42.25577283  -57.53933392  -44.38977701
   -31.58453973]
 [ -17.66562612  -29.74704684  -53.96128564 -101.80692488  -55.16170944
   -33.89258518]
 [ -11.54414164  -15.6320247   -20.92031342  -48.15871245  -39.32007447
   -24.55431133]
 [  -8.93448484   -8.01692992    0.          -18.44501244  -21.16250713
   -15.39466475]
 [  -6.13379704   -8.08514112   -6.096876    -10.09204615  -15.13271069
   -12.53768911]
 [  -3.37001771   -4.60460524   -5.72760526   -8.4259349    -9.79481231
    -9.34686409]]






