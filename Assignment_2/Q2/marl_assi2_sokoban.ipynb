{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dynamic_programming.py\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class DPAgent:\n",
    "    \"\"\"\n",
    "    Agent that uses Dynamic Programming for action_policy computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "        self.all_states = self._enumerate_all_states()\n",
    "        self.state_map = {state: idx for idx, state in enumerate(self.all_states)}\n",
    "        self.num_all_states = len(self.all_states)\n",
    "        self.n_actions = environment.action_space.n\n",
    "        self.v_func = np.zeros(self.num_all_states)\n",
    "        self.action_policy = np.zeros(self.num_all_states, dtype=int)\n",
    "\n",
    "    def _enumerate_all_states(self):\n",
    "        # Generate all possible all_states\n",
    "        all_states = []\n",
    "        positions = [\n",
    "            (i, j) for i in range(self.environment.grid_height) for j in range(self.environment.grid_width)\n",
    "        ]\n",
    "        for player_pos in positions:\n",
    "            for box_pos in positions:\n",
    "                for target_pos in positions:\n",
    "                    if player_pos != box_pos and box_pos != target_pos and player_pos != target_pos:\n",
    "                        all_states.append((player_pos, box_pos, target_pos))\n",
    "        return all_states\n",
    "\n",
    "    def val_iterate(self, gamma=0.99, convergence_limit=1e-6, max_iters=1000):\n",
    "        # Perform value iteration\n",
    "        for _ in tqdm(range(max_iters), desc=\"Value Iteration Progress\"):\n",
    "            delta = 0\n",
    "            for idx, state in enumerate(self.all_states):\n",
    "                v = self.v_func[idx]\n",
    "                act_vals = []\n",
    "                for action in range(self.n_actions):\n",
    "                    new_state, return_val, terminal = self._simulate_step(state, action)\n",
    "                    if new_state in self.state_map:\n",
    "                        next_idx = self.state_map[new_state]\n",
    "                        action_value = return_val + gamma * self.v_func[next_idx] * (not terminal)\n",
    "                    else:\n",
    "                        action_value = return_val\n",
    "                    act_vals.append(action_value)\n",
    "                self.v_func[idx] = max(act_vals)\n",
    "                delta = max(delta, abs(v - self.v_func[idx]))\n",
    "            if delta < convergence_limit:\n",
    "                break\n",
    "        self._derive_action_policy(gamma)\n",
    "\n",
    "    def _simulate_step(self, state, action):\n",
    "        # Simulate the environmentironment step without modifying the actual environmentironment\n",
    "        self.environment.reset()\n",
    "        self.environment.player_position, self.environment.box_position, self.environment.target_position = state\n",
    "        self.environment.grid[self.environment.player_position[0], self.environment.player_position[1]] = self.environment.PLAYER\n",
    "        self.environment.grid[self.environment.box_position[0], self.environment.box_position[1]] = self.environment.BOX\n",
    "        self.environment.grid[self.environment.target_position[0], self.environment.target_position[1]] = self.environment.TARGET\n",
    "\n",
    "        _, return_val, terminal, _, _ = self.environment.step(action)\n",
    "        new_state = self.environment.get_state()\n",
    "\n",
    "        return new_state, return_val, terminal\n",
    "\n",
    "    def _derive_action_policy(self, gamma):\n",
    "        # Extract action_policy from the computed value function\n",
    "        for idx, state in enumerate(self.all_states):\n",
    "            act_vals = []\n",
    "            for action in range(self.n_actions):\n",
    "                new_state, return_val, terminal = self._simulate_step(state, action)\n",
    "                if new_state in self.state_map:\n",
    "                    next_idx = self.state_map[new_state]\n",
    "                    action_value = return_val + gamma * self.v_func[next_idx] * (not terminal)\n",
    "                else:\n",
    "                    action_value = return_val\n",
    "                act_vals.append(action_value)\n",
    "            self.action_policy[idx] = np.argmax(act_vals)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Select action based on the derived action_policy\n",
    "        if state in self.state_map:\n",
    "            idx = self.state_map[state]\n",
    "            return self.action_policy[idx]\n",
    "        else:\n",
    "            return self.environment.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# monte_carlo.py\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class MCAgent:\n",
    "    \"\"\"\n",
    "    Agent that uses Monte Carlo methods for action_policy computation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, environment):\n",
    "        self.environment = environment\n",
    "        self.Q_values = defaultdict(lambda: np.zeros(environment.action_space.n))\n",
    "        self.returns = defaultdict(lambda: defaultdict(list))\n",
    "        self.action_policy = {}\n",
    "        self.initial_epsilon = 1.0\n",
    "        self.minimum_epsilon = 0.01\n",
    "        self.epsilon_decay_rate = 0.995\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def _epsilon_greedy(self, state, epsilon):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() < epsilon:\n",
    "            return self.environment.action_space.sample()\n",
    "        else:\n",
    "            return np.argmax(self.Q_values[state]) if state in self.Q_values else self.environment.action_space.sample()\n",
    "\n",
    "    def generate_episode(self, epsilon):\n",
    "        # Generate an episode using the current action_policy\n",
    "        episode = []\n",
    "        self.environment.reset()\n",
    "        terminal = False\n",
    "        steps = 0\n",
    "        while not terminal and steps < 100:\n",
    "            state = self.environment.get_state()\n",
    "            action = self._epsilon_greedy(state, epsilon)\n",
    "            _, return_val, terminal, _, _ = self.environment.step(action)\n",
    "            episode.append((state, action, return_val))\n",
    "            steps += 1\n",
    "        return episode\n",
    "\n",
    "    def learn_policy(self, episode_count=10000):\n",
    "        # Train the agent using Monte Carlo method\n",
    "        epsilon = self.initial_epsilon\n",
    "        for _ in tqdm(range(episode_count), desc=\"Monte Carlo Training Progress\"):\n",
    "            episode = self.generate_episode(epsilon)\n",
    "            G = 0\n",
    "            visited_state_actions = set()\n",
    "            for state, action, return_val in reversed(episode):\n",
    "                G = self.gamma * G + return_val\n",
    "                if (state, action) not in visited_state_actions:\n",
    "                    visited_state_actions.add((state, action))\n",
    "                    self.returns[state][action].append(G)\n",
    "                    self.Q_values[state][action] = np.mean(self.returns[state][action])\n",
    "                    self.action_policy[state] = np.argmax(self.Q_values[state])\n",
    "            epsilon = max(self.minimum_epsilon, epsilon * self.epsilon_decay_rate)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        # Select action based on the learned action_policy\n",
    "        return self.action_policy.get(state, self.environment.action_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sokoban_environment.py\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "class SokobanEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Sokoban environmentironment for reinforcement learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SokobanEnv, self).__init__()\n",
    "\n",
    "        # Define the grid size\n",
    "        self.grid_height = 6\n",
    "        self.grid_width = 7\n",
    "\n",
    "        # Define action and observation space\n",
    "        # Actions: Up, Down, Left, Right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        # Observation: The grid representation\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0, high=5, shape=(self.grid_height, self.grid_width), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "        # Define grid elements\n",
    "        self.EMPTY = 0\n",
    "        self.WALL = 1\n",
    "        self.BOX = 2\n",
    "        self.TARGET = 3\n",
    "        self.PLAYER = 4\n",
    "        self.BOX_ON_TARGET = 5\n",
    "\n",
    "        # Map actions to movements\n",
    "        self.action_map = {\n",
    "            0: (-1, 0),  # Up\n",
    "            1: (1, 0),   # Down\n",
    "            2: (0, -1),  # Left\n",
    "            3: (0, 1),   # Right\n",
    "        }\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Initialize the grid\n",
    "        self.grid = np.zeros((self.grid_height, self.grid_width), dtype=int)\n",
    "        self.grid[1:-1, 1:-1] = self.EMPTY  # Set inner area to empty\n",
    "\n",
    "        # Add walls around the grid\n",
    "        self.grid[0, :] = self.WALL\n",
    "        self.grid[-1, :] = self.WALL\n",
    "        self.grid[:, 0] = self.WALL\n",
    "        self.grid[:, -1] = self.WALL\n",
    "\n",
    "        # Place the player at a random empty position\n",
    "        empty_positions = list(zip(*np.where(self.grid == self.EMPTY)))\n",
    "        self.player_position = np.array(empty_positions[np.random.choice(len(empty_positions))])\n",
    "        self.grid[self.player_position[0], self.player_position[1]] = self.PLAYER\n",
    "\n",
    "        # Place the box at a random empty position\n",
    "        empty_positions = list(zip(*np.where(self.grid == self.EMPTY)))\n",
    "        self.box_position = np.array(empty_positions[np.random.choice(len(empty_positions))])\n",
    "        self.grid[self.box_position[0], self.box_position[1]] = self.BOX\n",
    "\n",
    "        # Place the target at a random empty position\n",
    "        empty_positions = list(zip(*np.where(self.grid == self.EMPTY)))\n",
    "        self.target_position = np.array(empty_positions[np.random.choice(len(empty_positions))])\n",
    "        self.grid[self.target_position[0], self.target_position[1]] = self.TARGET\n",
    "\n",
    "        return self.grid.copy(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        move = self.action_map[action]\n",
    "        new_player_pos = self.player_position + move\n",
    "\n",
    "        # Check for wall collision\n",
    "        if self.grid[new_player_pos[0], new_player_pos[1]] == self.WALL:\n",
    "            return self.grid.copy(), -1, False, False, {}\n",
    "\n",
    "        # Check if the player is pushing the box\n",
    "        if np.array_equal(new_player_pos, self.box_position):\n",
    "            new_box_pos = self.box_position + move\n",
    "\n",
    "            # Check if the box can be moved\n",
    "            if self.grid[new_box_pos[0], new_box_pos[1]] in [self.WALL, self.BOX]:\n",
    "                return self.grid.copy(), -1, False, False, {}\n",
    "\n",
    "            # Move the box\n",
    "            self.grid[self.box_position[0], self.box_position[1]] = self.EMPTY\n",
    "            self.box_position = new_box_pos\n",
    "\n",
    "            if np.array_equal(self.box_position, self.target_position):\n",
    "                self.grid[self.box_position[0], self.box_position[1]] = self.BOX_ON_TARGET\n",
    "            else:\n",
    "                self.grid[self.box_position[0], self.box_position[1]] = self.BOX\n",
    "\n",
    "        # Move the player\n",
    "        self.grid[self.player_position[0], self.player_position[1]] = self.EMPTY\n",
    "        self.player_position = new_player_pos\n",
    "        self.grid[self.player_position[0], self.player_position[1]] = self.PLAYER\n",
    "\n",
    "        # Check if the box is on the target\n",
    "        terminal = np.array_equal(self.box_position, self.target_position)\n",
    "        return_val = 10 if terminal else -1  # Positive return_val if solved, negative otherwise\n",
    "\n",
    "        return self.grid.copy(), return_val, terminal, False, {}\n",
    "\n",
    "    def render(self, mode=\"human\"):\n",
    "        if mode == \"human\":\n",
    "            symbols = {\n",
    "                self.EMPTY: ' ',\n",
    "                self.WALL: '#',\n",
    "                self.BOX: '$',\n",
    "                self.TARGET: '.',\n",
    "                self.PLAYER: '@',\n",
    "                self.BOX_ON_TARGET: '*'\n",
    "            }\n",
    "            print(\"\\n\".join(\"\".join(symbols[cell] for cell in row) for row in self.grid))\n",
    "        elif mode == \"rgb_array\":\n",
    "            # Optional implementation for visualizing the environmentironment\n",
    "            pass\n",
    "\n",
    "    def get_state(self):\n",
    "        # Returns a tuple representing the current state\n",
    "        return (tuple(self.player_position), tuple(self.box_position), tuple(self.target_position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Dynamic Programming Agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Value Iteration Progress:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Value Iteration Progress:   0%|          | 1/1000 [03:00<50:00:52, 180.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic Programming training completed in 266.38 seconds\n",
      "\n",
      "Training Monte Carlo Agent...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monte Carlo Training Progress: 100%|██████████| 10000/10000 [01:03<00:00, 157.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo training completed in 63.40 seconds\n",
      "\n",
      "Evaluating Dynamic Programming Agent...\n",
      "Dynamic Programming - Average Reward: -97.81, Average Steps: 98.03\n",
      "\n",
      "Evaluating Monte Carlo Agent...\n",
      "Monte Carlo - Average Reward: -88.30, Average Steps: 89.51\n",
      "\n",
      "Comparison:\n",
      "Training Time - DP: 266.38s, MC: 63.40s\n",
      "Average Reward - DP: -97.81, MC: -88.30\n",
      "Average Steps - DP: 98.03, MC: 89.51\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "def simulate_episode(environment, agent, max_steps=100, render=False):\n",
    "    state, _ = environment.reset()\n",
    "    total_return_val = 0\n",
    "    terminal = False\n",
    "    steps = 0\n",
    "    while not terminal and steps < max_steps:\n",
    "        current_state = environment.get_state()\n",
    "        action = agent.select_action(current_state)\n",
    "        state, return_val, terminal, _, _ = environment.step(action)\n",
    "        total_return_val += return_val\n",
    "        steps += 1\n",
    "        if render:\n",
    "            environment.render()\n",
    "    return total_return_val, steps\n",
    "\n",
    "\n",
    "def agent_assess(environment, agent, episode_count=100):\n",
    "    return_vals = []\n",
    "    step_count_list = []\n",
    "    for _ in range(episode_count):\n",
    "        total_return_val, steps = simulate_episode(environment, agent)\n",
    "        return_vals.append(total_return_val)\n",
    "        step_count_list.append(steps)\n",
    "    return np.mean(return_vals), np.mean(step_count_list)\n",
    "\n",
    "\n",
    "def main():\n",
    "    environment = SokobanEnv()\n",
    "\n",
    "    print(\"Training Dynamic Programming Agent...\")\n",
    "    init_time = time.time()\n",
    "    dp_agent = DPAgent(environment)\n",
    "    dp_agent.val_iterate()\n",
    "    dp_learn_policying_time = time.time() - init_time\n",
    "    print(f\"Dynamic Programming learn_policying completed in {dp_learn_policying_time:.2f} seconds\")\n",
    "\n",
    "    print(\"\\nTraining Monte Carlo Agent...\")\n",
    "    init_time = time.time()\n",
    "    mc_agent = MCAgent(environment)\n",
    "    mc_agent.learn_policy(episode_count=10000)\n",
    "    mc_learn_policying_time = time.time() - init_time\n",
    "    print(f\"Monte Carlo learn_policying completed in {mc_learn_policying_time:.2f} seconds\")\n",
    "\n",
    "    print(\"\\nEvaluating Dynamic Programming Agent...\")\n",
    "    dp_avg_return_val, dp_avg_steps = agent_assess(environment, dp_agent)\n",
    "    print(f\"Dynamic Programming - Average Reward: {dp_avg_return_val:.2f}, Average Steps: {dp_avg_steps:.2f}\")\n",
    "\n",
    "    print(\"\\nEvaluating Monte Carlo Agent...\")\n",
    "    mc_avg_return_val, mc_avg_steps = agent_assess(environment, mc_agent)\n",
    "    print(f\"Monte Carlo - Average Reward: {mc_avg_return_val:.2f}, Average Steps: {mc_avg_steps:.2f}\")\n",
    "\n",
    "    print(\"\\nComparison:\")\n",
    "    print(f\"Training Time - DP: {dp_learn_policying_time:.2f}s, MC: {mc_learn_policying_time:.2f}s\")\n",
    "    print(f\"Average Reward - DP: {dp_avg_return_val:.2f}, MC: {mc_avg_return_val:.2f}\")\n",
    "    print(f\"Average Steps - DP: {dp_avg_steps:.2f}, MC: {mc_avg_steps:.2f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
