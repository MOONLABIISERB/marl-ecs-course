{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0de9110a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_solutionrs/dynamic_programming_compute_solutionr.py\n",
    "\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "\n",
    "class DPSolver:\n",
    "    \"\"\"\n",
    "    Dynamic Programming compute_solutionr for the TSP.\n",
    "    Computes the shortest path visiting all cities exactly once.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_cities: int, dist_matrix: np.ndarray):\n",
    "        self.n_cities = n_cities\n",
    "        self.dist_matrix = dist_matrix\n",
    "        self.cost_table = {}\n",
    "        self.route_policy = {}\n",
    "        self.total_min_cost = None\n",
    "        self.last_city = None\n",
    "\n",
    "    def compute_solution(self):\n",
    "        \"\"\"Solves the TSP using dynamic programming.\"\"\"\n",
    "        num_subsets = 1 << self.n_cities  # Total number of subsets\n",
    "\n",
    "        # Initialize the value table with infinity\n",
    "        for subset in range(num_subsets):\n",
    "            for last in range(self.n_cities):\n",
    "                self.cost_table[(subset, last)] = np.inf\n",
    "        self.cost_table[(1 << 0, 0)] = 0  # Starting from city 0\n",
    "\n",
    "        # Dynamic programming to fill the value table\n",
    "        for subset_size in range(2, self.n_cities + 1):\n",
    "            for subset in [s for s in range(num_subsets) if bin(s).count('1') == subset_size and s & (1 << 0)]:\n",
    "                for last in range(self.n_cities):\n",
    "                    if not (subset & (1 << last)):\n",
    "                        continue\n",
    "                    prev_subset = subset ^ (1 << last)\n",
    "                    if prev_subset == 0:\n",
    "                        continue\n",
    "                    for k in range(self.n_cities):\n",
    "                        if (prev_subset & (1 << k)):\n",
    "                            cost = self.cost_table[(prev_subset, k)] + self.dist_matrix[k, last]\n",
    "                            if cost < self.cost_table[(subset, last)]:\n",
    "                                self.cost_table[(subset, last)] = cost\n",
    "                                self.route_policy[(subset, last)] = k\n",
    "\n",
    "        # Close the tour by returning to the starting city\n",
    "        full_subset = (1 << self.n_cities) - 1\n",
    "        min_cost = np.inf\n",
    "        last_city = None\n",
    "        for k in range(self.n_cities):\n",
    "            if k == 0:\n",
    "                continue\n",
    "            cost = self.cost_table[(full_subset, k)] + self.dist_matrix[k, 0]\n",
    "            if cost < min_cost:\n",
    "                min_cost = cost\n",
    "                last_city = k\n",
    "        self.total_min_cost = min_cost\n",
    "        self.last_city = last_city\n",
    "\n",
    "    def reconstruct_path(self) -> List[int]:\n",
    "        \"\"\"Reconstructs the optimal path from the computed policy.\"\"\"\n",
    "        subset = (1 << self.n_cities) - 1\n",
    "        path = [0]\n",
    "        last_city = self.last_city\n",
    "        for _ in range(self.n_cities - 1):\n",
    "            path.append(last_city)\n",
    "            subset ^= (1 << last_city)\n",
    "            last_city = self.route_policy.get((subset | (1 << last_city), last_city))\n",
    "            if last_city is None:\n",
    "                break\n",
    "        path.append(0)  # Return to the starting city\n",
    "        path.reverse()\n",
    "        return path\n",
    "\n",
    "    def get_action(self, current_city: int, visited_cities: np.ndarray) -> int:\n",
    "        \"\"\"Determines the next city to visit based on the policy.\"\"\"\n",
    "        subset = sum(1 << i for i in range(self.n_cities) if visited_cities[i])\n",
    "        if subset == (1 << self.n_cities) - 1:\n",
    "            return 0  # Return to starting city if all visited\n",
    "\n",
    "        min_cost = np.inf\n",
    "        next_city = None\n",
    "        for city in range(self.n_cities):\n",
    "            if not visited_cities[city]:\n",
    "                s = subset | (1 << city)\n",
    "                if (s, city) in self.cost_table:\n",
    "                    cost = self.cost_table[(s, city)] + self.dist_matrix[city, 0]\n",
    "                    if cost < min_cost:\n",
    "                        min_cost = cost\n",
    "                        next_city = city\n",
    "        if next_city is not None:\n",
    "            return next_city\n",
    "        else:\n",
    "            # If policy is undefined, select the nearest unvisited city\n",
    "            unvisited = [i for i in range(self.n_cities) if not visited_cities[i]]\n",
    "            return min(unvisited, key=lambda x: self.dist_matrix[current_city, x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7335bd28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_solutionrs/monte_carlo_epsilon_greedy_compute_solutionr.py\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "class MonteCarloEpsilonGreedySolver:\n",
    "    \"\"\"\n",
    "    Monte Carlo compute_solutionr with epsilon-greedy policy for the TSP.\n",
    "    Balances exploration and exploitation during policy learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_cities: int, dist_matrix: np.ndarray, epsilon: float = 0.1):\n",
    "        self.n_cities = n_cities\n",
    "        self.dist_matrix = dist_matrix\n",
    "        self.epsilon = epsilon\n",
    "        self.Q_values = {}\n",
    "        self.returns = {}\n",
    "        self.policy = {}\n",
    "\n",
    "    def epsilon_greedy(self, state: Tuple[int, int], possible_actions: List[int]) -> int:\n",
    "        \"\"\"Selects an action using epsilon-greedy strategy.\"\"\"\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.choice(possible_actions)\n",
    "        else:\n",
    "            q_values = [(action, self.Q_values.get((state, action), float('-inf'))) for action in possible_actions]\n",
    "            max_q = max(q_values, key=lambda x: x[1])[1]\n",
    "            best_actions = [action for action, q in q_values if q == max_q]\n",
    "            return random.choice(best_actions)\n",
    "\n",
    "    def generate_episode(self) -> List[Tuple[Tuple[int, int], int, float]]:\n",
    "        \"\"\"Generates an episode using the epsilon-greedy policy.\"\"\"\n",
    "        episode = []\n",
    "        visited = np.zeros(self.n_cities, dtype=bool)\n",
    "        current_city = random.randint(0, self.n_cities - 1)\n",
    "        visited[current_city] = True\n",
    "\n",
    "        while not visited.all():\n",
    "            possible_actions = [i for i in range(self.n_cities) if not visited[i]]\n",
    "            state_bitmask = self._state_to_bitmask(visited)\n",
    "            state = (current_city, state_bitmask)\n",
    "            action = self.epsilon_greedy(state, possible_actions)\n",
    "            reward = -self.dist_matrix[current_city, action]\n",
    "            episode.append((state, action, reward))\n",
    "            current_city = action\n",
    "            visited[current_city] = True\n",
    "\n",
    "        return episode\n",
    "\n",
    "    def _state_to_bitmask(self, visited: np.ndarray) -> int:\n",
    "        \"\"\"Converts the visited cities array to a bitmask.\"\"\"\n",
    "        return sum(1 << i for i, v in enumerate(visited) if v)\n",
    "\n",
    "    def compute_solution(self, num_episodes: int = 10000, method: str = \"first_visit\"):\n",
    "        \"\"\"Trains the policy using Monte Carlo simulations with epsilon-greedy exploration.\"\"\"\n",
    "        for _ in range(num_episodes):\n",
    "            episode = self.generate_episode()\n",
    "            G = 0\n",
    "            visited_state_actions = set()\n",
    "\n",
    "            for t in reversed(range(len(episode))):\n",
    "                state, action, reward = episode[t]\n",
    "                G += reward\n",
    "\n",
    "                if method == \"first_visit\" and (state, action) in visited_state_actions:\n",
    "                    continue\n",
    "\n",
    "                visited_state_actions.add((state, action))\n",
    "                if (state, action) not in self.returns:\n",
    "                    self.returns[(state, action)] = []\n",
    "                self.returns[(state, action)].append(G)\n",
    "                self.Q_values[(state, action)] = np.mean(self.returns[(state, action)])\n",
    "\n",
    "                # Update policy\n",
    "                current_city, _ = state\n",
    "                possible_actions = [a for a in range(self.n_cities) if a != current_city]\n",
    "                self.policy[state] = max(\n",
    "                    possible_actions,\n",
    "                    key=lambda a: self.Q_values.get((state, a), float('-inf'))\n",
    "                )\n",
    "\n",
    "    def get_action(self, current_city: int, visited_cities: np.ndarray) -> int:\n",
    "        \"\"\"Selects the next city to visit using the epsilon-greedy policy.\"\"\"\n",
    "        possible_actions = [i for i in range(self.n_cities) if not visited_cities[i]]\n",
    "        if not possible_actions:\n",
    "            return 0  # Return to starting city if all cities are visited\n",
    "\n",
    "        state_bitmask = self._state_to_bitmask(visited_cities)\n",
    "        state = (current_city, state_bitmask)\n",
    "        return self.epsilon_greedy(state, possible_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccf43bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute_solutionrs/monte_carlo_compute_solutionr.py\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "\n",
    "class MonteCarloSolver:\n",
    "    \"\"\"\n",
    "    Monte Carlo compute_solutionr for the TSP.\n",
    "    Learns a policy by simulating episodes and updating value estimates.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_cities: int, dist_matrix: np.ndarray):\n",
    "        self.n_cities = n_cities\n",
    "        self.dist_matrix = dist_matrix\n",
    "        self.Q_values = {}\n",
    "        self.returns = {}\n",
    "        self.policy = {}\n",
    "\n",
    "    def generate_episode(self) -> List[Tuple[Tuple[int, int], int, float]]:\n",
    "        \"\"\"Generates an episode by randomly selecting actions.\"\"\"\n",
    "        episode = []\n",
    "        visited = np.zeros(self.n_cities, dtype=bool)\n",
    "        current_city = random.randint(0, self.n_cities - 1)\n",
    "        visited[current_city] = True\n",
    "\n",
    "        while not visited.all():\n",
    "            possible_actions = [i for i in range(self.n_cities) if not visited[i]]\n",
    "            action = random.choice(possible_actions)\n",
    "            reward = -self.dist_matrix[current_city, action]\n",
    "            state = (current_city, self._state_to_bitmask(visited))\n",
    "            episode.append((state, action, reward))\n",
    "            current_city = action\n",
    "            visited[current_city] = True\n",
    "\n",
    "        return episode\n",
    "\n",
    "    def _state_to_bitmask(self, visited: np.ndarray) -> int:\n",
    "        \"\"\"Converts the visited cities array to a bitmask.\"\"\"\n",
    "        return sum(1 << i for i, v in enumerate(visited) if v)\n",
    "\n",
    "    def compute_solution(self, num_episodes: int = 10000, method: str = \"first_visit\"):\n",
    "        \"\"\"Trains the policy using Monte Carlo simulations.\"\"\"\n",
    "        for _ in range(num_episodes):\n",
    "            episode = self.generate_episode()\n",
    "            G = 0\n",
    "            visited_state_actions = set()\n",
    "\n",
    "            for t in reversed(range(len(episode))):\n",
    "                state, action, reward = episode[t]\n",
    "                G += reward\n",
    "\n",
    "                if method == \"first_visit\" and (state, action) in visited_state_actions:\n",
    "                    continue\n",
    "\n",
    "                visited_state_actions.add((state, action))\n",
    "                if (state, action) not in self.returns:\n",
    "                    self.returns[(state, action)] = []\n",
    "                self.returns[(state, action)].append(G)\n",
    "                self.Q_values[(state, action)] = np.mean(self.returns[(state, action)])\n",
    "\n",
    "                # Update policy\n",
    "                current_city, _ = state\n",
    "                possible_actions = [a for a in range(self.n_cities) if a != current_city]\n",
    "                self.policy[state] = max(\n",
    "                    possible_actions,\n",
    "                    key=lambda a: self.Q_values.get((state, a), float('-inf'))\n",
    "                )\n",
    "\n",
    "    def get_action(self, current_city: int, visited_cities: np.ndarray) -> int:\n",
    "        \"\"\"Selects the next city to visit based on the learned policy.\"\"\"\n",
    "        state_bitmask = self._state_to_bitmask(visited_cities)\n",
    "        state = (current_city, state_bitmask)\n",
    "        possible_actions = [i for i in range(self.n_cities) if not visited_cities[i]]\n",
    "        if state in self.policy and self.policy[state] in possible_actions:\n",
    "            return self.policy[state]\n",
    "        elif possible_actions:\n",
    "            return min(possible_actions, key=lambda x: self.dist_matrix[current_city, x])\n",
    "        else:\n",
    "            return 0  # Return to starting city if no unvisited cities left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dc6cbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsp_env.py\n",
    "\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "from typing import Optional, Tuple, List, Dict\n",
    "\n",
    "\n",
    "class TSPEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom OpenAI Gym environment for the Traveling Salesman Problem (TSP).\n",
    "    The goal is to find the shortest possible route that visits each city exactly once.\n",
    "    \"\"\"\n",
    "\n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, n_cities: int, area_size: int = 30, seed: Optional[int] = None):\n",
    "        super(TSPEnv, self).__init__()\n",
    "        self.n_cities = n_cities\n",
    "        self.area_size = area_size\n",
    "        self.seed(seed)\n",
    "\n",
    "        # Generate random positions for the cities\n",
    "        self.city_positions = self._generate_city_positions()\n",
    "\n",
    "        # Calculate the distance matrix\n",
    "        self.dist_matrix = self._calculate_dist_matrix()\n",
    "\n",
    "        # Define action and observation spaces\n",
    "        self.action_space = spaces.Discrete(self.n_cities)\n",
    "        self.observation_space = spaces.Tuple((\n",
    "            spaces.Discrete(self.n_cities),  # Current city\n",
    "            spaces.MultiBinary(self.n_cities)  # Visited cities\n",
    "        ))\n",
    "\n",
    "        self.current_city = 0\n",
    "        self.visited_cities = np.zeros(self.n_cities, dtype=np.int8)\n",
    "        self.total_distance = 0.0\n",
    "\n",
    "    def _generate_city_positions(self) -> np.ndarray:\n",
    "        \"\"\"Generates random (x, y) positions for each city within the specified area.\"\"\"\n",
    "        return np.random.uniform(0, self.area_size, size=(self.n_cities, 2))\n",
    "\n",
    "    def _calculate_dist_matrix(self) -> np.ndarray:\n",
    "        \"\"\"Calculates the Euclidean distance between all pairs of cities.\"\"\"\n",
    "        num = self.n_cities\n",
    "        dist_matrix = np.zeros((num, num))\n",
    "        for i in range(num):\n",
    "            for j in range(num):\n",
    "                if i != j:\n",
    "                    dist_matrix[i, j] = np.linalg.norm(self.city_positions[i] - self.city_positions[j])\n",
    "        return dist_matrix\n",
    "\n",
    "    def reset(self) -> Tuple[int, np.ndarray]:\n",
    "        \"\"\"Resets the environment to the initial state.\"\"\"\n",
    "        self.current_city = 0\n",
    "        self.visited_cities = np.zeros(self.n_cities, dtype=np.int8)\n",
    "        self.visited_cities[self.current_city] = 1\n",
    "        self.total_distance = 0.0\n",
    "        return self.current_city, self.visited_cities.copy()\n",
    "\n",
    "    def step(self, action: int) -> Tuple[Tuple[int, np.ndarray], float, bool, bool, Dict]:\n",
    "        \"\"\"Performs the action of moving to the next city.\"\"\"\n",
    "        assert self.action_space.contains(action), f\"Invalid action: {action}\"\n",
    "\n",
    "        reward = 0.0\n",
    "        done = False\n",
    "\n",
    "        if self.visited_cities[action]:\n",
    "            # High penalty for revisiting a city\n",
    "            reward = -10000.0\n",
    "            done = True  # End the episode on invalid action\n",
    "        else:\n",
    "            # Calculate distance to the next city\n",
    "            distance = self.dist_matrix[self.current_city, action]\n",
    "            self.total_distance += distance\n",
    "            reward = -distance  # Negative reward for traveling distance\n",
    "            self.current_city = action\n",
    "            self.visited_cities[action] = 1\n",
    "\n",
    "            # Check if all cities have been visited\n",
    "            if self.visited_cities.sum() == self.n_cities:\n",
    "                done = True\n",
    "\n",
    "        observation = (self.current_city, self.visited_cities.copy())\n",
    "        info = {}\n",
    "\n",
    "        return observation, reward, done, False, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"Renders the environment (not implemented).\"\"\"\n",
    "        pass\n",
    "\n",
    "    def seed(self, seed: Optional[int] = None):\n",
    "        \"\"\"Sets the random seed for reproducibility.\"\"\"\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "49440001",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_dp_compute_solutionr(env: TSPEnv) -> DPSolver:\n",
    "    \"\"\"Creates and compute_solutions a Dynamic Programming compute_solutionr for the TSP environment.\"\"\"\n",
    "    compute_solutionr = DPSolver(env.n_cities, env.dist_matrix)\n",
    "    compute_solutionr.compute_solution()\n",
    "    return compute_solutionr\n",
    "\n",
    "\n",
    "def create_monte_carlo_compute_solutionr(env: TSPEnv, num_episodes: int = 10000, method: str = \"first_visit\") -> MonteCarloSolver:\n",
    "    \"\"\"Creates and trains a Monte Carlo compute_solutionr for the TSP environment.\"\"\"\n",
    "    compute_solutionr = MonteCarloSolver(env.n_cities, env.dist_matrix)\n",
    "    compute_solutionr.compute_solution(num_episodes, method)\n",
    "    return compute_solutionr\n",
    "\n",
    "\n",
    "def create_monte_carlo_epsilon_greedy_compute_solutionr(env: TSPEnv, num_episodes: int = 10000, epsilon: float = 0.1, method: str = \"first_visit\") -> MonteCarloEpsilonGreedySolver:\n",
    "    \"\"\"Creates and trains a Monte Carlo Epsilon-Greedy compute_solutionr for the TSP environment.\"\"\"\n",
    "    compute_solutionr = MonteCarloEpsilonGreedySolver(env.n_cities, env.dist_matrix, epsilon)\n",
    "    compute_solutionr.compute_solution(num_episodes, method)\n",
    "    return compute_solutionr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "849bb9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing Dynamic Programming:\n",
      "Episode 0: Total Reward = -59.15168247679169\n",
      "Episode 1: Total Reward = -59.15168247679169\n",
      "Episode 2: Total Reward = -59.15168247679169\n",
      "Episode 3: Total Reward = -59.15168247679169\n",
      "Episode 4: Total Reward = -59.15168247679169\n",
      "Episode 5: Total Reward = -59.15168247679169\n",
      "Episode 6: Total Reward = -59.15168247679169\n",
      "Episode 7: Total Reward = -59.15168247679169\n",
      "Episode 8: Total Reward = -59.15168247679169\n",
      "Episode 9: Total Reward = -59.15168247679169\n",
      "Episode 10: Total Reward = -59.15168247679169\n",
      "Episode 11: Total Reward = -59.15168247679169\n",
      "Episode 12: Total Reward = -59.15168247679169\n",
      "Episode 13: Total Reward = -59.15168247679169\n",
      "Episode 14: Total Reward = -59.15168247679169\n",
      "Episode 15: Total Reward = -59.15168247679169\n",
      "Episode 16: Total Reward = -59.15168247679169\n",
      "Episode 17: Total Reward = -59.15168247679169\n",
      "Episode 18: Total Reward = -59.15168247679169\n",
      "Episode 19: Total Reward = -59.15168247679169\n",
      "Episode 20: Total Reward = -59.15168247679169\n",
      "Episode 21: Total Reward = -59.15168247679169\n",
      "Episode 22: Total Reward = -59.15168247679169\n",
      "Episode 23: Total Reward = -59.15168247679169\n",
      "Episode 24: Total Reward = -59.15168247679169\n",
      "Episode 25: Total Reward = -59.15168247679169\n",
      "Episode 26: Total Reward = -59.15168247679169\n",
      "Episode 27: Total Reward = -59.15168247679169\n",
      "Episode 28: Total Reward = -59.15168247679169\n",
      "Episode 29: Total Reward = -59.15168247679169\n",
      "Episode 30: Total Reward = -59.15168247679169\n",
      "Episode 31: Total Reward = -59.15168247679169\n",
      "Episode 32: Total Reward = -59.15168247679169\n",
      "Episode 33: Total Reward = -59.15168247679169\n",
      "Episode 34: Total Reward = -59.15168247679169\n",
      "Episode 35: Total Reward = -59.15168247679169\n",
      "Episode 36: Total Reward = -59.15168247679169\n",
      "Episode 37: Total Reward = -59.15168247679169\n",
      "Episode 38: Total Reward = -59.15168247679169\n",
      "Episode 39: Total Reward = -59.15168247679169\n",
      "Episode 40: Total Reward = -59.15168247679169\n",
      "Episode 41: Total Reward = -59.15168247679169\n",
      "Episode 42: Total Reward = -59.15168247679169\n",
      "Episode 43: Total Reward = -59.15168247679169\n",
      "Episode 44: Total Reward = -59.15168247679169\n",
      "Episode 45: Total Reward = -59.15168247679169\n",
      "Episode 46: Total Reward = -59.15168247679169\n",
      "Episode 47: Total Reward = -59.15168247679169\n",
      "Episode 48: Total Reward = -59.15168247679169\n",
      "Episode 49: Total Reward = -59.15168247679169\n",
      "Episode 50: Total Reward = -59.15168247679169\n",
      "Episode 51: Total Reward = -59.15168247679169\n",
      "Episode 52: Total Reward = -59.15168247679169\n",
      "Episode 53: Total Reward = -59.15168247679169\n",
      "Episode 54: Total Reward = -59.15168247679169\n",
      "Episode 55: Total Reward = -59.15168247679169\n",
      "Episode 56: Total Reward = -59.15168247679169\n",
      "Episode 57: Total Reward = -59.15168247679169\n",
      "Episode 58: Total Reward = -59.15168247679169\n",
      "Episode 59: Total Reward = -59.15168247679169\n",
      "Episode 60: Total Reward = -59.15168247679169\n",
      "Episode 61: Total Reward = -59.15168247679169\n",
      "Episode 62: Total Reward = -59.15168247679169\n",
      "Episode 63: Total Reward = -59.15168247679169\n",
      "Episode 64: Total Reward = -59.15168247679169\n",
      "Episode 65: Total Reward = -59.15168247679169\n",
      "Episode 66: Total Reward = -59.15168247679169\n",
      "Episode 67: Total Reward = -59.15168247679169\n",
      "Episode 68: Total Reward = -59.15168247679169\n",
      "Episode 69: Total Reward = -59.15168247679169\n",
      "Episode 70: Total Reward = -59.15168247679169\n",
      "Episode 71: Total Reward = -59.15168247679169\n",
      "Episode 72: Total Reward = -59.15168247679169\n",
      "Episode 73: Total Reward = -59.15168247679169\n",
      "Episode 74: Total Reward = -59.15168247679169\n",
      "Episode 75: Total Reward = -59.15168247679169\n",
      "Episode 76: Total Reward = -59.15168247679169\n",
      "Episode 77: Total Reward = -59.15168247679169\n",
      "Episode 78: Total Reward = -59.15168247679169\n",
      "Episode 79: Total Reward = -59.15168247679169\n",
      "Episode 80: Total Reward = -59.15168247679169\n",
      "Episode 81: Total Reward = -59.15168247679169\n",
      "Episode 82: Total Reward = -59.15168247679169\n",
      "Episode 83: Total Reward = -59.15168247679169\n",
      "Episode 84: Total Reward = -59.15168247679169\n",
      "Episode 85: Total Reward = -59.15168247679169\n",
      "Episode 86: Total Reward = -59.15168247679169\n",
      "Episode 87: Total Reward = -59.15168247679169\n",
      "Episode 88: Total Reward = -59.15168247679169\n",
      "Episode 89: Total Reward = -59.15168247679169\n",
      "Episode 90: Total Reward = -59.15168247679169\n",
      "Episode 91: Total Reward = -59.15168247679169\n",
      "Episode 92: Total Reward = -59.15168247679169\n",
      "Episode 93: Total Reward = -59.15168247679169\n",
      "Episode 94: Total Reward = -59.15168247679169\n",
      "Episode 95: Total Reward = -59.15168247679169\n",
      "Episode 96: Total Reward = -59.15168247679169\n",
      "Episode 97: Total Reward = -59.15168247679169\n",
      "Episode 98: Total Reward = -59.15168247679169\n",
      "Episode 99: Total Reward = -59.15168247679169\n",
      "Average return over 100 episodes: -59.151682476791684\n",
      "\n",
      "Testing Monte Carlo:\n",
      "Episode 0: Total Reward = -64.28124298113774\n",
      "Episode 1: Total Reward = -64.28124298113774\n",
      "Episode 2: Total Reward = -64.28124298113774\n",
      "Episode 3: Total Reward = -64.28124298113774\n",
      "Episode 4: Total Reward = -64.28124298113774\n",
      "Episode 5: Total Reward = -64.28124298113774\n",
      "Episode 6: Total Reward = -64.28124298113774\n",
      "Episode 7: Total Reward = -64.28124298113774\n",
      "Episode 8: Total Reward = -64.28124298113774\n",
      "Episode 9: Total Reward = -64.28124298113774\n",
      "Episode 10: Total Reward = -64.28124298113774\n",
      "Episode 11: Total Reward = -64.28124298113774\n",
      "Episode 12: Total Reward = -64.28124298113774\n",
      "Episode 13: Total Reward = -64.28124298113774\n",
      "Episode 14: Total Reward = -64.28124298113774\n",
      "Episode 15: Total Reward = -64.28124298113774\n",
      "Episode 16: Total Reward = -64.28124298113774\n",
      "Episode 17: Total Reward = -64.28124298113774\n",
      "Episode 18: Total Reward = -64.28124298113774\n",
      "Episode 19: Total Reward = -64.28124298113774\n",
      "Episode 20: Total Reward = -64.28124298113774\n",
      "Episode 21: Total Reward = -64.28124298113774\n",
      "Episode 22: Total Reward = -64.28124298113774\n",
      "Episode 23: Total Reward = -64.28124298113774\n",
      "Episode 24: Total Reward = -64.28124298113774\n",
      "Episode 25: Total Reward = -64.28124298113774\n",
      "Episode 26: Total Reward = -64.28124298113774\n",
      "Episode 27: Total Reward = -64.28124298113774\n",
      "Episode 28: Total Reward = -64.28124298113774\n",
      "Episode 29: Total Reward = -64.28124298113774\n",
      "Episode 30: Total Reward = -64.28124298113774\n",
      "Episode 31: Total Reward = -64.28124298113774\n",
      "Episode 32: Total Reward = -64.28124298113774\n",
      "Episode 33: Total Reward = -64.28124298113774\n",
      "Episode 34: Total Reward = -64.28124298113774\n",
      "Episode 35: Total Reward = -64.28124298113774\n",
      "Episode 36: Total Reward = -64.28124298113774\n",
      "Episode 37: Total Reward = -64.28124298113774\n",
      "Episode 38: Total Reward = -64.28124298113774\n",
      "Episode 39: Total Reward = -64.28124298113774\n",
      "Episode 40: Total Reward = -64.28124298113774\n",
      "Episode 41: Total Reward = -64.28124298113774\n",
      "Episode 42: Total Reward = -64.28124298113774\n",
      "Episode 43: Total Reward = -64.28124298113774\n",
      "Episode 44: Total Reward = -64.28124298113774\n",
      "Episode 45: Total Reward = -64.28124298113774\n",
      "Episode 46: Total Reward = -64.28124298113774\n",
      "Episode 47: Total Reward = -64.28124298113774\n",
      "Episode 48: Total Reward = -64.28124298113774\n",
      "Episode 49: Total Reward = -64.28124298113774\n",
      "Episode 50: Total Reward = -64.28124298113774\n",
      "Episode 51: Total Reward = -64.28124298113774\n",
      "Episode 52: Total Reward = -64.28124298113774\n",
      "Episode 53: Total Reward = -64.28124298113774\n",
      "Episode 54: Total Reward = -64.28124298113774\n",
      "Episode 55: Total Reward = -64.28124298113774\n",
      "Episode 56: Total Reward = -64.28124298113774\n",
      "Episode 57: Total Reward = -64.28124298113774\n",
      "Episode 58: Total Reward = -64.28124298113774\n",
      "Episode 59: Total Reward = -64.28124298113774\n",
      "Episode 60: Total Reward = -64.28124298113774\n",
      "Episode 61: Total Reward = -64.28124298113774\n",
      "Episode 62: Total Reward = -64.28124298113774\n",
      "Episode 63: Total Reward = -64.28124298113774\n",
      "Episode 64: Total Reward = -64.28124298113774\n",
      "Episode 65: Total Reward = -64.28124298113774\n",
      "Episode 66: Total Reward = -64.28124298113774\n",
      "Episode 67: Total Reward = -64.28124298113774\n",
      "Episode 68: Total Reward = -64.28124298113774\n",
      "Episode 69: Total Reward = -64.28124298113774\n",
      "Episode 70: Total Reward = -64.28124298113774\n",
      "Episode 71: Total Reward = -64.28124298113774\n",
      "Episode 72: Total Reward = -64.28124298113774\n",
      "Episode 73: Total Reward = -64.28124298113774\n",
      "Episode 74: Total Reward = -64.28124298113774\n",
      "Episode 75: Total Reward = -64.28124298113774\n",
      "Episode 76: Total Reward = -64.28124298113774\n",
      "Episode 77: Total Reward = -64.28124298113774\n",
      "Episode 78: Total Reward = -64.28124298113774\n",
      "Episode 79: Total Reward = -64.28124298113774\n",
      "Episode 80: Total Reward = -64.28124298113774\n",
      "Episode 81: Total Reward = -64.28124298113774\n",
      "Episode 82: Total Reward = -64.28124298113774\n",
      "Episode 83: Total Reward = -64.28124298113774\n",
      "Episode 84: Total Reward = -64.28124298113774\n",
      "Episode 85: Total Reward = -64.28124298113774\n",
      "Episode 86: Total Reward = -64.28124298113774\n",
      "Episode 87: Total Reward = -64.28124298113774\n",
      "Episode 88: Total Reward = -64.28124298113774\n",
      "Episode 89: Total Reward = -64.28124298113774\n",
      "Episode 90: Total Reward = -64.28124298113774\n",
      "Episode 91: Total Reward = -64.28124298113774\n",
      "Episode 92: Total Reward = -64.28124298113774\n",
      "Episode 93: Total Reward = -64.28124298113774\n",
      "Episode 94: Total Reward = -64.28124298113774\n",
      "Episode 95: Total Reward = -64.28124298113774\n",
      "Episode 96: Total Reward = -64.28124298113774\n",
      "Episode 97: Total Reward = -64.28124298113774\n",
      "Episode 98: Total Reward = -64.28124298113774\n",
      "Episode 99: Total Reward = -64.28124298113774\n",
      "Average return over 100 episodes: -64.28124298113774\n",
      "\n",
      "Testing Monte Carlo Epsilon-Greedy:\n",
      "Episode 0: Total Reward = -64.09068837836057\n",
      "Episode 1: Total Reward = -99.79861205635953\n",
      "Episode 2: Total Reward = -64.09068837836057\n",
      "Episode 3: Total Reward = -85.53477496568037\n",
      "Episode 4: Total Reward = -57.82130226463099\n",
      "Episode 5: Total Reward = -64.09068837836057\n",
      "Episode 6: Total Reward = -64.09068837836057\n",
      "Episode 7: Total Reward = -80.30997769704902\n",
      "Episode 8: Total Reward = -64.09068837836057\n",
      "Episode 9: Total Reward = -85.53477496568037\n",
      "Episode 10: Total Reward = -64.09068837836057\n",
      "Episode 11: Total Reward = -64.09068837836057\n",
      "Episode 12: Total Reward = -64.09068837836057\n",
      "Episode 13: Total Reward = -64.09068837836057\n",
      "Episode 14: Total Reward = -64.09068837836057\n",
      "Episode 15: Total Reward = -64.09068837836057\n",
      "Episode 16: Total Reward = -80.67613588279367\n",
      "Episode 17: Total Reward = -78.39540954501615\n",
      "Episode 18: Total Reward = -78.39540954501615\n",
      "Episode 19: Total Reward = -64.09068837836057\n",
      "Episode 20: Total Reward = -64.09068837836057\n",
      "Episode 21: Total Reward = -64.09068837836057\n",
      "Episode 22: Total Reward = -64.09068837836057\n",
      "Episode 23: Total Reward = -64.09068837836057\n",
      "Episode 24: Total Reward = -64.09068837836057\n",
      "Episode 25: Total Reward = -61.686150325315566\n",
      "Episode 26: Total Reward = -64.09068837836057\n",
      "Episode 27: Total Reward = -64.09068837836057\n",
      "Episode 28: Total Reward = -64.09068837836057\n",
      "Episode 29: Total Reward = -64.09068837836057\n",
      "Episode 30: Total Reward = -64.09068837836057\n",
      "Episode 31: Total Reward = -64.09068837836057\n",
      "Episode 32: Total Reward = -64.09068837836057\n",
      "Episode 33: Total Reward = -66.25992351946951\n",
      "Episode 34: Total Reward = -64.09068837836057\n",
      "Episode 35: Total Reward = -64.09068837836057\n",
      "Episode 36: Total Reward = -64.09068837836057\n",
      "Episode 37: Total Reward = -64.09068837836057\n",
      "Episode 38: Total Reward = -64.09068837836057\n",
      "Episode 39: Total Reward = -64.09068837836057\n",
      "Episode 40: Total Reward = -64.09068837836057\n",
      "Episode 41: Total Reward = -57.82130226463099\n",
      "Episode 42: Total Reward = -64.09068837836057\n",
      "Episode 43: Total Reward = -64.09068837836057\n",
      "Episode 44: Total Reward = -64.09068837836057\n",
      "Episode 45: Total Reward = -64.09068837836057\n",
      "Episode 46: Total Reward = -64.09068837836057\n",
      "Episode 47: Total Reward = -64.09068837836057\n",
      "Episode 48: Total Reward = -64.09068837836057\n",
      "Episode 49: Total Reward = -78.29999845942875\n",
      "Episode 50: Total Reward = -64.09068837836057\n",
      "Episode 51: Total Reward = -64.09068837836057\n",
      "Episode 52: Total Reward = -97.0770892654909\n",
      "Episode 53: Total Reward = -64.09068837836057\n",
      "Episode 54: Total Reward = -85.53477496568037\n",
      "Episode 55: Total Reward = -64.09068837836057\n",
      "Episode 56: Total Reward = -64.09068837836057\n",
      "Episode 57: Total Reward = -64.09068837836057\n",
      "Episode 58: Total Reward = -64.09068837836057\n",
      "Episode 59: Total Reward = -85.53477496568037\n",
      "Episode 60: Total Reward = -64.09068837836057\n",
      "Episode 61: Total Reward = -64.09068837836057\n",
      "Episode 62: Total Reward = -64.09068837836057\n",
      "Episode 63: Total Reward = -64.09068837836057\n",
      "Episode 64: Total Reward = -64.09068837836057\n",
      "Episode 65: Total Reward = -64.09068837836057\n",
      "Episode 66: Total Reward = -61.686150325315566\n",
      "Episode 67: Total Reward = -90.00700855048318\n",
      "Episode 68: Total Reward = -64.09068837836057\n",
      "Episode 69: Total Reward = -64.09068837836057\n",
      "Episode 70: Total Reward = -64.09068837836057\n",
      "Episode 71: Total Reward = -64.09068837836057\n",
      "Episode 72: Total Reward = -64.09068837836057\n",
      "Episode 73: Total Reward = -64.09068837836057\n",
      "Episode 74: Total Reward = -64.09068837836057\n",
      "Episode 75: Total Reward = -85.53477496568037\n",
      "Episode 76: Total Reward = -64.09068837836057\n",
      "Episode 77: Total Reward = -64.09068837836057\n",
      "Episode 78: Total Reward = -64.09068837836057\n",
      "Episode 79: Total Reward = -64.09068837836057\n",
      "Episode 80: Total Reward = -80.49256723938753\n",
      "Episode 81: Total Reward = -64.09068837836057\n",
      "Episode 82: Total Reward = -64.09068837836057\n",
      "Episode 83: Total Reward = -96.70487059870496\n",
      "Episode 84: Total Reward = -64.09068837836057\n",
      "Episode 85: Total Reward = -64.09068837836057\n",
      "Episode 86: Total Reward = -64.09068837836057\n",
      "Episode 87: Total Reward = -64.09068837836057\n",
      "Episode 88: Total Reward = -85.53477496568037\n",
      "Episode 89: Total Reward = -64.09068837836057\n",
      "Episode 90: Total Reward = -66.25992351946951\n",
      "Episode 91: Total Reward = -64.09068837836057\n",
      "Episode 92: Total Reward = -64.09068837836057\n",
      "Episode 93: Total Reward = -64.09068837836057\n",
      "Episode 94: Total Reward = -64.09068837836057\n",
      "Episode 95: Total Reward = -64.09068837836057\n",
      "Episode 96: Total Reward = -64.09068837836057\n",
      "Episode 97: Total Reward = -64.09068837836057\n",
      "Episode 98: Total Reward = -65.19876333831235\n",
      "Episode 99: Total Reward = -85.53477496568037\n",
      "Average return over 100 episodes: -67.66526335912042\n",
      "\n",
      "Performance Comparison:\n",
      "Dynamic Programming Average Return: -59.151682476791684\n",
      "Monte Carlo Average Return: -64.28124298113774\n",
      "Monte Carlo Epsilon-Greedy Average Return: -67.66526335912042\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "def main():\n",
    "    n_cities = 6\n",
    "    num_episodes = 100\n",
    "\n",
    "    env = TSPEnv(n_cities, seed=42)\n",
    "\n",
    "    # Create compute_solutionrs\n",
    "    dp_compute_solutionr = initialize_dp_compute_solutionr(env)\n",
    "    mc_compute_solutionr = create_monte_carlo_compute_solutionr(env, num_episodes=10000, method=\"first_visit\")\n",
    "    mc_epsilon_greedy_compute_solutionr = create_monte_carlo_epsilon_greedy_compute_solutionr(env, num_episodes=10000, epsilon=0.1, method=\"first_visit\")\n",
    "\n",
    "    compute_solutionrs = {\n",
    "        \"Dynamic Programming\": dp_compute_solutionr,\n",
    "        \"Monte Carlo\": mc_compute_solutionr,\n",
    "        \"Monte Carlo Epsilon-Greedy\": mc_epsilon_greedy_compute_solutionr\n",
    "    }\n",
    "\n",
    "    total_performance = {}\n",
    "\n",
    "    for compute_solutionr_name, compute_solutionr in compute_solutionrs.items():\n",
    "        print(f\"\\nTesting {compute_solutionr_name}:\")\n",
    "        episode_returns = []\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            observation = env.reset()\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "            current_city, visited_cities = observation\n",
    "\n",
    "            while not done:\n",
    "                action = compute_solutionr.get_action(current_city, visited_cities)\n",
    "                observation, reward, done, _, _ = env.step(action)\n",
    "                total_reward += reward\n",
    "                current_city, visited_cities = observation\n",
    "\n",
    "            episode_returns.append(total_reward)\n",
    "            print(f\"Episode {episode}: Total Reward = {total_reward}\")\n",
    "\n",
    "        avg_return = np.mean(episode_returns)\n",
    "        print(f\"Average return over {num_episodes} episodes: {avg_return}\")\n",
    "        total_performance[compute_solutionr_name] = avg_return\n",
    "\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    for compute_solutionr_name, avg_return in total_performance.items():\n",
    "        print(f\"{compute_solutionr_name} Average Return: {avg_return}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
